---
title: "AutoML in Practice"
author: RAJ KUMAR
date: '2019-01-04'
slug: automl-with-h2o
#lastmod: '2020-04-25'
categories: []
tags:
  - automl
  - R
banner: 'banners/employee_churn.jpeg'
description: "Predicting Employee Churn using H2O and LIME"
images: ['banners/employee_churn.jpeg']
menu: ''    
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="introduction-the-problem" class="section level2">
<h2>Introduction: The Problem</h2>
<p>Dataset Source:</p>
<p>[<a href="https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/" class="uri">https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/</a>]</p>
</div>
<div id="required-packages" class="section level2">
<h2>Required packages</h2>
<pre class="r"><code>library(h2o)          # High performance machine learning
library(lime)         # Explaining black-box models
library(recipes)      # Creating ML preprocessing recipes
library(tidyverse)    # Set of pkgs for data science: dplyr, ggplot2, purrr, tidyr, ...
library(tidyquant)    # Financial time series pkg - Used for theme_tq ggplot2 theme
library(glue)         # Pasting text
library(cowplot)      # Handling multiple ggplots
library(GGally)       # Data understanding - visualizations
library(skimr)        # Data understanding - summary information
library(fs)           # Working with the file system - directory structure
library(readxl)       # Reading excel files
library(forcats)      # Tools for working with categorical variables
library(writexl)      # Writing to excel files</code></pre>
<p><strong>Reading dataset in R</strong></p>
<pre class="r"><code>path_train &lt;- &quot;00_Data/telco_train.xlsx&quot;
train_raw_tbl&lt;-read_excel(path_train,sheet = 1)
colnames(train_raw_tbl)</code></pre>
<pre><code>##  [1] &quot;Age&quot;                      &quot;Attrition&quot;               
##  [3] &quot;BusinessTravel&quot;           &quot;DailyRate&quot;               
##  [5] &quot;Department&quot;               &quot;DistanceFromHome&quot;        
##  [7] &quot;Education&quot;                &quot;EducationField&quot;          
##  [9] &quot;EmployeeCount&quot;            &quot;EmployeeNumber&quot;          
## [11] &quot;EnvironmentSatisfaction&quot;  &quot;Gender&quot;                  
## [13] &quot;HourlyRate&quot;               &quot;JobInvolvement&quot;          
## [15] &quot;JobLevel&quot;                 &quot;JobRole&quot;                 
## [17] &quot;JobSatisfaction&quot;          &quot;MaritalStatus&quot;           
## [19] &quot;MonthlyIncome&quot;            &quot;MonthlyRate&quot;             
## [21] &quot;NumCompaniesWorked&quot;       &quot;Over18&quot;                  
## [23] &quot;OverTime&quot;                 &quot;PercentSalaryHike&quot;       
## [25] &quot;PerformanceRating&quot;        &quot;RelationshipSatisfaction&quot;
## [27] &quot;StandardHours&quot;            &quot;StockOptionLevel&quot;        
## [29] &quot;TotalWorkingYears&quot;        &quot;TrainingTimesLastYear&quot;   
## [31] &quot;WorkLifeBalance&quot;          &quot;YearsAtCompany&quot;          
## [33] &quot;YearsInCurrentRole&quot;       &quot;YearsSinceLastPromotion&quot; 
## [35] &quot;YearsWithCurrManager&quot;</code></pre>
</div>
<div id="business-science-problem-framework" class="section level2">
<h2>Business Science Problem Framework</h2>
<div class="figure">
<img src="https://www.business-science.io/assets/2018-06-19_BSPF/bspf_top.PNG" alt="" />
<p class="caption">Credits: Business Science
University</p>
</div>
<div id="phase-1-business-understanding" class="section level3">
<h3>Phase 1 : Business Understanding</h3>
<div id="view-business-as-machine" class="section level4">
<h4>View Business as Machine</h4>
<p><strong>Step 1: Isolate Business Units</strong></p>
<p>In realistic situation, we may not have the data on all the columns
listed, so subsetting data here for fields of interest like
EmployeeNumber, Department, JobRole, Attrition</p>
<pre class="r"><code>dept_job_role_tbl &lt;- train_raw_tbl %&gt;% select(EmployeeNumber,Department,JobRole,Attrition)

dept_job_role_tbl</code></pre>
<pre><code>## # A tibble: 1,250 x 4
##    EmployeeNumber Department             JobRole                  
##             &lt;dbl&gt; &lt;chr&gt;                  &lt;chr&gt;                    
##  1              1 Sales                  Sales Executive          
##  2              2 Research &amp; Development Research Scientist       
##  3              4 Research &amp; Development Laboratory Technician    
##  4              5 Research &amp; Development Research Scientist       
##  5              7 Research &amp; Development Laboratory Technician    
##  6              8 Research &amp; Development Laboratory Technician    
##  7             10 Research &amp; Development Laboratory Technician    
##  8             11 Research &amp; Development Laboratory Technician    
##  9             12 Research &amp; Development Manufacturing Director   
## 10             13 Research &amp; Development Healthcare Representative
##    Attrition
##    &lt;chr&gt;    
##  1 Yes      
##  2 No       
##  3 Yes      
##  4 No       
##  5 No       
##  6 No       
##  7 No       
##  8 No       
##  9 No       
## 10 No       
## # … with 1,240 more rows</code></pre>
<p>In this problem, potential candidates of business units are
<strong>Department</strong> and <strong>Job Roles</strong></p>
<p><strong>Step 2: Objectives</strong></p>
<p>Our business objective is to “<strong>Retain High Performers</strong>”</p>
<pre class="r"><code>dept_job_role_tbl %&gt;% 
  group_by(Attrition) %&gt;% 
  summarize(n=n()) %&gt;% 
  ungroup() %&gt;% 
  mutate(pct=n/sum(n))</code></pre>
<pre><code>## # A tibble: 2 x 3
##   Attrition     n   pct
##   &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;
## 1 No         1049 0.839
## 2 Yes         201 0.161</code></pre>
<p><strong>Step 3: Collect outcomes in terms of feedback. Feedback identifies
problems</strong></p>
<p>This implies 16% Attrition. We should understand that not all attrition
is bad, employee productivity varies. Some attrition is necessary, lets
say poor job employee fit and some attrition is bad, say loosing a high
performer. We need to assess whether this 16% is good attrition or bad
attrition</p>
</div>
<div id="understand-the-drivers" class="section level4">
<h4>Understand the Drivers</h4>
<p><strong>Step 1: Investigate if objectives are being met</strong></p>
<p>Organization has 16% Attrition rate</p>
<p><strong>Step 2: Synthesize outcomes</strong></p>
<p><strong>High counts and High Percentages</strong></p>
<pre class="r"><code>options(dplyr.width = Inf)

dept_job_role_tbl %&gt;% 
  
  group_by(Department,Attrition) %&gt;% 
  summarize(n=n()) %&gt;% 
  ungroup() %&gt;% 
  
  group_by(Department) %&gt;%
  mutate(pct=n/sum(n))</code></pre>
<pre><code>## # A tibble: 6 x 4
## # Groups:   Department [3]
##   Department             Attrition     n   pct
##   &lt;chr&gt;                  &lt;chr&gt;     &lt;int&gt; &lt;dbl&gt;
## 1 Human Resources        No           37 0.755
## 2 Human Resources        Yes          12 0.245
## 3 Research &amp; Development No          721 0.867
## 4 Research &amp; Development Yes         111 0.133
## 5 Sales                  No          291 0.789
## 6 Sales                  Yes          78 0.211</code></pre>
<p>We observe that there may be something going on in department since
there are different percentages of attrition within different
departments.</p>
<p><strong>Attrition by Department &amp; Job Role</strong></p>
<pre class="r"><code>dept_job_role_tbl %&gt;% 
  
  group_by(Department,JobRole, Attrition) %&gt;% 
  summarize(n=n()) %&gt;% 
  ungroup() %&gt;% 
  
  group_by(Department,JobRole) %&gt;%
  mutate(pct=n/sum(n)) %&gt;%
  ungroup() %&gt;%
  
  filter(Attrition %in% &quot;Yes&quot; )</code></pre>
<pre><code>## # A tibble: 10 x 5
##    Department             JobRole                   Attrition     n    pct
##    &lt;chr&gt;                  &lt;chr&gt;                     &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;
##  1 Human Resources        Human Resources           Yes          12 0.308 
##  2 Research &amp; Development Healthcare Representative Yes           8 0.0762
##  3 Research &amp; Development Laboratory Technician     Yes          49 0.219 
##  4 Research &amp; Development Manager                   Yes           2 0.0417
##  5 Research &amp; Development Manufacturing Director    Yes           7 0.0569
##  6 Research &amp; Development Research Director         Yes           2 0.0274
##  7 Research &amp; Development Research Scientist        Yes          43 0.166 
##  8 Sales                  Manager                   Yes           2 0.0645
##  9 Sales                  Sales Executive           Yes          50 0.183 
## 10 Sales                  Sales Representative      Yes          26 0.4</code></pre>
<p>We observe high counts and high percentages by Job Role and Departments,
for ex: high counts in sales department</p>
<p><strong>Step 3: Hypothesize drivers</strong></p>
<p>In this step, we should work with either subject matter expert or
stakeholders to decide what drives the business. In this case, we have
<strong>Job Role</strong> and <strong>Department</strong></p>
</div>
<div id="measure-the-drivers" class="section level4">
<h4>Measure the Drivers</h4>
<p><strong>Step 1: Collect information on employee attrition : Ongoing</strong></p>
<p>In this case, we should ask following:</p>
<ul>
<li>Is data available ?<br />
</li>
<li>Treat information as assets<br />
</li>
<li>Build Strategic Databases</li>
</ul>
<p><strong>Step 2: Develop KPI’s</strong></p>
<ul>
<li>In this step, we can either use <strong>benchmark industry metrics</strong> if
available or<br />
</li>
<li>Develop <strong>internal metrics</strong> using goals</li>
<li><strong>Industry KPI: 8.8% attrition rate</strong><br />
</li>
<li>We will consider this as a conservative KPI that indicates a major
problem if exceeded</li>
</ul>
</div>
<div id="uncover-problems-and-opportunities" class="section level4">
<h4>Uncover Problems and opportunities</h4>
<p><strong>Step 1: Evaluate performance vs KPIs</strong></p>
<p><strong>Attrition Cost</strong> : Cost incurred when an employee leaves</p>
<p>Calculating <strong>Attrition cost</strong> which can be broken down into following:</p>
<ul>
<li>Direct Costs<br />
</li>
<li>Lost Productivity Costs<br />
</li>
<li>Savings of salary and Benefits (Cost Reduction)</li>
</ul>
<p>Cost per employee = Direct Costs + Lost Productivity Costs + Savings of
salary and Benefits</p>
<p>Total Attrition Cost = No. of employees * Cost per employee</p>
<pre class="r"><code>calculate_attrition_cost&lt;- function (
  #Employee  
  n                  = 1,
  salary             = 80000,
  
  
  #Direct Costs
  separation_cost          = 500,
  vacancy_cost             = 10000,
  acquisition_cost         = 4900,
  placement_cost           = 3500, 
  
  
  #Productivity Costs
  net_revenue_per_employee = 250000, 
  workdays_per_year        = 240,
  workdays_position_open   = 40,
  workdays_onboarding      = 60, 
  onboarding_efficiency    = 0.50
  
){
  
  #Direct Costs
  direct_cost &lt;- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)
  
  #Lost Productivity Costs
  productivity_cost &lt;- net_revenue_per_employee /workdays_per_year *
    (workdays_position_open + workdays_onboarding * onboarding_efficiency)
  
  # Savings of salary and Benefits (Cost Reduction)
  salary_benefit_reduction &lt;- salary / workdays_per_year * workdays_position_open
  
  #Estimated Turnover per Employee 
  cost_per_employee &lt;- direct_cost + productivity_cost - salary_benefit_reduction
  
  # Total Cost of Employee Turnover 
  total_cost &lt;- n * cost_per_employee
  
  return(total_cost)
}</code></pre>
<pre class="r"><code>calculate_attrition_cost()</code></pre>
<pre><code>## [1] 78483.33</code></pre>
<p><strong>Attriton Cost per person is : 78483.33</strong></p>
<p><strong>Attriton Cost for 200 employees is : 15,696,667 ~ 15.7 million</strong></p>
<p>This implies that it is a <strong>$15.7 million/year</strong> problem for an
organisation that loses 200 employees per year</p>
<p><strong>Workflow of Attrition</strong></p>
<pre class="r"><code>count_to_pct &lt;-  function(data,...,col=n){
  
  grouping_vars_expr &lt;- quos(...) 
  col_expr &lt;- enquo(col)  
  
  ret &lt;- data %&gt;%
    group_by(!!! grouping_vars_expr) %&gt;%
    mutate(pct = (!! col_expr) /sum(!! col_expr)) %&gt;%
    ungroup()
  
  return(ret)
}

assess_attrition &lt;- function(data, attrition_col, attrition_value, baseline_pct) {
      
      attrition_col_expr &lt;- enquo(attrition_col)
        
      data %&gt;%
            filter((!! attrition_col_expr) %in% attrition_value ) %&gt;%
            arrange(desc(pct)) %&gt;%
            mutate (
                above_industry_avg =  case_when(
                pct&gt; baseline_pct ~ &quot;Yes&quot; ,
                TRUE ~ &quot;No&quot;
            )
          )
            
}</code></pre>
<p><strong>Step 2: Highlight potential problem areas</strong></p>
<p>Calculating Attrition cost by <strong>Department</strong> &amp; <strong>JobRole</strong></p>
<pre class="r"><code>dept_job_role_tbl %&gt;% 
  
  count(Department, JobRole, Attrition) %&gt;% 
  
  count_to_pct(Department, JobRole)%&gt;%
  
  assess_attrition(Attrition, attrition_value = &quot;Yes&quot;, baseline_pct = 0.088) %&gt;% 
  
  mutate(cost_of_attrition= calculate_attrition_cost(n=n,80000))</code></pre>
<pre><code>## # A tibble: 10 x 7
##    Department             JobRole                   Attrition     n    pct
##    &lt;chr&gt;                  &lt;chr&gt;                     &lt;chr&gt;     &lt;int&gt;  &lt;dbl&gt;
##  1 Sales                  Sales Representative      Yes          26 0.4   
##  2 Human Resources        Human Resources           Yes          12 0.308 
##  3 Research &amp; Development Laboratory Technician     Yes          49 0.219 
##  4 Sales                  Sales Executive           Yes          50 0.183 
##  5 Research &amp; Development Research Scientist        Yes          43 0.166 
##  6 Research &amp; Development Healthcare Representative Yes           8 0.0762
##  7 Sales                  Manager                   Yes           2 0.0645
##  8 Research &amp; Development Manufacturing Director    Yes           7 0.0569
##  9 Research &amp; Development Manager                   Yes           2 0.0417
## 10 Research &amp; Development Research Director         Yes           2 0.0274
##    above_industry_avg cost_of_attrition
##    &lt;chr&gt;                          &lt;dbl&gt;
##  1 Yes                         2040567.
##  2 Yes                          941800.
##  3 Yes                         3845683.
##  4 Yes                         3924167.
##  5 Yes                         3374783.
##  6 No                           627867.
##  7 No                           156967.
##  8 No                           549383.
##  9 No                           156967.
## 10 No                           156967.</code></pre>
<p>We can observe that <strong>Sales Representative</strong> (~2 million) and
<strong>Laboratory Technician</strong> (~3.8 million) have higher cost of attrition
within Sales and Research &amp; Development department even with lower
attrition rate.</p>
<p>10% reduction in turnover saves the organization <strong>$1.6 million /
year</strong>. So, this problem is worth solving for an organization.</p>
<p><strong>Step 3: Review process and consider what could be missed or needed to
answer questions</strong></p>
<p><strong>Visualizing of Attrition cost by JobRole</strong></p>
<pre class="r"><code>plot_attrition &lt;- function(data,...,.value,
                           fct_reorder = TRUE,
                           fct_rev = FALSE,
                           include_lbl = TRUE,
                           color = palette_light()[[1]],
                           units = c(&quot;0&quot;, &quot;K&quot;, &quot;M&quot;)) {
  
  
  #Inputs
  group_vars_expr &lt;- quos(...)
  if(length(group_vars_expr)==0)
    group_vars_expr &lt;- quos(rlang::sym(colnames(data)[[1]]))
  
  value_expr &lt;- enquo(.value)
  value_name &lt;- quo_name(value_expr)
  
  units_val &lt;- switch(units[[1]],
                      &quot;M&quot; = 1e6,
                      &quot;K&quot; = 1e3,
                      &quot;0&quot; = 1) 
  if(units[[1]] == &quot;0&quot;) units &lt;- &quot;&quot;
  
  #Data Manipulation
  usd &lt;- scales::dollar_format(prefix = &quot;$&quot; , largest_with_cents = 1e3)
  
  data_manipulated &lt;- data %&gt;%
    mutate(name = str_c(!!! group_vars_expr, sep = &quot;: &quot;) %&gt;% as_factor()) %&gt;%
    mutate(value_text = str_c(usd(!! value_expr / units_val ), units[[1]] , sep = &quot;&quot;))
  
  if(fct_reorder){
    data_manipulated &lt;-  data_manipulated %&gt;% 
      mutate(name = forcats::fct_reorder(name, !! value_expr)) %&gt;% 
      arrange(name)
  }
    
  if(fct_rev){
    data_manipulated &lt;-  data_manipulated %&gt;% 
      mutate(name = forcats::fct_rev(name)) %&gt;% 
      arrange(name)
  }
  
  
  #Visualization
  
  g &lt;- data_manipulated  %&gt;%
    ggplot(aes_string(x = value_name, y = &quot;name&quot;)) +
    geom_segment(aes(xend = 0, yend = name), color = color) +
    geom_point(aes_string(size = value_name), color = color) +
    scale_x_continuous(labels = scales::dollar) +
    theme_tq() +
    scale_size(range = c(3, 5)) +
    theme(legend.position = &quot;none&quot;)
  
  
  if(include_lbl){
    g &lt;- g + 
    geom_label(
      aes_string(label = &quot;value_text&quot;, size = value_name),
      hjust = &quot;inward&quot;,
      color = color
    )
  } 
  
  return(g)

}</code></pre>
<p>It looks like <strong>Sales Executive</strong> and <strong>Laboratory Technician</strong> are
biggest sales driver of cost</p>
<pre class="r"><code>dept_job_role_tbl %&gt;% 
  
  count(JobRole, Attrition) %&gt;% 
  count_to_pct(JobRole)%&gt;%
  assess_attrition(Attrition, attrition_value = &quot;Yes&quot;, baseline_pct = 0.088) %&gt;% 
  mutate(cost_of_attrition= calculate_attrition_cost(n=n,80000)
  ) %&gt;% 
  
  plot_attrition(JobRole, .value = cost_of_attrition, units= &quot;M&quot;) +
  labs(title = &quot;Estimated Cost of Attrition By Job Role&quot;,
       x =  &quot;Cost of Attrition&quot; ,
       y=&quot;&quot;,
       subtitle = &quot;Looks like Sales Executive and Laboratory Technician are biggest sales driver of cost&quot;)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-12-1.png" /></p>
</div>
</div>
<div id="phase-2-data-understanding" class="section level3">
<h3>Phase 2 : Data Understanding</h3>
<p><strong>Loading datasets</strong></p>
<pre class="r"><code>#Load Data
path_train &lt;- &quot;00_Data/telco_train.xlsx&quot;
path_test &lt;- &quot;00_Data/telco_test.xlsx&quot;

path_data_definitions &lt;- &quot;00_Data/telco_data_definitions.xlsx&quot;

train_raw_tbl &lt;- read_excel(path_train, sheet=1)
test_raw_tbl &lt;- read_excel(path_test, sheet=1)

definitions_raw_tbl &lt;- read_excel(path_data_definitions, sheet = 1, col_names = FALSE)</code></pre>
<p><strong>Reading data definitions table</strong></p>
<pre class="r"><code>definitions_raw_tbl </code></pre>
<pre><code>## # A tibble: 35 x 2
##    X__1                    X__2             
##    &lt;chr&gt;                   &lt;chr&gt;            
##  1 Education               1 &#39;Below College&#39;
##  2 &lt;NA&gt;                    2 &#39;College&#39;      
##  3 &lt;NA&gt;                    3 &#39;Bachelor&#39;     
##  4 &lt;NA&gt;                    4 &#39;Master&#39;       
##  5 &lt;NA&gt;                    5 &#39;Doctor&#39;       
##  6 &lt;NA&gt;                    &lt;NA&gt;             
##  7 EnvironmentSatisfaction 1 &#39;Low&#39;          
##  8 &lt;NA&gt;                    2 &#39;Medium&#39;       
##  9 &lt;NA&gt;                    3 &#39;High&#39;         
## 10 &lt;NA&gt;                    4 &#39;Very High&#39;    
## # … with 25 more rows</code></pre>
<p><strong>Reading training dataset</strong></p>
<pre class="r"><code>train_raw_tbl %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 1,250
## Variables: 35
## $ Age                      &lt;dbl&gt; 41, 49, 37, 33, 27, 32, 59, 30, 38, 36,…
## $ Attrition                &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;…
## $ BusinessTravel           &lt;chr&gt; &quot;Travel_Rarely&quot;, &quot;Travel_Frequently&quot;, &quot;…
## $ DailyRate                &lt;dbl&gt; 1102, 279, 1373, 1392, 591, 1005, 1324,…
## $ Department               &lt;chr&gt; &quot;Sales&quot;, &quot;Research &amp; Development&quot;, &quot;Res…
## $ DistanceFromHome         &lt;dbl&gt; 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15…
## $ Education                &lt;dbl&gt; 2, 1, 2, 4, 1, 2, 3, 1, 3, 3, 3, 2, 1, …
## $ EducationField           &lt;chr&gt; &quot;Life Sciences&quot;, &quot;Life Sciences&quot;, &quot;Othe…
## $ EmployeeCount            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
## $ EmployeeNumber           &lt;dbl&gt; 1, 2, 4, 5, 7, 8, 10, 11, 12, 13, 14, 1…
## $ EnvironmentSatisfaction  &lt;dbl&gt; 2, 3, 4, 4, 1, 4, 3, 4, 4, 3, 1, 4, 1, …
## $ Gender                   &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Ma…
## $ HourlyRate               &lt;dbl&gt; 94, 61, 92, 56, 40, 79, 81, 67, 44, 94,…
## $ JobInvolvement           &lt;dbl&gt; 3, 2, 2, 3, 3, 3, 4, 3, 2, 3, 4, 2, 3, …
## $ JobLevel                 &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, …
## $ JobRole                  &lt;chr&gt; &quot;Sales Executive&quot;, &quot;Research Scientist&quot;…
## $ JobSatisfaction          &lt;dbl&gt; 4, 2, 3, 3, 2, 4, 1, 3, 3, 3, 2, 3, 3, …
## $ MaritalStatus            &lt;chr&gt; &quot;Single&quot;, &quot;Married&quot;, &quot;Single&quot;, &quot;Married…
## $ MonthlyIncome            &lt;dbl&gt; 5993, 5130, 2090, 2909, 3468, 3068, 267…
## $ MonthlyRate              &lt;dbl&gt; 19479, 24907, 2396, 23159, 16632, 11864…
## $ NumCompaniesWorked       &lt;dbl&gt; 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, …
## $ Over18                   &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;,…
## $ OverTime                 &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, …
## $ PercentSalaryHike        &lt;dbl&gt; 11, 23, 15, 11, 12, 13, 20, 22, 21, 13,…
## $ PerformanceRating        &lt;dbl&gt; 3, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, …
## $ RelationshipSatisfaction &lt;dbl&gt; 1, 4, 2, 3, 4, 3, 1, 2, 2, 2, 3, 4, 4, …
## $ StandardHours            &lt;dbl&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,…
## $ StockOptionLevel         &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, …
## $ TotalWorkingYears        &lt;dbl&gt; 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10…
## $ TrainingTimesLastYear    &lt;dbl&gt; 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, …
## $ WorkLifeBalance          &lt;dbl&gt; 1, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, …
## $ YearsAtCompany           &lt;dbl&gt; 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5,…
## $ YearsInCurrentRole       &lt;dbl&gt; 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, …
## $ YearsSinceLastPromotion  &lt;dbl&gt; 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, …
## $ YearsWithCurrManager     &lt;dbl&gt; 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, …</code></pre>
<p><strong>Reading test dataset</strong></p>
<pre class="r"><code>test_raw_tbl  %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 220
## Variables: 35
## $ Age                      &lt;dbl&gt; 30, 55, 54, 49, 43, 58, 34, 37, 35, 50,…
## $ Attrition                &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;N…
## $ BusinessTravel           &lt;chr&gt; &quot;Travel_Rarely&quot;, &quot;Non-Travel&quot;, &quot;Travel_…
## $ DailyRate                &lt;dbl&gt; 1339, 177, 685, 1023, 807, 848, 1346, 1…
## $ Department               &lt;chr&gt; &quot;Sales&quot;, &quot;Research &amp; Development&quot;, &quot;Res…
## $ DistanceFromHome         &lt;dbl&gt; 5, 8, 3, 2, 17, 23, 19, 5, 1, 1, 1, 10,…
## $ Education                &lt;dbl&gt; 3, 1, 3, 3, 3, 4, 2, 2, 3, 3, 4, 4, 3, …
## $ EducationField           &lt;chr&gt; &quot;Life Sciences&quot;, &quot;Medical&quot;, &quot;Life Scien…
## $ EmployeeCount            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
## $ EmployeeNumber           &lt;dbl&gt; 228, 1278, 1250, 2065, 1767, 1308, 18, …
## $ EnvironmentSatisfaction  &lt;dbl&gt; 2, 4, 4, 4, 3, 1, 2, 4, 4, 4, 2, 4, 3, …
## $ Gender                   &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Male…
## $ HourlyRate               &lt;dbl&gt; 41, 37, 85, 63, 38, 88, 93, 61, 60, 95,…
## $ JobInvolvement           &lt;dbl&gt; 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 1, 2, 3, …
## $ JobLevel                 &lt;dbl&gt; 3, 4, 4, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, …
## $ JobRole                  &lt;chr&gt; &quot;Sales Executive&quot;, &quot;Healthcare Represen…
## $ JobSatisfaction          &lt;dbl&gt; 4, 2, 4, 2, 3, 3, 4, 4, 4, 3, 3, 4, 3, …
## $ MaritalStatus            &lt;chr&gt; &quot;Married&quot;, &quot;Divorced&quot;, &quot;Married&quot;, &quot;Marr…
## $ MonthlyIncome            &lt;dbl&gt; 9419, 13577, 17779, 5390, 2437, 2372, 2…
## $ MonthlyRate              &lt;dbl&gt; 8053, 25592, 23474, 13243, 15587, 26076…
## $ NumCompaniesWorked       &lt;dbl&gt; 2, 1, 3, 2, 9, 1, 0, 7, 0, 0, 1, 2, 2, …
## $ Over18                   &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;,…
## $ OverTime                 &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;…
## $ PercentSalaryHike        &lt;dbl&gt; 12, 15, 14, 14, 16, 12, 11, 16, 12, 12,…
## $ PerformanceRating        &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …
## $ RelationshipSatisfaction &lt;dbl&gt; 3, 4, 1, 4, 4, 4, 3, 3, 2, 1, 2, 3, 3, …
## $ StandardHours            &lt;dbl&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,…
## $ StockOptionLevel         &lt;dbl&gt; 1, 1, 0, 0, 1, 2, 1, 2, 1, 1, 0, 3, 1, …
## $ TotalWorkingYears        &lt;dbl&gt; 12, 34, 36, 17, 6, 2, 3, 8, 10, 19, 1, …
## $ TrainingTimesLastYear    &lt;dbl&gt; 2, 3, 2, 3, 4, 3, 2, 2, 0, 3, 3, 5, 5, …
## $ WorkLifeBalance          &lt;dbl&gt; 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, …
## $ YearsAtCompany           &lt;dbl&gt; 10, 33, 10, 9, 1, 2, 2, 6, 9, 18, 1, 1,…
## $ YearsInCurrentRole       &lt;dbl&gt; 9, 9, 9, 6, 0, 2, 2, 2, 7, 7, 0, 0, 7, …
## $ YearsSinceLastPromotion  &lt;dbl&gt; 7, 15, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0,…
## $ YearsWithCurrManager     &lt;dbl&gt; 4, 0, 9, 8, 0, 2, 2, 4, 0, 13, 0, 0, 8,…</code></pre>
</div>
<div id="exploratory-data-analysis" class="section level3">
<h3>Exploratory Data Analysis</h3>
<div id="step-1-data-summarisation" class="section level4">
<h4>Step 1 : Data Summarisation</h4>
<p><strong>Summarising data with skim</strong></p>
<p>It allows you to skim useful summary statistics in console or use those
statitics in a pipeable workflow</p>
<pre class="r"><code>skim(train_raw_tbl)</code></pre>
<pre><code>## Skim summary statistics
##  n obs: 1250 
##  n variables: 35 
## 
## ── Variable type:character ───────────────────────────────────────────────────────────────────────────────────────────
##        variable missing complete    n min max empty n_unique
##       Attrition       0     1250 1250   2   3     0        2
##  BusinessTravel       0     1250 1250  10  17     0        3
##      Department       0     1250 1250   5  22     0        3
##  EducationField       0     1250 1250   5  16     0        6
##          Gender       0     1250 1250   4   6     0        2
##         JobRole       0     1250 1250   7  25     0        9
##   MaritalStatus       0     1250 1250   6   8     0        3
##          Over18       0     1250 1250   1   1     0        1
##        OverTime       0     1250 1250   2   3     0        2
## 
## ── Variable type:numeric ─────────────────────────────────────────────────────────────────────────────────────────────
##                  variable missing complete    n     mean      sd   p0
##                       Age       0     1250 1250    36.95    9.08   18
##                 DailyRate       0     1250 1250   799.74  403.52  102
##          DistanceFromHome       0     1250 1250     9.1     8.1     1
##                 Education       0     1250 1250     2.91    1.03    1
##             EmployeeCount       0     1250 1250     1       0       1
##            EmployeeNumber       0     1250 1250  1022.71  600.42    1
##   EnvironmentSatisfaction       0     1250 1250     2.71    1.09    1
##                HourlyRate       0     1250 1250    65.82   20.14   30
##            JobInvolvement       0     1250 1250     2.75    0.71    1
##                  JobLevel       0     1250 1250     2.08    1.12    1
##           JobSatisfaction       0     1250 1250     2.73    1.11    1
##             MonthlyIncome       0     1250 1250  6569.4  4777.89 1009
##               MonthlyRate       0     1250 1250 14294.5  7071.61 2097
##        NumCompaniesWorked       0     1250 1250     2.71    2.49    0
##         PercentSalaryHike       0     1250 1250    15.23    3.69   11
##         PerformanceRating       0     1250 1250     3.16    0.36    3
##  RelationshipSatisfaction       0     1250 1250     2.71    1.09    1
##             StandardHours       0     1250 1250    80       0      80
##          StockOptionLevel       0     1250 1250     0.8     0.86    0
##         TotalWorkingYears       0     1250 1250    11.4     7.79    0
##     TrainingTimesLastYear       0     1250 1250     2.81    1.29    0
##           WorkLifeBalance       0     1250 1250     2.76    0.71    1
##            YearsAtCompany       0     1250 1250     7.09    6.21    0
##        YearsInCurrentRole       0     1250 1250     4.26    3.66    0
##   YearsSinceLastPromotion       0     1250 1250     2.21    3.2     0
##      YearsWithCurrManager       0     1250 1250     4.16    3.59    0
##      p25     p50      p75  p100     hist
##    30       36      43       60 ▂▃▆▇▅▃▂▂
##   464      798    1156     1498 ▇▇▇▇▇▇▇▇
##     2        7      13.75    29 ▇▅▃▁▁▁▁▁
##     2        3       4        5 ▂▅▁▇▁▆▁▁
##     1        1       1        1 ▁▁▁▇▁▁▁▁
##   486.25  1020.5  1553.75  2068 ▇▇▇▇▇▇▇▇
##     2        3       4        4 ▅▁▅▁▁▇▁▇
##    48       66      83      100 ▆▇▇▇▆▇▇▇
##     2        3       3        4 ▁▁▃▁▁▇▁▂
##     1        2       3        5 ▇▇▁▃▁▂▁▁
##     2        3       4        4 ▅▁▅▁▁▇▁▇
##  2935.25  4903.5  8395    19999 ▇▇▃▂▁▁▁▂
##  8191.25 14328   20337.25 26999 ▇▇▇▇▇▇▇▆
##     1        2       4        9 ▇▂▂▂▁▁▁▁
##    12       14      18       25 ▇▇▃▃▂▂▂▁
##     3        3       3        4 ▇▁▁▁▁▁▁▂
##     2        3       4        4 ▅▁▆▁▁▇▁▇
##    80       80      80       80 ▁▁▁▇▁▁▁▁
##     0        1       1        3 ▇▁▇▁▁▂▁▁
##     6       10      15       40 ▃▇▂▂▂▁▁▁
##     2        3       3        6 ▁▁▇▇▁▂▂▁
##     2        3       3        4 ▁▁▃▁▁▇▁▂
##     3        5      10       40 ▇▅▁▁▁▁▁▁
##     2        3       7       18 ▇▃▁▅▁▁▁▁
##     0        1       3       15 ▇▂▁▁▁▁▁▁
##     2        3       7       17 ▇▃▁▃▁▁▁▁</code></pre>
<p>Separating your data by data type is a great way to investigate
properties of data :</p>
<p><strong>1. Character / Categorical</strong><br />
<strong>2. Numeric</strong></p>
<p><strong>Exploring features which are of Character Data Type</strong></p>
<pre class="r"><code># Character Data Type
train_raw_tbl %&gt;% 
  select_if(is.character) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Observations: 1,250
## Variables: 9
## $ Attrition      &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;,…
## $ BusinessTravel &lt;chr&gt; &quot;Travel_Rarely&quot;, &quot;Travel_Frequently&quot;, &quot;Travel_Rar…
## $ Department     &lt;chr&gt; &quot;Sales&quot;, &quot;Research &amp; Development&quot;, &quot;Research &amp; De…
## $ EducationField &lt;chr&gt; &quot;Life Sciences&quot;, &quot;Life Sciences&quot;, &quot;Other&quot;, &quot;Life …
## $ Gender         &lt;chr&gt; &quot;Female&quot;, &quot;Male&quot;, &quot;Male&quot;, &quot;Female&quot;, &quot;Male&quot;, &quot;Male…
## $ JobRole        &lt;chr&gt; &quot;Sales Executive&quot;, &quot;Research Scientist&quot;, &quot;Laborat…
## $ MaritalStatus  &lt;chr&gt; &quot;Single&quot;, &quot;Married&quot;, &quot;Single&quot;, &quot;Married&quot;, &quot;Marrie…
## $ Over18         &lt;chr&gt; &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;, &quot;Y&quot;,…
## $ OverTime       &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No…</code></pre>
<p>Unique values in each character type feature and proportion of values in
it</p>
<pre class="r"><code>train_raw_tbl %&gt;% 
  select_if(is.character) %&gt;% 
  map(~ table(.) %&gt;% prop.table())</code></pre>
<pre><code>## $Attrition
## .
##     No    Yes 
## 0.8392 0.1608 
## 
## $BusinessTravel
## .
##        Non-Travel Travel_Frequently     Travel_Rarely 
##            0.0992            0.1912            0.7096 
## 
## $Department
## .
##        Human Resources Research &amp; Development                  Sales 
##                 0.0392                 0.6656                 0.2952 
## 
## $EducationField
## .
##  Human Resources    Life Sciences        Marketing          Medical 
##           0.0184           0.4160           0.1080           0.3136 
##            Other Technical Degree 
##           0.0528           0.0912 
## 
## $Gender
## .
## Female   Male 
## 0.3928 0.6072 
## 
## $JobRole
## .
## Healthcare Representative           Human Resources 
##                    0.0840                    0.0312 
##     Laboratory Technician                   Manager 
##                    0.1792                    0.0712 
##    Manufacturing Director         Research Director 
##                    0.0984                    0.0584 
##        Research Scientist           Sales Executive 
##                    0.2072                    0.2184 
##      Sales Representative 
##                    0.0520 
## 
## $MaritalStatus
## .
## Divorced  Married   Single 
##   0.2200   0.4608   0.3192 
## 
## $Over18
## .
## Y 
## 1 
## 
## $OverTime
## .
##     No    Yes 
## 0.7152 0.2848</code></pre>
<p><strong>Exploring features which are of Numeric Data Type</strong></p>
<pre class="r"><code># Numeric Data Type

train_raw_tbl %&gt;% 
  select_if(is.numeric) %&gt;% 
  map(~ unique(.) %&gt;% length())</code></pre>
<pre><code>## $Age
## [1] 43
## 
## $DailyRate
## [1] 808
## 
## $DistanceFromHome
## [1] 29
## 
## $Education
## [1] 5
## 
## $EmployeeCount
## [1] 1
## 
## $EmployeeNumber
## [1] 1250
## 
## $EnvironmentSatisfaction
## [1] 4
## 
## $HourlyRate
## [1] 71
## 
## $JobInvolvement
## [1] 4
## 
## $JobLevel
## [1] 5
## 
## $JobSatisfaction
## [1] 4
## 
## $MonthlyIncome
## [1] 1161
## 
## $MonthlyRate
## [1] 1217
## 
## $NumCompaniesWorked
## [1] 10
## 
## $PercentSalaryHike
## [1] 15
## 
## $PerformanceRating
## [1] 2
## 
## $RelationshipSatisfaction
## [1] 4
## 
## $StandardHours
## [1] 1
## 
## $StockOptionLevel
## [1] 4
## 
## $TotalWorkingYears
## [1] 40
## 
## $TrainingTimesLastYear
## [1] 7
## 
## $WorkLifeBalance
## [1] 4
## 
## $YearsAtCompany
## [1] 37
## 
## $YearsInCurrentRole
## [1] 19
## 
## $YearsSinceLastPromotion
## [1] 16
## 
## $YearsWithCurrManager
## [1] 18</code></pre>
<p>We should note that numeric varibales that are lower in levels are
likely to be discrete and that are higher in levels are likely to be
continuous, filtering features where levels are less than 10, those
features are likely to be factor.</p>
<pre class="r"><code>train_raw_tbl %&gt;% 
  select_if(is.numeric) %&gt;% 
  map_df(~ unique(.) %&gt;% length()) %&gt;%
  gather() %&gt;% 
  arrange(value) %&gt;%
  filter(value &lt;= 10)</code></pre>
<pre><code>## # A tibble: 13 x 2
##    key                      value
##    &lt;chr&gt;                    &lt;int&gt;
##  1 EmployeeCount                1
##  2 StandardHours                1
##  3 PerformanceRating            2
##  4 EnvironmentSatisfaction      4
##  5 JobInvolvement               4
##  6 JobSatisfaction              4
##  7 RelationshipSatisfaction     4
##  8 StockOptionLevel             4
##  9 WorkLifeBalance              4
## 10 Education                    5
## 11 JobLevel                     5
## 12 TrainingTimesLastYear        7
## 13 NumCompaniesWorked          10</code></pre>
</div>
<div id="step-2-data-visualization" class="section level4">
<h4>Step 2 : Data Visualization</h4>
<p>Visualising the feature-target interactions with
<a href="http://ggobi.github.io/ggally/">GGally</a> package</p>
<pre class="r"><code>plot_ggpairs &lt;- function(data, color = NULL, density_alpha = 0.5) {
  
  color_expr &lt;- enquo(color)
  
  if (rlang::quo_is_null(color_expr)) {
    
    g &lt;- data %&gt;%
      ggpairs(lower = &quot;blank&quot;) 
    
  } else {
    
    color_name &lt;- quo_name(color_expr)
    
    g &lt;- data %&gt;%
      ggpairs(mapping = aes_string(color = color_name), 
              lower = &quot;blank&quot;, legend = 1,
              diag = list(continuous = wrap(&quot;densityDiag&quot;, 
                                            alpha = density_alpha))) +
      theme(legend.position = &quot;bottom&quot;)
  }
  
  return(g)
  
}</code></pre>
<p><strong>Exploring features Visually by Category</strong></p>
<p>Based on intuition we can divide the features into different categories,
which would helps us doing a better analysis.</p>
<p><strong>1. Descriptive Features : Age, Gender, MaritalStatus,
NumCompaniesWorked, Over18, DistanceFromHome</strong></p>
<pre class="r"><code>train_raw_tbl %&gt;% 
  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %&gt;% 
  plot_ggpairs(color = Attrition)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-23-1.png" /></p>
<ul>
<li>It looks like younger people are leaving the company more so than
older people.<br />
</li>
<li>It seems like people who are living between 20-30 miles away from
company are leaving<br />
as compared to people who are within 10 miles stay at company
(skewed distribution).<br />
</li>
<li>It looks like there is an upward trend of attrition (Yes) in marital
status in Single, Married and Divorced while on the other side its
small, large and medium.</li>
</ul>
<p><strong>2. Employment Features : Department, JobRole , JobLevel,
JobInvolvement, JobSatisfaction</strong></p>
<pre class="r"><code>train_raw_tbl %&gt;% 
  select(Attrition, contains(&quot;employee&quot;), contains(&quot;department&quot;), contains(&quot;job&quot;) ) %&gt;% 
  plot_ggpairs(color = Attrition)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-24-1.png" /></p>
<ul>
<li>Employee count won’t add any value to predictive power of model.<br />
</li>
<li>Also, Employee number won’t help but it would help in identifying a
particular employee.<br />
</li>
<li>If you look at Department wise R &amp; D has higher attrition, then
Sales and then HR.<br />
</li>
<li>On the contrary, density for JobInvolvement type 3 has a higher
spike for people who are staying with relation to people who are
leaving or conversely JobInvolvement type 3 has less population as
compared to lower levels. So, people in 1 and 2 type are more likely
to leave as compared to type 3 &amp; 4.<br />
</li>
<li>There are lot more people leaving in Joblevel 1 and lot more people
are staying who are in Joblevel 2nd</li>
<li>We observe that certain JobRoles has higher attrition rate.<br />
</li>
<li>It looks like people who are at Jobsatisfaction level 1 are more
likely to leave than others.</li>
</ul>
<p><strong>3. Compensation Features : DailyRate , HourlyRate, MonthlyRate,
MonthlyIncome, PercentSalaryHike, StockOptionLevel</strong></p>
<pre class="r"><code>train_raw_tbl %&gt;% 
  select(Attrition, contains(&quot;income&quot;), contains(&quot;rate&quot;), contains(&quot;salary&quot;), contains(&quot;stock&quot;) ) %&gt;% 
  plot_ggpairs(color = Attrition)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-25-1.png" /></p>
<ul>
<li>Persons who have less MonthlyIncome are more likely to leave.<br />
</li>
<li>Persons who have less DailyRate are more likely to leave.<br />
</li>
<li>It looks like people with less percent salary hike are more likely
to leave.<br />
</li>
<li>We observe that people with StockOptionLevel 0 i.e no stocks are
more likely to leave than others.</li>
</ul>
<p><strong>4. Survey Results : EnvironmentSatisfaction, JobSatisfaction ,
RelationshipSatisfaction, WorkLifeBalance</strong></p>
<pre class="r"><code>train_raw_tbl %&gt;% 
  select(Attrition, contains(&quot;satisfaction&quot;), contains(&quot;life&quot;) ) %&gt;% 
  plot_ggpairs(color = Attrition)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-26-1.png" /></p>
<ul>
<li>It looks like density of environment satisfaction 1 has higher spike
for attrition i.e those people are more likely to leave.<br />
</li>
<li>People with less JobSatisfaction are more likely to leave.<br />
</li>
<li>People in RelationshipSatisfaction level 1 are more likely to leave
than the others.<br />
</li>
<li>It seems like density for WorkLifeBalance type 2 &amp; 3 has a higher
spike for people who are staying with relation to people who are
leaving or conversely WorkLifeBalance type 1 &amp; 4 has less population
as compared to lower levels. So, people in 1 and 4 type are more
likely to leave as compared to type 2 &amp; 3.</li>
</ul>
<p><strong>5. Performance Data : JobInvolvement, PerformanceRating</strong></p>
<pre class="r"><code>train_raw_tbl %&gt;% 
  select(Attrition, contains(&quot;performance&quot;), contains(&quot;involvement&quot;)) %&gt;% 
  plot_ggpairs(color = Attrition)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-27-1.png" /></p>
<ul>
<li>People either get a performance rating of either 3 and 4.<br />
</li>
<li>It looks like average performance rating at the company is 3.<br />
</li>
<li>People are more likely to stay in both the cases or less people
leave who have a rating of 4 (less population with rating 4).</li>
</ul>
<p><strong>6. Work - Life Features : BusinessTravel , OverTime</strong></p>
<pre class="r"><code>train_raw_tbl %&gt;% 
  select(Attrition, contains(&quot;overtime&quot;), contains(&quot;travel&quot;)) %&gt;% 
  plot_ggpairs(color = Attrition)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-28-1.png" /></p>
<ul>
<li>People who work overtime are more likely to leave as compared to
those who don’t.<br />
</li>
<li>People who business travel frequently and rarely are more likely to
leave.</li>
</ul>
<p><strong>7. Training &amp; Education : Education, EducationField ,
TrainingTimesLastYear</strong></p>
<pre class="r"><code>train_raw_tbl %&gt;% 
  select(Attrition, contains(&quot;training&quot;), contains(&quot;education&quot;)) %&gt;% 
  plot_ggpairs(color = Attrition)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-29-1.png" /></p>
<ul>
<li>People with TrainingTimesLastYear &gt; 0 are more likely to stay
than others .Also, in TrainingTimesLastYear type 4 are more likely
to leave.<br />
</li>
<li>People who comes from LifeSciences, Medical, Technical Degree &amp;
Marketing are more likely to leave the company.<br />
</li>
<li>People with Education level 4 and 5 are more likely to leave.</li>
</ul>
<p><strong>8. Time - Based Features: TotalWorkingYears , YearsAtCompany ,
YearsInCurrentRole , YearsSinceLastPromotion, YearsWithCurrManager</strong></p>
<pre class="r"><code>train_raw_tbl %&gt;% 
  select(Attrition, contains(&quot;years&quot;)) %&gt;% 
  plot_ggpairs(color = Attrition)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-30-1.png" /></p>
<ul>
<li>People with greater TotalWorkingYears are more likely to stay.<br />
</li>
<li>People who have less YearsAtCompany are more likely to leave.<br />
</li>
<li>It looks like people with YearsInCurrentRole has a skewed
distribution although people with less YearsInCurrentRole are more
likely to stay.<br />
</li>
<li>People with more YearsSinceLastPromotion are more likely to leave.<br />
</li>
<li>People whose YearsWithCurrManager are within 2 years are more likely
to leave.</li>
</ul>
</div>
</div>
<div id="phase-3-data-preparation" class="section level3">
<h3>Phase 3 : Data Preparation</h3>
</div>
<div id="data-preparation---human-readable" class="section level3">
<h3>Data Preparation - Human Readable</h3>
<p>Function <strong>process_hr_data_readable</strong> fills up empty values in data
definition table and re-levels features like Business Travel and Marital
Status in correct order.</p>
<pre class="r"><code>process_hr_data_readable &lt;- function(data, definitions_tbl) {
  
definitions_list &lt;- definitions_tbl %&gt;%
    fill(X__1, .direction = &quot;down&quot;) %&gt;%
    filter(!is.na(X__2)) %&gt;%
    separate(X__2,into = c(&quot;key&quot;, &quot;value&quot;),sep = &quot; &#39;&quot;,remove = TRUE) %&gt;%
    rename(column_name = X__1) %&gt;%
    mutate(key = as.numeric(key)) %&gt;%
    mutate(value = value %&gt;% str_replace(pattern = &quot;&#39;&quot;, replacement = &quot;&quot;)) %&gt;%
    split(.$column_name) %&gt;%
    map( ~ select(.,-column_name)) %&gt;%
    map( ~ mutate(., value = as_factor(value)))
  
  for (i in seq_along(definitions_list)) {
    list_name &lt;- names(definitions_list)[i]
    colnames(definitions_list[[i]]) &lt;-c(list_name, paste0(list_name, &quot;_value&quot;))
  }
  
  data_merged_tbl &lt;- list(HR_Data = data) %&gt;%
    append(definitions_list, after = 1) %&gt;%
    reduce(left_join) %&gt;%
    select(-one_of(names(definitions_list))) %&gt;%
    set_names(str_replace_all(names(.), pattern = &quot;_value&quot;, replacement = &quot;&quot;)) %&gt;%
    select(sort(names(.))) %&gt;%
    mutate_if(is.character, as.factor) %&gt;%
    mutate(
      BusinessTravel = BusinessTravel %&gt;% fct_relevel(&quot;Non-Travel&quot;, &quot;Travel_Rarely&quot;, &quot;Travel_Frequently&quot;),
      MaritalStatus  = MaritalStatus %&gt;% fct_relevel(&quot;Single&quot;, &quot;Married&quot;, &quot;Divorced&quot;)
    )
  
  return(data_merged_tbl)
  
} 

process_hr_data_readable(train_raw_tbl,definitions_tbl = definitions_raw_tbl) %&gt;%
  glimpse()</code></pre>
<pre><code>## Observations: 1,250
## Variables: 35
## $ Age                      &lt;dbl&gt; 41, 49, 37, 33, 27, 32, 59, 30, 38, 36,…
## $ Attrition                &lt;fct&gt; Yes, No, Yes, No, No, No, No, No, No, N…
## $ BusinessTravel           &lt;fct&gt; Travel_Rarely, Travel_Frequently, Trave…
## $ DailyRate                &lt;dbl&gt; 1102, 279, 1373, 1392, 591, 1005, 1324,…
## $ Department               &lt;fct&gt; Sales, Research &amp; Development, Research…
## $ DistanceFromHome         &lt;dbl&gt; 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15…
## $ Education                &lt;fct&gt; College, Below College, College, Master…
## $ EducationField           &lt;fct&gt; Life Sciences, Life Sciences, Other, Li…
## $ EmployeeCount            &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
## $ EmployeeNumber           &lt;dbl&gt; 1, 2, 4, 5, 7, 8, 10, 11, 12, 13, 14, 1…
## $ EnvironmentSatisfaction  &lt;fct&gt; Medium, High, Very High, Very High, Low…
## $ Gender                   &lt;fct&gt; Female, Male, Male, Female, Male, Male,…
## $ HourlyRate               &lt;dbl&gt; 94, 61, 92, 56, 40, 79, 81, 67, 44, 94,…
## $ JobInvolvement           &lt;fct&gt; High, Medium, Medium, High, High, High,…
## $ JobLevel                 &lt;dbl&gt; 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, …
## $ JobRole                  &lt;fct&gt; Sales Executive, Research Scientist, La…
## $ JobSatisfaction          &lt;fct&gt; Very High, Medium, High, High, Medium, …
## $ MaritalStatus            &lt;fct&gt; Single, Married, Single, Married, Marri…
## $ MonthlyIncome            &lt;dbl&gt; 5993, 5130, 2090, 2909, 3468, 3068, 267…
## $ MonthlyRate              &lt;dbl&gt; 19479, 24907, 2396, 23159, 16632, 11864…
## $ NumCompaniesWorked       &lt;dbl&gt; 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, …
## $ Over18                   &lt;fct&gt; Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, …
## $ OverTime                 &lt;fct&gt; Yes, No, Yes, Yes, No, No, Yes, No, No,…
## $ PercentSalaryHike        &lt;dbl&gt; 11, 23, 15, 11, 12, 13, 20, 22, 21, 13,…
## $ PerformanceRating        &lt;fct&gt; Excellent, Outstanding, Excellent, Exce…
## $ RelationshipSatisfaction &lt;fct&gt; Low, Very High, Medium, High, Very High…
## $ StandardHours            &lt;dbl&gt; 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,…
## $ StockOptionLevel         &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, …
## $ TotalWorkingYears        &lt;dbl&gt; 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10…
## $ TrainingTimesLastYear    &lt;dbl&gt; 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, …
## $ WorkLifeBalance          &lt;fct&gt; Bad, Better, Better, Better, Better, Go…
## $ YearsAtCompany           &lt;dbl&gt; 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5,…
## $ YearsInCurrentRole       &lt;dbl&gt; 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, …
## $ YearsSinceLastPromotion  &lt;dbl&gt; 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, …
## $ YearsWithCurrManager     &lt;dbl&gt; 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, …</code></pre>
</div>
<div id="data-preparation---machine-readable-with-recipes" class="section level3">
<h3>Data Preparation - Machine Readable With Recipes</h3>
<p><strong>Recipes:</strong></p>
<ul>
<li>Alternative Method for creating and pre-processing design matrices
that can be used for modeling or visualization.<br />
</li>
<li>Creates a template assigning roles to variables within data.</li>
<li>Formula sets the stage for subsequent pre-processing steps.<br />
</li>
<li>Formula creates the outcomes and predictor roles, assigning each
variable or feature to a role<br />
</li>
<li>Features are transformed into a format that can be digested by ML
algorithms.</li>
</ul>
<p><strong>Faceted Histogram Plotting Function</strong></p>
<ul>
<li>It would help us in inspecting feature distributions to check
<strong>Skewness</strong>.</li>
<li>Identify transformations needed to get them properly formatted for
<strong>Correlation Analysis</strong>.</li>
</ul>
<pre class="r"><code>train_readable_tbl &lt;- process_hr_data_readable(train_raw_tbl, definitions_tbl = definitions_raw_tbl)
test_readable_tbl &lt;- process_hr_data_readable(test_raw_tbl, definitions_tbl = definitions_raw_tbl)


plot_hist_facet &lt;- function(data, bins = 10, ncol = 5, 
                            fct_reorder = FALSE, fct_rev = FALSE,
                            fill = palette_light()[[3]],
                            color = &quot;white&quot;, scale = &quot;free&quot;) {
  
  data_factored &lt;- data %&gt;%
    mutate_if(is.character, as.factor) %&gt;%
    mutate_if(is.factor, as.numeric) %&gt;%
    gather(key = key, value = value, factor_key = TRUE)
  
  if(fct_reorder) {
    data_factored &lt;- data_factored %&gt;%
      mutate(key = as.character(key) %&gt;% as.factor())
  }
  
  if(fct_rev){
    data_factored &lt;- data_factored %&gt;%
      mutate(key = fct_rev(key))
  }
  
  g &lt;- data_factored %&gt;%
    ggplot(aes(x = value, group = key )) +
    geom_histogram(bins = bins, fill = fill , color = color) +
    facet_wrap(~ key, ncol = ncol, scale = scale) +
    theme_tq()

  
  return(g)
  
}


#Removing zero variance features for facet histogram 
zeroVar &lt;- function(data, useNA = &#39;ifany&#39;) {
  out &lt;- apply(data, 2, function(x) {length(table(x, useNA = useNA))})
  which(out==1)
}

zv &lt;- names(zeroVar(train_raw_tbl))</code></pre>
<pre class="r"><code>train_raw_tbl %&gt;%
  select(-zv) %&gt;%
  select(Attrition, everything()) %&gt;%
  plot_hist_facet(bins = 10, ncol = 5, fct_rev = F)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-33-1.png" /></p>
<ul>
<li><p>Features like YearsSinceLastPromotion, YearsAtCompany,
MonthlyIncome, TotalWorkingYears, NumCompaniesWorked,
DistanceFromHome, YearsInCurrentRole, PercentSalaryHike,
YearsWithCurrManager are skewed features, we might want to inspect
them for transformations.</p></li>
<li><p>Features like EmployeeCount, Over18, Standard Hours have same value
i.e zero variance, so its a good idea to not consider these features
in analysis.</p></li>
<li><p>Features like Gender, JobLevel, JobSatisfaction, StockOptionLevel
are factors since histograms works with numeric features we
converted them to numeric variables for this plot.</p></li>
</ul>
<p><strong>Data Preprocessing with Recipes</strong></p>
<p><strong>Recipes are generally a 3-part process</strong></p>
<ul>
<li>Create the instructions with recipes and steps<br />
</li>
<li>Prepare the recipe<br />
</li>
<li>Bake the new data</li>
</ul>
<p>There are quite a few steps available, following guidelines might be
useful in creating a generic checklist for pre-processing data for ML :</p>
<ul>
<li>Impute - Act of filling in missing values within features. Common
methods include recency (tidyr::fill), similarity (knn impute)<br />
</li>
<li>Individual transformation for skewness and other issues<br />
</li>
<li>Discretize (if needed and if you have no other choice) i.e making
continuous variable discrete. For ex, think of turning a variable
like age into cohorts of less than 20 years, 20-30, 30-40 etc.<br />
</li>
<li>Create dummy variable i.e turning categorical data into separate
columns of zeros and ones. This is important for machine learning to
detect patterns in unordered data.<br />
</li>
<li>Data Normalization steps (centering, scaling, range) . Normalization
is getting the data onto a consistent scale. Some machine learning
algorithms (e.g PCA, KNN, Deep learning etc) depend on the data to
all have same scale</li>
<li>Multivariate Transformation (e.g PCA, spatial sign etc…)</li>
</ul>
<p><strong>Note</strong>: Discretization can hurt correlations. It’s often best not to
discretize unless there is a specific need to do so</p>
<p><strong>1. Zero Variance Features</strong></p>
<ul>
<li>Features that have no variance, and therefore lend nothing the
predictive quality of model<br />
</li>
<li>Remove attributes with a zero variance (i.e all the same value)</li>
</ul>
<pre class="r"><code>recipe_obj &lt;- recipe(Attrition ~ ., data = train_readable_tbl) %&gt;%
  step_zv(all_predictors())

recipe_obj</code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         34
## 
## Operations:
## 
## Zero variance filter on all_predictors()</code></pre>
<pre class="r"><code>recipe_obj %&gt;%
  prep() %&gt;%
  bake(new_data = train_readable_tbl)</code></pre>
<pre><code>## # A tibble: 1,250 x 32
##      Age Attrition BusinessTravel    DailyRate Department            
##    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;                 &lt;dbl&gt; &lt;fct&gt;                 
##  1    41 Yes       Travel_Rarely          1102 Sales                 
##  2    49 No        Travel_Frequently       279 Research &amp; Development
##  3    37 Yes       Travel_Rarely          1373 Research &amp; Development
##  4    33 No        Travel_Frequently      1392 Research &amp; Development
##  5    27 No        Travel_Rarely           591 Research &amp; Development
##  6    32 No        Travel_Frequently      1005 Research &amp; Development
##  7    59 No        Travel_Rarely          1324 Research &amp; Development
##  8    30 No        Travel_Rarely          1358 Research &amp; Development
##  9    38 No        Travel_Frequently       216 Research &amp; Development
## 10    36 No        Travel_Rarely          1299 Research &amp; Development
##    DistanceFromHome Education     EducationField EmployeeNumber
##               &lt;dbl&gt; &lt;fct&gt;         &lt;fct&gt;                   &lt;dbl&gt;
##  1                1 College       Life Sciences               1
##  2                8 Below College Life Sciences               2
##  3                2 College       Other                       4
##  4                3 Master        Life Sciences               5
##  5                2 Below College Medical                     7
##  6                2 College       Life Sciences               8
##  7                3 Bachelor      Medical                    10
##  8               24 Below College Life Sciences              11
##  9               23 Bachelor      Life Sciences              12
## 10               27 Bachelor      Medical                    13
##    EnvironmentSatisfaction Gender HourlyRate JobInvolvement JobLevel
##    &lt;fct&gt;                   &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;             &lt;dbl&gt;
##  1 Medium                  Female         94 High                  2
##  2 High                    Male           61 Medium                2
##  3 Very High               Male           92 Medium                1
##  4 Very High               Female         56 High                  1
##  5 Low                     Male           40 High                  1
##  6 Very High               Male           79 High                  1
##  7 High                    Female         81 Very High             1
##  8 Very High               Male           67 High                  1
##  9 Very High               Male           44 Medium                3
## 10 High                    Male           94 High                  2
##    JobRole                   JobSatisfaction MaritalStatus MonthlyIncome
##    &lt;fct&gt;                     &lt;fct&gt;           &lt;fct&gt;                 &lt;dbl&gt;
##  1 Sales Executive           Very High       Single                 5993
##  2 Research Scientist        Medium          Married                5130
##  3 Laboratory Technician     High            Single                 2090
##  4 Research Scientist        High            Married                2909
##  5 Laboratory Technician     Medium          Married                3468
##  6 Laboratory Technician     Very High       Single                 3068
##  7 Laboratory Technician     Low             Married                2670
##  8 Laboratory Technician     High            Divorced               2693
##  9 Manufacturing Director    High            Single                 9526
## 10 Healthcare Representative High            Married                5237
##    MonthlyRate NumCompaniesWorked OverTime PercentSalaryHike
##          &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt;                &lt;dbl&gt;
##  1       19479                  8 Yes                     11
##  2       24907                  1 No                      23
##  3        2396                  6 Yes                     15
##  4       23159                  1 Yes                     11
##  5       16632                  9 No                      12
##  6       11864                  0 No                      13
##  7        9964                  4 Yes                     20
##  8       13335                  1 No                      22
##  9        8787                  0 No                      21
## 10       16577                  6 No                      13
##    PerformanceRating RelationshipSatisfaction StockOptionLevel
##    &lt;fct&gt;             &lt;fct&gt;                               &lt;dbl&gt;
##  1 Excellent         Low                                     0
##  2 Outstanding       Very High                               1
##  3 Excellent         Medium                                  0
##  4 Excellent         High                                    0
##  5 Excellent         Very High                               1
##  6 Excellent         High                                    0
##  7 Outstanding       Low                                     3
##  8 Outstanding       Medium                                  1
##  9 Outstanding       Medium                                  0
## 10 Excellent         Medium                                  2
##    TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany
##                &lt;dbl&gt;                 &lt;dbl&gt; &lt;fct&gt;                    &lt;dbl&gt;
##  1                 8                     0 Bad                          6
##  2                10                     3 Better                      10
##  3                 7                     3 Better                       0
##  4                 8                     3 Better                       8
##  5                 6                     3 Better                       2
##  6                 8                     2 Good                         7
##  7                12                     3 Good                         1
##  8                 1                     2 Better                       1
##  9                10                     2 Better                       9
## 10                17                     3 Good                         7
##    YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager
##                 &lt;dbl&gt;                   &lt;dbl&gt;                &lt;dbl&gt;
##  1                  4                       0                    5
##  2                  7                       1                    7
##  3                  0                       0                    0
##  4                  7                       3                    0
##  5                  2                       2                    2
##  6                  7                       3                    6
##  7                  0                       0                    0
##  8                  0                       0                    0
##  9                  7                       1                    8
## 10                  7                       7                    7
## # … with 1,240 more rows</code></pre>
<p><strong>2. Data Transformations</strong></p>
<ul>
<li>Changes the data to remove skewness (e.g log), stabilize variance
(e.g Box Cox) or make stationary (e.g difference for time series)<br />
</li>
<li>Computing the skewness of numeric features and filtering which are
highly skewed</li>
</ul>
<pre class="r"><code>skewed_feature_names &lt;- train_readable_tbl %&gt;%
  select_if(is.numeric) %&gt;%
  map_df(skewness) %&gt;%
  gather(factor_key = TRUE) %&gt;%
  arrange(desc(value)) %&gt;%
  filter(value &gt;= 0.8) %&gt;%
  pull(key) %&gt;%
  as.character()

# Printing skewed feature names 
skewed_feature_names</code></pre>
<pre><code>##  [1] &quot;YearsSinceLastPromotion&quot; &quot;YearsAtCompany&quot;         
##  [3] &quot;MonthlyIncome&quot;           &quot;TotalWorkingYears&quot;      
##  [5] &quot;NumCompaniesWorked&quot;      &quot;JobLevel&quot;               
##  [7] &quot;DistanceFromHome&quot;        &quot;StockOptionLevel&quot;       
##  [9] &quot;YearsInCurrentRole&quot;      &quot;PercentSalaryHike&quot;      
## [11] &quot;YearsWithCurrManager&quot;</code></pre>
<p>Plotting skewed features to identify which needs to be converted into
factors</p>
<pre class="r"><code>train_readable_tbl %&gt;%
  select(skewed_feature_names) %&gt;%
  plot_hist_facet()</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-36-1.png" /></p>
<p>Based on plot we observe that <strong>Joblevel</strong> and <strong>StockOptionLevel</strong> are
not numeric. These features needs to be converted to factors and should
not be transformed</p>
<pre class="r"><code>skewed_feature_names &lt;- train_readable_tbl %&gt;%
  select_if(is.numeric) %&gt;%
  map_df(skewness) %&gt;%
  gather(factor_key = TRUE) %&gt;%
  arrange(desc(value)) %&gt;%
  filter(value &gt;= 0.8) %&gt;%
  filter(!key  %in% c(&quot;JobLevel&quot;, &quot;StockOptionLevel&quot;)) %&gt;%
  pull(key) %&gt;%
  as.character()

skewed_feature_names</code></pre>
<pre><code>## [1] &quot;YearsSinceLastPromotion&quot; &quot;YearsAtCompany&quot;         
## [3] &quot;MonthlyIncome&quot;           &quot;TotalWorkingYears&quot;      
## [5] &quot;NumCompaniesWorked&quot;      &quot;DistanceFromHome&quot;       
## [7] &quot;YearsInCurrentRole&quot;      &quot;PercentSalaryHike&quot;      
## [9] &quot;YearsWithCurrManager&quot;</code></pre>
<p><strong>Yeo-Johnson Transform</strong></p>
<p>Applying a Yeo-Johnson transform, like a BoxCox, but values can be
negative</p>
<pre class="r"><code>factor_names &lt;-  c(&quot;JobLevel&quot;, &quot;StockOptionLevel&quot;)

recipe_obj &lt;- recipe(Attrition ~ ., data = train_readable_tbl) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_YeoJohnson(skewed_feature_names) %&gt;%
  step_num2factor(factor_names)</code></pre>
<p><strong>Faceted Histogram for Skewed features</strong></p>
<pre class="r"><code>recipe_obj %&gt;%
  prep() %&gt;%
  bake(train_readable_tbl) %&gt;%
  select(skewed_feature_names) %&gt;%
  plot_hist_facet()</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-39-1.png" /></p>
<p><strong>3. Data Center/Scaling</strong></p>
<p>Converting numeric data into different scales to be on same scale since
some algorithm requires feature scaling e.g Kmeans, Deep learning, PCA &amp;
SVMs. Its hard to remember which algorithm needs it, it’s better to
center and scale. This typically don’t hurt the predictions.</p>
<p><strong>Note</strong> : Always center before you scale</p>
<pre class="r"><code>recipe_obj &lt;- recipe(Attrition ~ ., data = train_readable_tbl) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_YeoJohnson(skewed_feature_names) %&gt;%
  step_num2factor(factor_names) %&gt;%
  step_center(all_numeric()) %&gt;%
  step_scale(all_numeric())

# Before preparing Recipes, mean is NULL
recipe_obj$steps[[4]] </code></pre>
<pre><code>## $terms
## &lt;list_of&lt;quosure&gt;&gt;
## 
## [[1]]
## &lt;quosure&gt;
## expr: ^all_numeric()
## env:  0x1314efc10
## 
## 
## $role
## [1] NA
## 
## $trained
## [1] FALSE
## 
## $means
## NULL
## 
## $na_rm
## [1] TRUE
## 
## $skip
## [1] FALSE
## 
## $id
## [1] &quot;center_oIgVN&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;step_center&quot; &quot;step&quot;</code></pre>
<pre class="r"><code># After preparing recipes, mean is not null 
prepared_recipe &lt;- recipe_obj %&gt;% prep()
prepared_recipe$steps[[4]]</code></pre>
<pre><code>## $terms
## &lt;list_of&lt;quosure&gt;&gt;
## 
## [[1]]
## &lt;quosure&gt;
## expr: ^all_numeric()
## env:  0x1314efc10
## 
## 
## $role
## [1] NA
## 
## $trained
## [1] TRUE
## 
## $means
##                     Age               DailyRate        DistanceFromHome 
##            3.695040e+01            7.997448e+02            1.981462e+00 
##          EmployeeNumber              HourlyRate           MonthlyIncome 
##            1.022712e+03            6.582400e+01            3.769523e+00 
##             MonthlyRate      NumCompaniesWorked       PercentSalaryHike 
##            1.429450e+04            1.042074e+00            7.045663e-01 
##       TotalWorkingYears   TrainingTimesLastYear          YearsAtCompany 
##            3.409948e+00            2.813600e+00            2.028353e+00 
##      YearsInCurrentRole YearsSinceLastPromotion    YearsWithCurrManager 
##            1.748426e+00            5.446626e-01            1.733001e+00 
## 
## $na_rm
## [1] TRUE
## 
## $skip
## [1] FALSE
## 
## $id
## [1] &quot;center_oIgVN&quot;
## 
## attr(,&quot;class&quot;)
## [1] &quot;step_center&quot; &quot;step&quot;</code></pre>
<p><strong>Plotting Features After Transformation</strong></p>
<p>We observe that features are no more skewed now</p>
<pre class="r"><code>prepared_recipe %&gt;%
  bake(new_data = train_readable_tbl) %&gt;%
  select_if(is.numeric) %&gt;%
  plot_hist_facet()</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-41-1.png" /></p>
<p><strong>4. Dummy Variables</strong></p>
<p>Expanding categorical features into multiple columns of 0’s and 1’s.
This is important for machine learning algorithms to detect patterns in
unordered data</p>
<p><strong>Categorical feature before expanding</strong></p>
<p>We will be looking at histogram of feature which contains <strong>JobRole</strong> in
its name. We observe that it is just one feature before any
transformation</p>
<pre class="r"><code># Before expanding

recipe_obj %&gt;%
  prep() %&gt;%
  bake(new_data = train_readable_tbl) %&gt;%
  select(contains(&quot;JobRole&quot;)) %&gt;%
  plot_hist_facet()</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-42-1.png" /></p>
<p><strong>Categorical feature after expanding</strong></p>
<ul>
<li>We are using step_dummy which will convert character/factors into
one or more numeric terms</li>
<li>Generally, if factor has 3 levels, the feature is expanded into 2
columns (1 less than the number of levels)</li>
<li>We will obtain the full set of dummy variables using <code>one_hot</code>
argument</li>
</ul>
<pre class="r"><code># After expanding  

dummied_recipe_obj  &lt;- recipe(Attrition ~ ., data = train_readable_tbl) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_YeoJohnson(skewed_feature_names) %&gt;%
  step_num2factor(factor_names) %&gt;%
  step_center(all_numeric()) %&gt;%
  step_scale(all_numeric()) %&gt;%
  step_dummy(all_nominal(), one_hot = TRUE)

dummied_recipe_obj %&gt;%
  prep() %&gt;%
  bake(new_data = train_readable_tbl) %&gt;%
  select(contains(&quot;JobRole&quot;)) %&gt;%
  plot_hist_facet(ncol = 3)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-43-1.png" /></p>
<p>We notice that feature <strong>JobRole</strong> now has been expanded to following 9
features :</p>
<ul>
<li>JobRole_Healthcare.Representative<br />
</li>
<li>JobRole_Human.Resources<br />
</li>
<li>JobRole_Laboratory.Technician<br />
</li>
<li>JobRole_Manager<br />
</li>
<li>JobRole_Manufacturing.Director<br />
</li>
<li>JobRole_Research.Director<br />
</li>
<li>JobRole_Research.Scientist<br />
</li>
<li>JobRole_Sales.Executive<br />
</li>
<li>JobRole_Sales.Representative</li>
</ul>
<p><strong>Final Recipe for Correlation Evaluation</strong></p>
<pre class="r"><code>recipe_obj  &lt;- recipe(Attrition ~ ., data = train_readable_tbl) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_YeoJohnson(skewed_feature_names) %&gt;%
  step_num2factor(factor_names) %&gt;%
  step_center(all_numeric()) %&gt;%
  step_scale(all_numeric()) %&gt;%
  step_dummy(all_nominal(),one_hot = TRUE) %&gt;%
  prep()

# Printing Recipe
recipe_obj</code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         34
## 
## Training data contained 1250 data points and no missing data.
## 
## Operations:
## 
## Zero variance filter removed EmployeeCount, ... [trained]
## Yeo-Johnson transformation on 9 items [trained]
## Factor variables from JobLevel, StockOptionLevel [trained]
## Centering for Age, DailyRate, ... [trained]
## Scaling for Age, DailyRate, ... [trained]
## Dummy variables from BusinessTravel, Department, ... [trained]</code></pre>
<p><strong>Training data after applying recipe</strong></p>
<pre class="r"><code>train_tbl &lt;- bake(recipe_obj, new_data = train_readable_tbl)
train_tbl %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 1,250
## Variables: 83
## $ Age                                &lt;dbl&gt; 0.445810757, 1.326511581, 0.0…
## $ DailyRate                          &lt;dbl&gt; 0.74904690, -1.29050643, 1.42…
## $ DistanceFromHome                   &lt;dbl&gt; -1.49654918, 0.26177716, -1.0…
## $ EmployeeNumber                     &lt;dbl&gt; -1.701657, -1.699992, -1.6966…
## $ HourlyRate                         &lt;dbl&gt; 1.39874138, -0.23947787, 1.29…
## $ MonthlyIncome                      &lt;dbl&gt; 0.28123820, 0.04824053, -1.47…
## $ MonthlyRate                        &lt;dbl&gt; 0.7331422, 1.5007180, -1.6825…
## $ NumCompaniesWorked                 &lt;dbl&gt; 1.62470821, -0.58603051, 1.26…
## $ PercentSalaryHike                  &lt;dbl&gt; -1.4812855, 1.6672467, 0.1960…
## $ TotalWorkingYears                  &lt;dbl&gt; -0.26400582, 0.03531548, -0.4…
## $ TrainingTimesLastYear              &lt;dbl&gt; -2.1773654, 0.1442497, 0.1442…
## $ YearsAtCompany                     &lt;dbl&gt; 0.12937107, 0.74947123, -2.24…
## $ YearsInCurrentRole                 &lt;dbl&gt; 0.20272226, 0.87516459, -1.59…
## $ YearsSinceLastPromotion            &lt;dbl&gt; -1.11029069, 0.07030875, -1.1…
## $ YearsWithCurrManager               &lt;dbl&gt; 0.47708525, 0.89609325, -1.54…
## $ BusinessTravel_Non.Travel          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ BusinessTravel_Travel_Rarely       &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,…
## $ BusinessTravel_Travel_Frequently   &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,…
## $ Department_Human.Resources         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ Department_Research...Development  &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ Department_Sales                   &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ Education_Below.College            &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,…
## $ Education_College                  &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,…
## $ Education_Bachelor                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,…
## $ Education_Master                   &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…
## $ Education_Doctor                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ EducationField_Human.Resources     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ EducationField_Life.Sciences       &lt;dbl&gt; 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,…
## $ EducationField_Marketing           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ EducationField_Medical             &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,…
## $ EducationField_Other               &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…
## $ EducationField_Technical.Degree    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ EnvironmentSatisfaction_Low        &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…
## $ EnvironmentSatisfaction_Medium     &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ EnvironmentSatisfaction_High       &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,…
## $ EnvironmentSatisfaction_Very.High  &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,…
## $ Gender_Female                      &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,…
## $ Gender_Male                        &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,…
## $ JobInvolvement_Low                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobInvolvement_Medium              &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,…
## $ JobInvolvement_High                &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,…
## $ JobInvolvement_Very.High           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…
## $ JobLevel_X1                        &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,…
## $ JobLevel_X2                        &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,…
## $ JobLevel_X3                        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…
## $ JobLevel_X4                        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobLevel_X5                        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobRole_Healthcare.Representative  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…
## $ JobRole_Human.Resources            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobRole_Laboratory.Technician      &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,…
## $ JobRole_Manager                    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobRole_Manufacturing.Director     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…
## $ JobRole_Research.Director          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobRole_Research.Scientist         &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,…
## $ JobRole_Sales.Executive            &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobRole_Sales.Representative       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobSatisfaction_Low                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…
## $ JobSatisfaction_Medium             &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,…
## $ JobSatisfaction_High               &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,…
## $ JobSatisfaction_Very.High          &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,…
## $ MaritalStatus_Single               &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,…
## $ MaritalStatus_Married              &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,…
## $ MaritalStatus_Divorced             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…
## $ OverTime_No                        &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,…
## $ OverTime_Yes                       &lt;dbl&gt; 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,…
## $ PerformanceRating_Low              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ PerformanceRating_Good             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ PerformanceRating_Excellent        &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,…
## $ PerformanceRating_Outstanding      &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,…
## $ RelationshipSatisfaction_Low       &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,…
## $ RelationshipSatisfaction_Medium    &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,…
## $ RelationshipSatisfaction_High      &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,…
## $ RelationshipSatisfaction_Very.High &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,…
## $ StockOptionLevel_X0                &lt;dbl&gt; 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,…
## $ StockOptionLevel_X1                &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,…
## $ StockOptionLevel_X2                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…
## $ StockOptionLevel_X3                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…
## $ WorkLifeBalance_Bad                &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ WorkLifeBalance_Good               &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,…
## $ WorkLifeBalance_Better             &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,…
## $ WorkLifeBalance_Best               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ Attrition_No                       &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,…
## $ Attrition_Yes                      &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,…</code></pre>
<p><strong>Test data after applying recipe</strong></p>
<pre class="r"><code>test_tbl &lt;- bake(recipe_obj, new_data = test_readable_tbl)
test_tbl %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 220
## Variables: 83
## $ Age                                &lt;dbl&gt; -0.765152876, 1.987037199, 1.…
## $ DailyRate                          &lt;dbl&gt; 1.33637879, -1.54328218, -0.2…
## $ DistanceFromHome                   &lt;dbl&gt; -0.21324201, 0.26177716, -0.6…
## $ EmployeeNumber                     &lt;dbl&gt; -1.32358956, 0.42518111, 0.37…
## $ HourlyRate                         &lt;dbl&gt; -1.2323380, -1.4309100, 0.951…
## $ MonthlyIncome                      &lt;dbl&gt; 0.91393698, 1.38024738, 1.700…
## $ MonthlyRate                        &lt;dbl&gt; -8.826134e-01, 1.597584e+00, …
## $ NumCompaniesWorked                 &lt;dbl&gt; 0.03083829, -0.58603051, 0.45…
## $ PercentSalaryHike                  &lt;dbl&gt; -0.9455944, 0.1960170, -0.124…
## $ TotalWorkingYears                  &lt;dbl&gt; 0.29804725, 2.14458005, 2.264…
## $ TrainingTimesLastYear              &lt;dbl&gt; -0.6296220, 0.1442497, -0.629…
## $ YearsAtCompany                     &lt;dbl&gt; 0.74947123, 2.42388523, 0.749…
## $ YearsInCurrentRole                 &lt;dbl&gt; 1.2224028, 1.2224028, 1.22240…
## $ YearsSinceLastPromotion            &lt;dbl&gt; 1.44752633, 1.83567634, -1.11…
## $ YearsWithCurrManager               &lt;dbl&gt; 0.22693493, -1.54446392, 1.24…
## $ BusinessTravel_Non.Travel          &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,…
## $ BusinessTravel_Travel_Rarely       &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,…
## $ BusinessTravel_Travel_Frequently   &lt;dbl&gt; 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,…
## $ Department_Human.Resources         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ Department_Research...Development  &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,…
## $ Department_Sales                   &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,…
## $ Education_Below.College            &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ Education_College                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,…
## $ Education_Bachelor                 &lt;dbl&gt; 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,…
## $ Education_Master                   &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…
## $ Education_Doctor                   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ EducationField_Human.Resources     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ EducationField_Life.Sciences       &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,…
## $ EducationField_Marketing           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ EducationField_Medical             &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,…
## $ EducationField_Other               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ EducationField_Technical.Degree    &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…
## $ EnvironmentSatisfaction_Low        &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…
## $ EnvironmentSatisfaction_Medium     &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,…
## $ EnvironmentSatisfaction_High       &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…
## $ EnvironmentSatisfaction_Very.High  &lt;dbl&gt; 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,…
## $ Gender_Female                      &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,…
## $ Gender_Male                        &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,…
## $ JobInvolvement_Low                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobInvolvement_Medium              &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,…
## $ JobInvolvement_High                &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,…
## $ JobInvolvement_Very.High           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobLevel_X1                        &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,…
## $ JobLevel_X2                        &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,…
## $ JobLevel_X3                        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobLevel_X4                        &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,…
## $ JobLevel_X5                        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobRole_Healthcare.Representative  &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobRole_Human.Resources            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobRole_Laboratory.Technician      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,…
## $ JobRole_Manager                    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobRole_Manufacturing.Director     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…
## $ JobRole_Research.Director          &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…
## $ JobRole_Research.Scientist         &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,…
## $ JobRole_Sales.Executive            &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,…
## $ JobRole_Sales.Representative       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobSatisfaction_Low                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ JobSatisfaction_Medium             &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,…
## $ JobSatisfaction_High               &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,…
## $ JobSatisfaction_Very.High          &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,…
## $ MaritalStatus_Single               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ MaritalStatus_Married              &lt;dbl&gt; 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,…
## $ MaritalStatus_Divorced             &lt;dbl&gt; 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,…
## $ OverTime_No                        &lt;dbl&gt; 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,…
## $ OverTime_Yes                       &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,…
## $ PerformanceRating_Low              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ PerformanceRating_Good             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ PerformanceRating_Excellent        &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
## $ PerformanceRating_Outstanding      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ RelationshipSatisfaction_Low       &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,…
## $ RelationshipSatisfaction_Medium    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…
## $ RelationshipSatisfaction_High      &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,…
## $ RelationshipSatisfaction_Very.High &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,…
## $ StockOptionLevel_X0                &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,…
## $ StockOptionLevel_X1                &lt;dbl&gt; 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,…
## $ StockOptionLevel_X2                &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,…
## $ StockOptionLevel_X3                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ WorkLifeBalance_Bad                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ WorkLifeBalance_Good               &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,…
## $ WorkLifeBalance_Better             &lt;dbl&gt; 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,…
## $ WorkLifeBalance_Best               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
## $ Attrition_No                       &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,…
## $ Attrition_Yes                      &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…</code></pre>
</div>
<div id="correlation-analysis" class="section level3">
<h3>Correlation Analysis</h3>
<p>Correlation Analysis is a great way to determine if you are getting good
features prior to modeling. It is always a good idea to spend your time
on collecting data on good features. If you do not have any feature
exhibiting low correlation, get different data, else analysis isn’t
going to look good.</p>
<p>Features exhibiting high correlation, we should immediately report them
as potential area of focus. Early detection helps stakeholders building
strategies.</p>
<p>Function <strong>get_cor</strong> takes in data and measure correlation against
targeted feature.</p>
<pre class="r"><code># fct_reorder: reorders a factor by another column. It changes the level of factors but does not rearrange the data frame. We do this factor level reordering for plotting function, plot_cor()

get_cor &lt;- function(data, target, use = &quot;pairwise.complete.obs&quot;,
                     fct_reorder = FALSE, fct_rev = FALSE){
    
   feature_expr &lt;- enquo(target)
  feature_name &lt;- quo_name(feature_expr)
  
  data_cor &lt;-  data %&gt;%
    mutate_if(is.character, as.factor) %&gt;%
    mutate_if(is.factor, as.numeric) %&gt;%
    cor(use = use) %&gt;%
    as.tibble() %&gt;%
    mutate(feature = names(.)) %&gt;%
    select(feature, !! feature_expr) %&gt;%
    filter(!(feature == feature_name)) %&gt;%
    mutate_if(is.character, as_factor)

  if(fct_reorder){
    data_cor &lt;- data_cor %&gt;%
      mutate(feature = fct_reorder(feature, !! feature_expr)) %&gt;%
      arrange(feature)
  }
  
  if(fct_rev){
    data_cor &lt;- data_cor %&gt;%
      mutate(feature = fct_rev(feature)) %&gt;%
      arrange(feature)
  }
  
return(data_cor)  
}</code></pre>
<p>Using <strong>train_tbl</strong> which has been well formatted for Correlation
Analysis</p>
<pre class="r"><code>train_tbl %&gt;% 
  get_cor(Attrition_Yes, fct_reorder = T, fct_rev = T )</code></pre>
<pre><code>## # A tibble: 82 x 2
##    feature                          Attrition_Yes
##    &lt;fct&gt;                                    &lt;dbl&gt;
##  1 PerformanceRating_Good                  NA    
##  2 PerformanceRating_Low                   NA    
##  3 OverTime_Yes                             0.240
##  4 JobLevel_X1                              0.212
##  5 StockOptionLevel_X0                      0.199
##  6 MaritalStatus_Single                     0.172
##  7 JobRole_Sales.Representative             0.153
##  8 EnvironmentSatisfaction_Low              0.119
##  9 WorkLifeBalance_Bad                      0.109
## 10 BusinessTravel_Travel_Frequently         0.103
## # … with 72 more rows</code></pre>
<p>Function <strong>plot_cor</strong> takes in data and plot correlation against
targeted feature i.e Attrition in this case.</p>
<pre class="r"><code>data         &lt;- train_tbl
feature_expr &lt;- quo(Attrition_Yes)

plot_cor &lt;- function(data, target, fct_reorder = FALSE, fct_rev = FALSE,
                     include_lbl = TRUE, lbl_precision = 2, lbl_position = &quot;outward&quot;,
                     size = 2, line_size = 1, vert_size = 1,
                     color_pos = palette_light()[[1]],
                     color_neg = palette_light()[[2]]){

  
  feature_expr &lt;- enquo(target)
  feature_name &lt;- quo_name(feature_expr)

  data_cor &lt;-  data %&gt;%
      get_cor(!! feature_expr, fct_reorder = fct_reorder , fct_rev = fct_rev) %&gt;%
      mutate(feature_name_text = round(!! feature_expr, lbl_precision)) %&gt;%
      mutate(Correlation = case_when(
        (!! feature_expr) &gt;=0 ~ &quot;Positive&quot;,
        TRUE                  ~ &quot;Negative&quot;) %&gt;% as.factor())
  

      g &lt;- data_cor %&gt;%
        ggplot(aes_string(x = feature_name, y = &quot;feature&quot;, group = &quot;feature&quot;)) +
        geom_point(aes(color = Correlation), size = size) +
        geom_segment(aes(xend = 0, yend = feature, color = Correlation), size = line_size) +
        geom_vline(xintercept = 0, color = palette_light()[[1]], size = vert_size) +
        expand_limits(x = c(-1,1)) +
        theme_tq() +
        scale_color_manual(values = c(color_neg, color_pos))
      
      
    if(include_lbl) g &lt;- g + geom_label(aes(label = feature_name_text), hjust = lbl_position)
    
    return(g)    
}</code></pre>
<p><strong>Correlation Evaluation : After Transformation</strong></p>
<p>Variables that contribute most to attrition lies at the top</p>
<p><strong>Explore features by Category</strong></p>
<p><strong>1. Descriptive Features : Age, Gender, MaritalStatus</strong></p>
<pre class="r"><code>train_tbl %&gt;% 
  select(Attrition_Yes, Age, contains(&quot;Gender&quot;), contains(&quot;MaritalStatus&quot;), 
         NumCompaniesWorked, contains(&quot;Over18&quot;), DistanceFromHome) %&gt;% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-50-1.png" /></p>
<p>In this section, we will see how descriptive features correlate with
Attrition:</p>
<ul>
<li>As Age increases, person has lower probability of leaving the
company (negative correlation is good in this case) compared to a
person who are in their early 20’s.<br />
</li>
<li>There is a positive correlation between DistanceFromHome and
Attrition which means it has a effect on Attrition. For instance, as
the distance from home increases from company, there are more
chances that employee may leave.</li>
</ul>
<p><strong>2. Employment Features : Department, JobRole , JobLevel</strong></p>
<pre class="r"><code>train_tbl %&gt;% 
  select(Attrition_Yes, contains(&quot;employee&quot;), contains(&quot;department&quot;), contains(&quot;job&quot;) ) %&gt;% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-51-1.png" /></p>
<p>We can say that there is a relationship between Attrition and
Joblevel_X1. So, its a potential candidate for feature of interest.</p>
<p><strong>3. Compensation Features : DailyRate ,HourlyRate, MonthlyIncome,
MonthlyRate, PercentSalaryHike, StockOptionLevel</strong></p>
<pre class="r"><code>train_tbl %&gt;% 
  select(Attrition_Yes, contains(&quot;income&quot;), contains(&quot;rate&quot;), contains(&quot;salary&quot;), contains(&quot;stock&quot;) ) %&gt;% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-52-1.png" /></p>
<p>In this case, we observe that people who have StockOptionLevel_X0 have
a effect on Attrition. So, they are more likely to leave the company.</p>
<p><strong>4. Survey Results : EnvironmentSatisfaction, JobSatisfaction ,
RelationshipSatisfaction, WorkLifeBalance</strong></p>
<pre class="r"><code>train_tbl %&gt;% 
  select(Attrition_Yes, contains(&quot;satisfaction&quot;), contains(&quot;life&quot;) ) %&gt;% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-53-1.png" /></p>
<p>We can say that people who have low environment satisfaction level at
company and bad worklife balance are more likely to leave the company
since they exhibit a positive correlation with the Attrition which is
not much significant but for sure has an effect on Attrition.</p>
<p><strong>5. Performance Data : JobInvolvement, PerformanceRating</strong></p>
<pre class="r"><code>train_tbl %&gt;% 
  select(Attrition_Yes, contains(&quot;performance&quot;), contains(&quot;involvement&quot;)) %&gt;% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-54-1.png" /></p>
<p>People whose JobInvolvement is low are more likely to leave the company.</p>
<p><strong>6. Work - Life Features : BusinessTravel , OverTime</strong></p>
<pre class="r"><code>train_tbl %&gt;% 
  select(Attrition_Yes, contains(&quot;overtime&quot;), contains(&quot;travel&quot;)) %&gt;% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-55-1.png" /></p>
<ul>
<li>People who work OverTime has a strong effect on Attrition as
compared to people who don’t. They are more likely to leave the
company<br />
</li>
<li>People who travel for business frequently have a positive
relationship with Attrition. So, they are more likely to leave the
company.</li>
</ul>
<p><strong>7. Training &amp; Education : Education, EducationField ,
TrainingTimesLastYear</strong></p>
<pre class="r"><code>train_tbl %&gt;% 
  select(Attrition_Yes, contains(&quot;training&quot;), contains(&quot;education&quot;)) %&gt;% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-56-1.png" /></p>
<ul>
<li>We can say that people who have education field as Technical Degree,
Human Resources &amp; Marketing are more likley to leave the company</li>
</ul>
<p><strong>8. Time - Based Features: TotalWorkingYears , YearsAtCompany ,
YearsInCurrentRole , YearsSinceLastPromotion, YearsWithCurrManager</strong></p>
<pre class="r"><code>train_tbl %&gt;% 
  select(Attrition_Yes, contains(&quot;years&quot;)) %&gt;% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-57-1.png" /></p>
<p>We can say that all the time based features have a negative correlation
with Attrition which is good in our case.</p>
</div>
<div id="phase-4-modeling-churn-with-h2os-automl" class="section level3">
<h3>Phase 4 : Modeling Churn with H2Os AutoML</h3>
<p>In this phase, we will see how we turn information into business
insights ?</p>
</div>
<div id="encode-decision-making-algorithms" class="section level3">
<h3>Encode Decision Making Algorithms</h3>
<p><strong>Step 1: Develop algorithms to predict target and explain in terms of
business levers</strong></p>
<p><strong>First step is to select the technique:</strong></p>
<ul>
<li>Prediction</li>
<li>Classification</li>
<li>Regression</li>
<li>Clustering</li>
<li>Anomaly Detection</li>
</ul>
<p>In this case, it is a <strong>Binary Classification problem</strong></p>
<ul>
<li>Employees that are likely to leave are coded as 0<br />
</li>
<li>Employees that are likely to leave are coded as 1</li>
</ul>
<p><strong>ML Preprocessing with Recipes</strong></p>
<p>We will cut down some steps in our final recipe before automl since h2o
is a low maintenance algorithm. It handles factor and numeric data and
performs pre-processing internally and transformations are not
necessary.</p>
<pre class="r"><code>recipe_obj &lt;- recipe(Attrition ~ ., data = train_readable_tbl) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_num2factor(JobLevel, StockOptionLevel) %&gt;%
  prep()

recipe_obj</code></pre>
<pre><code>## Data Recipe
## 
## Inputs:
## 
##       role #variables
##    outcome          1
##  predictor         34
## 
## Training data contained 1250 data points and no missing data.
## 
## Operations:
## 
## Zero variance filter removed EmployeeCount, ... [trained]
## Factor variables from JobLevel, StockOptionLevel [trained]</code></pre>
<pre class="r"><code>train_tbl &lt;- bake(recipe_obj, new_data = train_readable_tbl)
test_tbl &lt;- bake(recipe_obj, new_data = test_readable_tbl)</code></pre>
<p><strong>Initialising H2O</strong></p>
<pre class="r"><code>h2o.init()</code></pre>
<pre><code>##  Connection successful!
## 
## R is connected to the H2O cluster: 
##     H2O cluster uptime:         4 days 20 hours 
##     H2O cluster timezone:       Asia/Kolkata 
##     H2O data parsing timezone:  UTC 
##     H2O cluster version:        3.23.0.4524 
##     H2O cluster version age:    2 months and 27 days  
##     H2O cluster name:           H2O_started_from_R_raj_fgz785 
##     H2O cluster total nodes:    1 
##     H2O cluster total memory:   2.84 GB 
##     H2O cluster total cores:    8 
##     H2O cluster allowed cores:  8 
##     H2O cluster healthy:        TRUE 
##     H2O Connection ip:          localhost 
##     H2O Connection port:        54321 
##     H2O Connection proxy:       NA 
##     H2O Internal Security:      FALSE 
##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
##     R Version:                  R version 3.5.1 (2018-07-02)</code></pre>
<pre class="r"><code># Splitting the data into train/valid/test and converting tibble to h2o data frame
split_h2o &lt;- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)</code></pre>
<pre class="r"><code>train_h2o &lt;- split_h2o[[1]]
valid_h2o &lt;- split_h2o[[2]]
test_h2o &lt;- as.h2o(test_tbl)</code></pre>
<pre class="r"><code># Defining outcome / response variable
y &lt;- &quot;Attrition&quot;

# Defining explanatory variables
x &lt;- setdiff(names(train_h2o),y)</code></pre>
<p><strong>Running AutoML</strong></p>
<ul>
<li><em>Training Frame</em> : Used to develop model<br />
</li>
<li><em>Validation Frame</em> : Used to tune hyperparameters via grid search<br />
</li>
<li><em>Leaderboard Frame</em>: Test set completely held out from model
training &amp; tuning<br />
</li>
<li>Use <em>max_run_time_secs</em> to minimize modeling time. Once results
look promising we can always increase the run time to get more
models with highly tuned parameters.</li>
</ul>
<pre class="r"><code>automl_models_h2o &lt;- h2o.automl(
    x = x,
    y = y,
    training_frame = train_h2o,
    validation_frame = valid_h2o,
    leaderboard_frame = test_h2o,
    max_runtime_secs = 30,
    nfolds = 5
)</code></pre>
<p>Producing Summary of models produced by H2O</p>
<pre class="r"><code>automl_models_h2o@leaderboard</code></pre>
<p>In the AutoML leaderboard, we get the models ordered with <strong>highest
AUC</strong> and <strong>minimial Logloss</strong></p>
<p><strong>Saving and Loading H2O Models</strong></p>
<pre class="r"><code>h2o.getModel(&quot;DeepLearning_1_AutoML_20190325_101852&quot;) %&gt;%
  h2o.saveModel(path = &quot;04_Modeling/h20_models/&quot;)

h2o.getModel(&quot;GLM_grid_1_AutoML_20190325_101852_model_1&quot;) %&gt;%
  h2o.saveModel(path = &quot;04_Modeling/h20_models/&quot;)

h2o.getModel(&quot;StackedEnsemble_BestOfFamily_AutoML_20190325_101852&quot;) %&gt;%
  h2o.saveModel(path = &quot;04_Modeling/h20_models/&quot;)

deeplearning_h2o &lt;- h2o.loadModel(&quot;04_Modeling/h20_models/DeepLearning_1_AutoML_20190325_101852&quot;)

glm_h2o &lt;- h2o.loadModel(&quot;04_Modeling/h20_models/GLM_grid_1_AutoML_20190325_101852_model_1&quot;) 

stacked_ensemble_h2o &lt;- h2o.loadModel(&quot;04_Modeling/h20_models/StackedEnsemble_BestOfFamily_AutoML_20190325_101852&quot;)</code></pre>
<p>In our case, We will focus more on exploring following leader models :</p>
<ul>
<li><em>GLM</em></li>
<li><em>Stacked Ensemble</em></li>
<li><em>Deep Learning</em></li>
</ul>
<p>Produces leader model in the summary by H2O</p>
<pre class="r"><code>automl_models_h2o@leader</code></pre>
<pre><code>## Model Details:
## ==============
## 
## H2OBinomialModel: stackedensemble
## Model ID:  StackedEnsemble_BestOfFamily_AutoML_20190325_101852 
## NULL
## 
## 
## H2OBinomialMetrics: stackedensemble
## ** Reported on training data. **
## 
## MSE:  0.0425205
## RMSE:  0.206205
## LogLoss:  0.1623063
## Mean Per-Class Error:  0.1141125
## AUC:  0.9829226
## pr_auc:  0.9185428
## Gini:  0.9658452
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##         No Yes    Error      Rate
## No     873  16 0.017998   =16/889
## Yes     37 139 0.210227   =37/176
## Totals 910 155 0.049765  =53/1065
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.409975 0.839879 127
## 2                       max f2  0.140350 0.877551 225
## 3                 max f0point5  0.523454 0.885714 105
## 4                 max accuracy  0.409975 0.950235 127
## 5                max precision  0.986089 1.000000   0
## 6                   max recall  0.094748 1.000000 264
## 7              max specificity  0.986089 1.000000   0
## 8             max absolute_mcc  0.409975 0.812852 127
## 9   max min_per_class_accuracy  0.185882 0.914773 194
## 10 max mean_per_class_accuracy  0.140350 0.930144 225
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## H2OBinomialMetrics: stackedensemble
## ** Reported on validation data. **
## 
## MSE:  0.08123465
## RMSE:  0.2850169
## LogLoss:  0.2863561
## Mean Per-Class Error:  0.245
## AUC:  0.813
## pr_auc:  0.5554092
## Gini:  0.626
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##         No Yes    Error     Rate
## No     152   8 0.050000   =8/160
## Yes     11  14 0.440000   =11/25
## Totals 163  22 0.102703  =19/185
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.346963 0.595745  21
## 2                       max f2  0.229984 0.629630  34
## 3                 max f0point5  0.678260 0.660377   6
## 4                 max accuracy  0.678260 0.902703   6
## 5                max precision  0.978007 1.000000   0
## 6                   max recall  0.035646 1.000000 158
## 7              max specificity  0.978007 1.000000   0
## 8             max absolute_mcc  0.346963 0.538636  21
## 9   max min_per_class_accuracy  0.092737 0.737500  60
## 10 max mean_per_class_accuracy  0.229984 0.783750  34
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## H2OBinomialMetrics: stackedensemble
## ** Reported on cross-validation data. **
## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
## 
## MSE:  0.09294898
## RMSE:  0.3048753
## LogLoss:  0.3169679
## Mean Per-Class Error:  0.2331143
## AUC:  0.8346425
## pr_auc:  0.6259658
## Gini:  0.6692849
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##         No Yes    Error       Rate
## No     818  71 0.079865    =71/889
## Yes     68 108 0.386364    =68/176
## Totals 886 179 0.130516  =139/1065
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.292277 0.608451 146
## 2                       max f2  0.141569 0.642570 219
## 3                 max f0point5  0.492226 0.658784  90
## 4                 max accuracy  0.492226 0.883568  90
## 5                max precision  0.986183 1.000000   0
## 6                   max recall  0.026760 1.000000 396
## 7              max specificity  0.986183 1.000000   0
## 8             max absolute_mcc  0.292277 0.530175 146
## 9   max min_per_class_accuracy  0.110456 0.750000 249
## 10 max mean_per_class_accuracy  0.141569 0.771398 219
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`</code></pre>
<p><strong>Making Predictions</strong></p>
<pre class="r"><code>predictions &lt;- h2o.predict(deeplearning_h2o, newdata = as.h2o(test_tbl))</code></pre>
<pre class="r"><code># Saving it as a tibble
predictions_tbl &lt;- predictions %&gt;% as.tibble()
predictions_tbl</code></pre>
<pre><code>## # A tibble: 220 x 3
##    predict    No      Yes
##    &lt;fct&gt;   &lt;dbl&gt;    &lt;dbl&gt;
##  1 No      0.983 0.0167  
##  2 No      0.975 0.0250  
##  3 No      0.999 0.000648
##  4 No      0.984 0.0155  
##  5 Yes     0.288 0.712   
##  6 No      0.892 0.108   
##  7 Yes     0.595 0.405   
##  8 No      0.999 0.00117 
##  9 No      0.998 0.00228 
## 10 No      0.999 0.000512
## # … with 210 more rows</code></pre>
<p><strong>Visualizing Leaderboard</strong></p>
<p><strong>Why is there need to Visualize leaderboard ?</strong></p>
<p>Answer to that is <strong>Model Comparison</strong> . We observe that in leaderboard
we get the model with <strong>highest AUC</strong> and <strong>minimial Logloss</strong> but
minimal logloss may not always be lowest for highest AUC model. So, it’s
a good idea to visualize these numbers which we will do later</p>
<ul>
<li>Here, idea is to see how the models are performing in comparison to
each other<br />
</li>
<li>We can also see if the model position gets altered since we have
added model position number<br />
</li>
<li>Also, each model type is represented with a different color in plot</li>
</ul>
<p>Function <strong>plot_h2o_leaderboard </strong> takes in H2O leaderboard and
measure correlation against targeted feature</p>
<pre class="r"><code>plot_h2o_leaderboard &lt;- function(h2o_leaderboard, order_by = c(&quot;auc&quot;, &quot;logloss&quot;),
                                 n_max = 20, size = 4, include_lbl = TRUE) {
  
  #Setup input 
  order_by &lt;- tolower(order_by[[1]])
  
  leaderboard_tbl &lt;- h2o_leaderboard %&gt;%
    as.tibble() %&gt;%
    select(model_id,auc,logloss) %&gt;%
    mutate(model_type = str_split(model_id, &#39;_&#39;, simplify = TRUE)[,1]) %&gt;%
    rownames_to_column(var = &quot;rowname&quot;) %&gt;%
    mutate(model_id = paste0(rowname, &quot;. &quot;,as.character(model_id)) %&gt;% as_factor())
  
  
  # Transformation 
  if(order_by == &quot;auc&quot; ) {
    
    data_transformed_tbl &lt;- leaderboard_tbl %&gt;%
      slice(1:n_max) %&gt;%
      mutate(
        model_id = as_factor(model_id) %&gt;% reorder(auc),
        model_type = as_factor(model_type)
      ) %&gt;%
      gather(key = key, value = value, 
             -c(model_id, model_type, rowname), factor_key = T) 
  } 
  else if(order_by == &quot;logloss&quot; ){
    data_transformed_tbl &lt;- leaderboard_tbl %&gt;%
      slice(1:n_max) %&gt;%
      mutate(
        model_id = as_factor(model_id) %&gt;% reorder(logloss)%&gt;% fct_rev(),
        model_type = as_factor(model_type)
      ) %&gt;%
      gather(key = key, value = value, 
             -c(model_id, model_type, rowname), factor_key = T)
  } else{
    stop(paste0(&quot; order_by = &#39;&quot;, order_by,&quot;&#39; is not a permitted option. &quot;))
  }
  
 g &lt;- data_transformed_tbl %&gt;%
        ggplot(aes(value, model_id, color = model_type)) +
        geom_point(size = size) +
        facet_wrap(~ key, scales = &quot;free_x&quot;) +
        theme_tq() +
        scale_color_tq() +
        labs(title = &quot;Leaderboard Metrics&quot;,
             subtitle = paste0(&quot;Ordered by: &quot;, toupper(order_by)),
             y = &quot;Model Postion, Model ID&quot;, x = &quot;&quot;)
  
  if(include_lbl) g &lt;- g +  geom_label(aes(label  = round(value, 2), hjust = &quot;inward&quot;)) 
  
  return(g)
  
}  </code></pre>
<p><strong>H2O LeaderBoard Performance : Ordered by AUC</strong></p>
<pre class="r"><code>automl_models_h2o@leaderboard %&gt;%
  plot_h2o_leaderboard(order_by = &quot;auc&quot;)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-67-1.png" /></p>
<p>We can see that first 3 models have much better performance than others
w.r.t highest auc and minimal logloss</p>
<p><strong>H2O LeaderBoard Performance : Ordered by LogLoss</strong></p>
<ul>
<li>We observe that now model positions have been changed for Xgboost. In some cases, you will
also find that top model position changes<br />
</li>
<li>After that we see a steep drop between auc and logloss, we might not
need to inspect those models further</li>
</ul>
<pre class="r"><code>automl_models_h2o@leaderboard %&gt;%
  plot_h2o_leaderboard(order_by = &quot;logloss&quot;)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-68-1.png" /></p>
<p><strong>Assessing H2O Performance</strong></p>
<p><strong>h2o.performance()</strong> : Creates an H2O performance object by providing
model and newdata</p>
<pre class="r"><code># Assessing Performance 
performance_h2o &lt;- h2o.performance(stacked_ensemble_h2o, newdata =  as.h2o(test_tbl))</code></pre>
<pre class="r"><code># prints all the classifier metrics 
performance_h2o@metrics</code></pre>
<pre><code>## $model
## $model$`__meta`
## $model$`__meta`$schema_version
## [1] 3
## 
## $model$`__meta`$schema_name
## [1] &quot;ModelKeyV3&quot;
## 
## $model$`__meta`$schema_type
## [1] &quot;Key&lt;Model&gt;&quot;
## 
## 
## $model$name
## [1] &quot;StackedEnsemble_BestOfFamily_AutoML_20190325_101852&quot;
## 
## $model$type
## [1] &quot;Key&lt;Model&gt;&quot;
## 
## $model$URL
## [1] &quot;/3/Models/StackedEnsemble_BestOfFamily_AutoML_20190325_101852&quot;
## 
## 
## $model_checksum
## [1] 7.207601e+18
## 
## $frame
## $frame$name
## [1] &quot;test_tbl_sid_969f_25&quot;
## 
## 
## $frame_checksum
## [1] -5.440343e+16
## 
## $description
## NULL
## 
## $scoring_time
## [1] 1.55349e+12
## 
## $predictions
## NULL
## 
## $MSE
## [1] 0.08300179
## 
## $RMSE
## [1] 0.2881003
## 
## $nobs
## [1] 220
## 
## $custom_metric_name
## NULL
## 
## $custom_metric_value
## [1] 0
## 
## $r2
## [1] 0.3935256
## 
## $logloss
## [1] 0.2878677
## 
## $AUC
## [1] 0.8759058
## 
## $pr_auc
## [1] 0.6685171
## 
## $Gini
## [1] 0.7518116
## 
## $mean_per_class_error
## [1] 0.1826691
## 
## $domain
## [1] &quot;No&quot;  &quot;Yes&quot;
## 
## $cm
## $cm$`__meta`
## $cm$`__meta`$schema_version
## [1] 3
## 
## $cm$`__meta`$schema_name
## [1] &quot;ConfusionMatrixV3&quot;
## 
## $cm$`__meta`$schema_type
## [1] &quot;ConfusionMatrix&quot;
## 
## 
## $cm$table
## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
##         No Yes  Error       Rate
## No     173  11 0.0598 = 11 / 184
## Yes     11  25 0.3056 =  11 / 36
## Totals 184  36 0.1000 = 22 / 220
## 
## 
## $thresholds_and_metric_scores
## Metrics for Thresholds: Binomial metrics as a function of classification thresholds
##   threshold       f1       f2 f0point5 accuracy precision   recall
## 1  0.956717 0.054054 0.034483 0.125000 0.840909  1.000000 0.027778
## 2  0.919105 0.105263 0.068493 0.227273 0.845455  1.000000 0.055556
## 3  0.902517 0.153846 0.102041 0.312500 0.850000  1.000000 0.083333
## 4  0.891069 0.200000 0.135135 0.384615 0.854545  1.000000 0.111111
## 5  0.877346 0.243902 0.167785 0.446429 0.859091  1.000000 0.138889
##   specificity absolute_mcc min_per_class_accuracy mean_per_class_accuracy
## 1    1.000000     0.152769               0.027778                0.513889
## 2    1.000000     0.216543               0.055556                0.527778
## 3    1.000000     0.265820               0.083333                0.541667
## 4    1.000000     0.307653               0.111111                0.555556
## 5    1.000000     0.344765               0.138889                0.569444
##   tns fns fps tps      tnr      fnr      fpr      tpr idx
## 1 184  35   0   1 1.000000 0.972222 0.000000 0.027778   0
## 2 184  34   0   2 1.000000 0.944444 0.000000 0.055556   1
## 3 184  33   0   3 1.000000 0.916667 0.000000 0.083333   2
## 4 184  32   0   4 1.000000 0.888889 0.000000 0.111111   3
## 5 184  31   0   5 1.000000 0.861111 0.000000 0.138889   4
## 
## ---
##     threshold       f1       f2 f0point5 accuracy precision   recall
## 215  0.029937 0.286853 0.501393 0.200893 0.186364  0.167442 1.000000
## 216  0.029886 0.285714 0.500000 0.200000 0.181818  0.166667 1.000000
## 217  0.029809 0.284585 0.498615 0.199115 0.177273  0.165899 1.000000
## 218  0.029739 0.283465 0.497238 0.198238 0.172727  0.165138 1.000000
## 219  0.029695 0.282353 0.495868 0.197368 0.168182  0.164384 1.000000
## 220  0.029617 0.281250 0.494505 0.196507 0.163636  0.163636 1.000000
##     specificity absolute_mcc min_per_class_accuracy
## 215    0.027174     0.067454               0.027174
## 216    0.021739     0.060193               0.021739
## 217    0.016304     0.052008               0.016304
## 218    0.010870     0.042367               0.010870
## 219    0.005435     0.029890               0.005435
## 220    0.000000     0.000000               0.000000
##     mean_per_class_accuracy tns fns fps tps      tnr      fnr      fpr
## 215                0.513587   5   0 179  36 0.027174 0.000000 0.972826
## 216                0.510870   4   0 180  36 0.021739 0.000000 0.978261
## 217                0.508152   3   0 181  36 0.016304 0.000000 0.983696
## 218                0.505435   2   0 182  36 0.010870 0.000000 0.989130
## 219                0.502717   1   0 183  36 0.005435 0.000000 0.994565
## 220                0.500000   0   0 184  36 0.000000 0.000000 1.000000
##          tpr idx
## 215 1.000000 214
## 216 1.000000 215
## 217 1.000000 216
## 218 1.000000 217
## 219 1.000000 218
## 220 1.000000 219
## 
## $max_criteria_and_metric_scores
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.323813 0.694444  35
## 2                       max f2  0.323813 0.694444  35
## 3                 max f0point5  0.525270 0.750000  20
## 4                 max accuracy  0.525270 0.904545  20
## 5                max precision  0.956717 1.000000   0
## 6                   max recall  0.049202 1.000000 147
## 7              max specificity  0.956717 1.000000   0
## 8             max absolute_mcc  0.323813 0.634662  35
## 9   max min_per_class_accuracy  0.147237 0.777778  64
## 10 max mean_per_class_accuracy  0.323813 0.817331  35
## 
## $gains_lift_table
## Gains/Lift Table: Avg response rate: 16.36 %, avg score: 17.37 %
##    group cumulative_data_fraction lower_threshold     lift cumulative_lift
## 1      1               0.01363636        0.900342 6.111111        6.111111
## 2      2               0.02272727        0.860247 6.111111        6.111111
## 3      3               0.03181818        0.808241 3.055556        5.238095
## 4      4               0.04090909        0.777759 6.111111        5.432099
## 5      5               0.05000000        0.745066 6.111111        5.555556
## 6      6               0.10000000        0.480243 4.444444        5.000000
## 7      7               0.15000000        0.333605 2.222222        4.074074
## 8      8               0.20000000        0.264923 1.666667        3.472222
## 9      9               0.30000000        0.145660 0.833333        2.592593
## 10    10               0.40000000        0.101660 0.555556        2.083333
## 11    11               0.50000000        0.070095 0.277778        1.722222
## 12    12               0.60000000        0.055868 0.833333        1.574074
## 13    13               0.70000000        0.046774 0.555556        1.428571
## 14    14               0.80000000        0.040402 0.000000        1.250000
## 15    15               0.90000000        0.033981 0.000000        1.111111
## 16    16               1.00000000        0.029617 0.000000        1.000000
##    response_rate    score cumulative_response_rate cumulative_score
## 1       1.000000 0.926113                 1.000000         0.926113
## 2       1.000000 0.884208                 1.000000         0.909351
## 3       0.500000 0.825633                 0.857143         0.885431
## 4       1.000000 0.791051                 0.888889         0.864458
## 5       1.000000 0.766819                 0.909091         0.846705
## 6       0.727273 0.629001                 0.818182         0.737853
## 7       0.363636 0.411330                 0.666667         0.629012
## 8       0.272727 0.305821                 0.568182         0.548214
## 9       0.136364 0.207113                 0.424242         0.434514
## 10      0.090909 0.122424                 0.340909         0.356491
## 11      0.045455 0.086237                 0.281818         0.302440
## 12      0.136364 0.061947                 0.257576         0.262358
## 13      0.090909 0.051162                 0.233766         0.232187
## 14      0.000000 0.042950                 0.204545         0.208533
## 15      0.000000 0.037123                 0.181818         0.189487
## 16      0.000000 0.031407                 0.163636         0.173679
##    capture_rate cumulative_capture_rate        gain cumulative_gain
## 1      0.083333                0.083333  511.111111      511.111111
## 2      0.055556                0.138889  511.111111      511.111111
## 3      0.027778                0.166667  205.555556      423.809524
## 4      0.055556                0.222222  511.111111      443.209877
## 5      0.055556                0.277778  511.111111      455.555556
## 6      0.222222                0.500000  344.444444      400.000000
## 7      0.111111                0.611111  122.222222      307.407407
## 8      0.083333                0.694444   66.666667      247.222222
## 9      0.083333                0.777778  -16.666667      159.259259
## 10     0.055556                0.833333  -44.444444      108.333333
## 11     0.027778                0.861111  -72.222222       72.222222
## 12     0.083333                0.944444  -16.666667       57.407407
## 13     0.055556                1.000000  -44.444444       42.857143
## 14     0.000000                1.000000 -100.000000       25.000000
## 15     0.000000                1.000000 -100.000000       11.111111
## 16     0.000000                1.000000 -100.000000        0.000000
## 
## $residual_deviance
## [1] 126.6618
## 
## $null_deviance
## [1] 196.0906
## 
## $AIC
## [1] 136.6618
## 
## $null_degrees_of_freedom
## [1] 219
## 
## $residual_degrees_of_freedom
## [1] 215</code></pre>
<p>We will now see how to extract some of the above metrics individually
using <em>performance object</em></p>
<p><strong>Classifier Summary metrics</strong></p>
<p><strong>Area Under Curve</strong><br />
It’s a way of measuring the performance of a binary classifier by
comparing the False Positive Rate (FPR x-axis) to True positive rate
(TPR y-axis)</p>
<pre class="r"><code># Retreives area under curve (AUC) for a classifier. If passing an h2O Model, can use the xval argument
# to retreive the average cross validation AUC
h2o.auc(performance_h2o)</code></pre>
<pre><code>## [1] 0.8759058</code></pre>
<p>We can see that accuracy is 87.6% on test data.</p>
<p>Arguments train, valid and xval only works for models, not for
performance objects.</p>
<pre class="r"><code>h2o.auc(stacked_ensemble_h2o, train = TRUE, valid =  TRUE, xval = TRUE)</code></pre>
<pre><code>##     train     valid      xval 
## 0.9829226 0.8130000 0.8346425</code></pre>
<p>This model has 98% accuracy on training set, 81% accuracy on validation
set &amp; 83% accuracy on cross validation set.</p>
<p><strong>Gini Coefficient</strong></p>
<ul>
<li>Gini coefficient is a statistic which measures the ability of a
scorecard or a characteristic to rank order risk.<br />
</li>
<li>A Gini value of 0% means that the characteristic cannot distinguish
good from bad cases. In general, Gini coeff above 60% is a good
model.<br />
</li>
<li>AUC = (GiniCoeff + 1) / 2</li>
</ul>
<pre class="r"><code>h2o.giniCoef(performance_h2o)</code></pre>
<pre><code>## [1] 0.7518116</code></pre>
<p>We have GiniCoeff of 0.75 which implies it is a good model</p>
<p><strong>Logloss</strong></p>
<ul>
<li>Log Loss quantifies the accuracy of a classifier by penalising false
classifications.<br />
</li>
<li>Great way to Measure the true performance of classifier by comparing
the class probability to actual value (1 or 0).<br />
</li>
<li>Minimising log loss gives greater accuracy for the classifier.</li>
</ul>
<pre class="r"><code>h2o.logloss(performance_h2o)</code></pre>
<pre><code>## [1] 0.2878677</code></pre>
<p><strong>Confusion Matrix</strong></p>
<ul>
<li>Used for summarizing the performance of a classification algorithm.</li>
<li>It shows the ways in which your classification model is confused
when making predictions.</li>
<li>Critical for <em>Business Analysis</em></li>
<li>Confusion matrix varies based on threshold ( value that determines
which class probability is 1 or 0).</li>
</ul>
<pre class="r"><code>h2o.confusionMatrix(stacked_ensemble_h2o)</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.409975269140079:
##         No Yes    Error      Rate
## No     873  16 0.017998   =16/889
## Yes     37 139 0.210227   =37/176
## Totals 910 155 0.049765  =53/1065</code></pre>
<pre class="r"><code>h2o.confusionMatrix(performance_h2o)</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.323813276742962:
##         No Yes    Error     Rate
## No     173  11 0.059783  =11/184
## Yes     11  25 0.305556   =11/36
## Totals 184  36 0.100000  =22/220</code></pre>
<p>We need to be able to undestand how these model change with thresholds
using <strong>h2o.metric()</strong> (Converts a performance object into a series of
metrics (e.g precision, recall, f1 etc) that vary by threshold)</p>
<p><strong>Important measures that vary by threshold :</strong></p>
<p><strong>Precision</strong></p>
<ul>
<li>Measures false positives (e.g predicted to leave but actually stayed
)<br />
</li>
<li>Precision = TP / (TP +FP) # 24 /(24+14) = 0.63</li>
<li>In other words, it detects how frequently your algorithm over-picks
the Yes class<br />
</li>
<li>Precision in business context: Precision indicates how often we
incorrectly say people will leave when they actually will stay</li>
</ul>
<p><strong>Recall</strong></p>
<ul>
<li>Measures false negatives (e.g predicted to stay but actually left)<br />
</li>
<li>Recall = TP / (TP + FN) # 24 /(24+12) = 0.66</li>
<li>In other words, it provides a metric for under-picking the Yes
class<br />
</li>
<li>Recall is typically more important than Precision in the business
context. We would rather give up some false positives (inadeverently
target stayers) to gain false negatives (accurately predict
quiters) * Recall in business context: Recall indicates how often
we miss people that will leave by incorrectly predicting they will
stay</li>
</ul>
<p><strong>F1</strong></p>
<ul>
<li>Optimal balance between precision and recall. Typically the
threshold that maximises F1 is used as threshold cutoff for turning
class probability into 0/1. However this is not always the best
case, An expected value optimization is required when costs of false
positives and false negatives are known<br />
</li>
<li>F1 = 2 (precision x recall) / (precision + recall) # 2 x (0.63 x
0.66) / (0.63+0.66) = 0.64<br />
</li>
<li>In other words, it provides a metric for balancing precision vs
recall<br />
</li>
<li>Max F1 Thresholds: Threshold that optimizes the balance between
precision and recall</li>
</ul>
<p><strong>More Important Measures</strong></p>
<ul>
<li>True positives (tps), True negatives (tns), False positives (fps),
and False negatives (fns) are often converted into rates to
understand cost / benefit of classifier<br />
</li>
<li>Rates are included as tpr, tnr, fpr and fnr</li>
</ul>
<pre class="r"><code>performance_h2o %&gt;%
# h2o.metric(): Returns the classifier performance
  h2o.metric() %&gt;%
  as.tibble() %&gt;%
  glimpse()</code></pre>
<pre><code>## Observations: 220
## Variables: 20
## $ threshold               &lt;dbl&gt; 0.9567169, 0.9191046, 0.9025171, 0.89106…
## $ f1                      &lt;dbl&gt; 0.05405405, 0.10526316, 0.15384615, 0.20…
## $ f2                      &lt;dbl&gt; 0.03448276, 0.06849315, 0.10204082, 0.13…
## $ f0point5                &lt;dbl&gt; 0.1250000, 0.2272727, 0.3125000, 0.38461…
## $ accuracy                &lt;dbl&gt; 0.8409091, 0.8454545, 0.8500000, 0.85454…
## $ precision               &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.00000…
## $ recall                  &lt;dbl&gt; 0.02777778, 0.05555556, 0.08333333, 0.11…
## $ specificity             &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.00000…
## $ absolute_mcc            &lt;dbl&gt; 0.1527691, 0.2165431, 0.2658205, 0.30765…
## $ min_per_class_accuracy  &lt;dbl&gt; 0.02777778, 0.05555556, 0.08333333, 0.11…
## $ mean_per_class_accuracy &lt;dbl&gt; 0.5138889, 0.5277778, 0.5416667, 0.55555…
## $ tns                     &lt;dbl&gt; 184, 184, 184, 184, 184, 184, 183, 183, …
## $ fns                     &lt;dbl&gt; 35, 34, 33, 32, 31, 30, 30, 29, 28, 27, …
## $ fps                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2…
## $ tps                     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11, 12…
## $ tnr                     &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.00000…
## $ fnr                     &lt;dbl&gt; 0.9722222, 0.9444444, 0.9166667, 0.88888…
## $ fpr                     &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0…
## $ tpr                     &lt;dbl&gt; 0.02777778, 0.05555556, 0.08333333, 0.11…
## $ idx                     &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…</code></pre>
<p>If we vary the threshold, metrics completely changed and if we do that
over 1 to 0 we can see how that affects the model across various
metrics.</p>
<p><strong>Gain &amp; Lift</strong></p>
<p>Emphasizes how much model improves results</p>
<p><strong>Ranked Predictions :</strong> By ranking by class probability of Yes, we
assess the models ability to truly detect someone that is leaving</p>
<pre class="r"><code>ranked_predictions_tbl &lt;- predictions_tbl %&gt;%
  bind_cols(test_tbl) %&gt;%
  select(predict:Yes, Attrition) %&gt;%
  arrange(desc(Yes))

ranked_predictions_tbl</code></pre>
<pre><code>## # A tibble: 220 x 4
##    predict     No   Yes Attrition
##    &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;fct&gt;    
##  1 Yes     0.0180 0.982 Yes      
##  2 Yes     0.0389 0.961 Yes      
##  3 Yes     0.0770 0.923 Yes      
##  4 Yes     0.103  0.897 Yes      
##  5 Yes     0.115  0.885 Yes      
##  6 Yes     0.142  0.858 Yes      
##  7 Yes     0.143  0.857 Yes      
##  8 Yes     0.159  0.841 Yes      
##  9 Yes     0.264  0.736 No       
## 10 Yes     0.288  0.712 Yes      
## # … with 210 more rows</code></pre>
<ul>
<li><p>In first 10, we have 90% accuracy (9 of 10 people)</p></li>
<li><p>Without model, we’d only expect the global attrition rate for first
10 (16% or 1.6 people)</p></li>
<li><p>If total expected quiters is 220 x 0.16 = 35</p></li>
<li><p>Gain : If 35 people expected to quit, we gained 9 of 35 or 25.7% in
first 10 cases</p></li>
<li><p>Lift : If the expectation is 1.6 people, we beat the expectation by
9/1.6 = 5.6X in first 10 cases</p></li>
<li><p><strong>Grouping By Likelihood:</strong> Grouping into cohorts of most likely to
least likely groups is as the heart of Gain/Lift chart. When we do
this, we can immediately show that if candidate has high prob of
leaving, how likely they are to leave.</p></li>
<li><p><strong>ntile:</strong> Thinking of grouping by ntile as splitting up a
continuious variable into n buckets This allows us to group the
group the Response (Attrition) based on ntile column.</p></li>
<li><p><strong>Cumulative Gain:</strong> Technically this is cumulative gain, because we
cumulatively sum the pct_responses</p></li>
</ul>
<pre class="r"><code>calculated_gain_lift_tbl &lt;- ranked_predictions_tbl %&gt;%
  mutate(ntile = ntile(Yes, n = 10)) %&gt;%
  group_by(ntile) %&gt;%
  summarise(
    cases = n(),
    responses = sum(Attrition == &quot;Yes&quot;)
  ) %&gt;%
  arrange(desc(ntile)) %&gt;%
  mutate(group = row_number()) %&gt;%
  select(group, cases, responses) %&gt;%
  mutate(
    cumulative_responses = cumsum(responses),
    pct_responses        =  responses/sum(responses),
    gain                 =  cumsum(pct_responses),
    cumulative_pct_cases =  cumsum(cases)/sum(cases),
    lift                 =  gain/cumulative_pct_cases,
    gain_baseline        =  cumulative_pct_cases,
    lift_baseline        =  gain_baseline / cumulative_pct_cases
  )</code></pre>
<p>We note that the 10th decile i.e this group has the highest class
probabilities for leaving, 18 of 22 actually left.</p>
<p><strong>Gain</strong></p>
<ul>
<li>Think of this as what we gained using the model. For example, if we
focused on first 2 groups , we gain ability to target 69.4% of our
quiters using our model</li>
<li>Cumulative gain baseline is always equal to cumulative ntile
percentage</li>
</ul>
<p><strong>Lift</strong></p>
<ul>
<li>Think of this as multiplier between what we gained divided by what
we expected to gain with no model. For example, if we focused on the
first two decile groups, we gained ability to target 69.4% of our
quiters but we only expected to be able to target 20% of the quiters
in 2 decile groups.<br />
</li>
<li>Cumulative lift baseline is always equal to 1.<br />
</li>
<li>Group: H2o groups into 16 ntiles with the first being the most
likely to be in the minority (Yes) class 16 n-tiles<br />
</li>
<li>220/ 16 n-tiles = 13.75 people / group<br />
</li>
<li>16 n-tiles gives great resolution with about 14 people in each group</li>
</ul>
<p>Defining terms * Cumulative Capture Rate : This is <em>cumulative
percentage gain</em> that we will use<br />
* Cumulative Lift : This is the <em>cumulative lift</em> that we will use</p>
<pre class="r"><code>gain_lift_tbl &lt;- performance_h2o %&gt;%
   h2o.gainsLift() %&gt;%
   as.tibble()

gain_lift_tbl</code></pre>
<pre><code>## # A tibble: 16 x 13
##    group cumulative_data_fraction lower_threshold  lift cumulative_lift
##    &lt;int&gt;                    &lt;dbl&gt;           &lt;dbl&gt; &lt;dbl&gt;           &lt;dbl&gt;
##  1     1                   0.0136          0.900  6.11             6.11
##  2     2                   0.0227          0.860  6.11             6.11
##  3     3                   0.0318          0.808  3.06             5.24
##  4     4                   0.0409          0.778  6.11             5.43
##  5     5                   0.05            0.745  6.11             5.56
##  6     6                   0.1             0.480  4.44             5   
##  7     7                   0.15            0.334  2.22             4.07
##  8     8                   0.2             0.265  1.67             3.47
##  9     9                   0.3             0.146  0.833            2.59
## 10    10                   0.4             0.102  0.556            2.08
## 11    11                   0.5             0.0701 0.278            1.72
## 12    12                   0.6             0.0559 0.833            1.57
## 13    13                   0.7             0.0468 0.556            1.43
## 14    14                   0.8             0.0404 0                1.25
## 15    15                   0.9             0.0340 0                1.11
## 16    16                   1               0.0296 0                1   
##    response_rate  score cumulative_response_rate cumulative_score
##            &lt;dbl&gt;  &lt;dbl&gt;                    &lt;dbl&gt;            &lt;dbl&gt;
##  1        1      0.926                     1                0.926
##  2        1      0.884                     1                0.909
##  3        0.5    0.826                     0.857            0.885
##  4        1      0.791                     0.889            0.864
##  5        1      0.767                     0.909            0.847
##  6        0.727  0.629                     0.818            0.738
##  7        0.364  0.411                     0.667            0.629
##  8        0.273  0.306                     0.568            0.548
##  9        0.136  0.207                     0.424            0.435
## 10        0.0909 0.122                     0.341            0.356
## 11        0.0455 0.0862                    0.282            0.302
## 12        0.136  0.0619                    0.258            0.262
## 13        0.0909 0.0512                    0.234            0.232
## 14        0      0.0429                    0.205            0.209
## 15        0      0.0371                    0.182            0.189
## 16        0      0.0314                    0.164            0.174
##    capture_rate cumulative_capture_rate   gain cumulative_gain
##           &lt;dbl&gt;                   &lt;dbl&gt;  &lt;dbl&gt;           &lt;dbl&gt;
##  1       0.0833                  0.0833  511.            511. 
##  2       0.0556                  0.139   511.            511. 
##  3       0.0278                  0.167   206.            424. 
##  4       0.0556                  0.222   511.            443. 
##  5       0.0556                  0.278   511.            456. 
##  6       0.222                   0.5     344.            400  
##  7       0.111                   0.611   122.            307. 
##  8       0.0833                  0.694    66.7           247. 
##  9       0.0833                  0.778   -16.7           159. 
## 10       0.0556                  0.833   -44.4           108. 
## 11       0.0278                  0.861   -72.2            72.2
## 12       0.0833                  0.944   -16.7            57.4
## 13       0.0556                  1       -44.4            42.9
## 14       0                       1      -100              25  
## 15       0                       1      -100              11.1
## 16       0                       1      -100               0</code></pre>
<p><strong>Model Performance Comparison Dashboard</strong></p>
<p>Visualizing important model metrics and Gain &amp; lift chart in this
dashboard</p>
<ul>
<li><strong>ROC Plot</strong>: Compares AUC of different models<br />
</li>
<li><strong>Precision &amp; Recall Plot</strong>: Highly useful in comparing binary
classification models<br />
</li>
<li><strong>Gain &amp; Lift Chart:</strong> Charts designed to communicate model
performance to non-technical stakeholders</li>
</ul>
<pre class="r"><code># Performance Visualization 

plot_h2o_performance &lt;- function(h2o_leaderboard, newdata, order_by = c(&quot;auc&quot;, &quot;logloss&quot;),
                                 max_models = 3, size = 1.5) {
    
    # Inputs
    
    leaderboard_tbl &lt;- h2o_leaderboard %&gt;%
        as.tibble() %&gt;%
        slice(1:max_models)
    
    newdata_tbl &lt;- newdata %&gt;%
        as.tibble()
    
    order_by &lt;- tolower(order_by[[1]])
    order_by_expr &lt;- rlang::sym(order_by)
    
    h2o.no_progress()
    
    # Model metrics
    
    get_model_performance_metrics &lt;- function(model_id, test_tbl) {
        
        model_h2o &lt;- h2o.getModel(model_id)
        perf_h2o  &lt;- h2o.performance(model_h2o, newdata = as.h2o(test_tbl))
        
        perf_h2o %&gt;%
            h2o.metric() %&gt;%
            as.tibble() %&gt;%
            select(threshold, tpr, fpr, precision, recall)
        
    }
    
    model_metrics_tbl &lt;- leaderboard_tbl %&gt;%
        mutate(metrics = map(model_id, get_model_performance_metrics, newdata_tbl)) %&gt;%
        unnest() %&gt;%
        mutate(
            model_id = as_factor(model_id) %&gt;% 
                fct_reorder(!! order_by_expr, .desc = ifelse(order_by == &quot;auc&quot;, TRUE, FALSE)),
            auc  = auc %&gt;% 
                round(3) %&gt;% 
                as.character() %&gt;% 
                as_factor() %&gt;% 
                fct_reorder(as.numeric(model_id)),
            logloss = logloss %&gt;% 
                round(4) %&gt;% 
                as.character() %&gt;% 
                as_factor() %&gt;% 
                fct_reorder(as.numeric(model_id))
        )
    
    
    # ROC Plot
    
    p1 &lt;- model_metrics_tbl %&gt;%
        ggplot(aes_string(&quot;fpr&quot;, &quot;tpr&quot;, color = &quot;model_id&quot;, linetype = order_by)) +
        geom_line(size = size) +
        theme_tq() +
        scale_color_tq() +
        labs(title = &quot;ROC&quot;, x = &quot;FPR&quot;, y = &quot;TPR&quot;) +
        theme(legend.direction = &quot;vertical&quot;)
    
    # Precision vs Recall
    
    p2 &lt;- model_metrics_tbl %&gt;%
        ggplot(aes_string(&quot;recall&quot;, &quot;precision&quot;, color = &quot;model_id&quot;, linetype = order_by)) +
        geom_line(size = size) +
        theme_tq() +
        scale_color_tq() +
        labs(title = &quot;Precision Vs Recall&quot;, x = &quot;Recall&quot;, y = &quot;Precision&quot;) +
        theme(legend.position = &quot;none&quot;)
    
    
    # Gain / Lift
    
    get_gain_lift &lt;- function(model_id, test_tbl) {
        
        model_h2o &lt;- h2o.getModel(model_id)
        perf_h2o  &lt;- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) 
        
        perf_h2o %&gt;%
            h2o.gainsLift() %&gt;%
            as.tibble() %&gt;%
            select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift)
        
    }
    
    gain_lift_tbl &lt;- leaderboard_tbl %&gt;%
        mutate(metrics = map(model_id, get_gain_lift, newdata_tbl)) %&gt;%
        unnest() %&gt;%
        mutate(
            model_id = as_factor(model_id) %&gt;% 
                fct_reorder(!! order_by_expr, .desc = ifelse(order_by == &quot;auc&quot;, TRUE, FALSE)),
            auc  = auc %&gt;% 
                round(3) %&gt;% 
                as.character() %&gt;% 
                as_factor() %&gt;% 
                fct_reorder(as.numeric(model_id)),
            logloss = logloss %&gt;% 
                round(4) %&gt;% 
                as.character() %&gt;% 
                as_factor() %&gt;% 
                fct_reorder(as.numeric(model_id))
        ) %&gt;%
        rename(
            gain = cumulative_capture_rate,
            lift = cumulative_lift
        ) 
    
    #  Gain Plot
    
    p3 &lt;- gain_lift_tbl %&gt;%
        ggplot(aes_string(&quot;cumulative_data_fraction&quot;, &quot;gain&quot;, 
                          color = &quot;model_id&quot;, linetype = order_by)) +
        geom_line(size = size) +
        geom_segment(x = 0, y = 0, xend = 1, yend = 1, 
                     color = &quot;black&quot;, size = size) +
        theme_tq() +
        scale_color_tq() +
        expand_limits(x = c(0, 1), y = c(0, 1)) +
        labs(title = &quot;Gain&quot;,
             x = &quot;Cumulative Data Fraction&quot;, y = &quot;Gain&quot;) +
        theme(legend.position = &quot;none&quot;)
    
    # Lift Plot
    
    p4 &lt;- gain_lift_tbl %&gt;%
        ggplot(aes_string(&quot;cumulative_data_fraction&quot;, &quot;lift&quot;, 
                          color = &quot;model_id&quot;, linetype = order_by)) +
        geom_line(size = size) +
        geom_segment(x = 0, y = 1, xend = 1, yend = 1, 
                     color = &quot;black&quot;, size = size) +
        theme_tq() +
        scale_color_tq() +
        expand_limits(x = c(0, 1), y = c(0, 1)) +
        labs(title = &quot;Lift&quot;,
             x = &quot;Cumulative Data Fraction&quot;, y = &quot;Lift&quot;) +
        theme(legend.position = &quot;none&quot;)
    
    
    # Combine using cowplot
    p_legend &lt;- get_legend(p1)
    p1 &lt;- p1 + theme(legend.position = &quot;none&quot;)
    
    p &lt;- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2) 
    
    p_title &lt;- ggdraw() + 
        draw_label(&quot;H2O Model Metrics&quot;, size = 18, fontface = &quot;bold&quot;, 
                   colour = palette_light()[[1]])
    
    p_subtitle &lt;- ggdraw() + 
        draw_label(glue(&quot;Ordered by {toupper(order_by)}&quot;), size = 10,  
                   colour = palette_light()[[1]])
    
    ret &lt;- plot_grid(p_title, p_subtitle, p, p_legend, 
                     ncol = 1, rel_heights = c(0.05, 0.05, 1, 0.05 * max_models))
    
    h2o.show_progress()
    
    return(ret)
    
}</code></pre>
<p><strong>Model Performance Comparison Dashboard : Ordered by AUC</strong></p>
<pre class="r"><code>automl_models_h2o@leaderboard %&gt;%
    plot_h2o_performance(newdata = test_tbl, order_by = &quot;auc&quot;, 
                         size = 1, max_models = 4)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-81-1.png" /></p>
<p><strong>Takeaways</strong></p>
<ul>
<li>We can conclude from ROC Curve that GLM model has higher AUC than
Stacked Ensemble.<br />
</li>
<li>For Stacked Ensemble Best of Family models, precision drops off very
quickly as recall increases which is not desirable.<br />
</li>
<li>We can notice from Precision &amp; Recall plot that black one i.e GLM
has better recall than others especially early one and precision
begins to drop for all 3 models.<br />
</li>
<li>We can notice from <em>Gain Plot</em> that organisation can get 69% of
gains by just focusing on the top 25% of employees as compared to
baseline model where gains are only 25%.</li>
<li>We notice from lift chart, we get 3x positive responses as compared
to basline model which has been lifted from 1x.<br />
</li>
<li>The two charts work together to show the results of using the
modeling approach versus just targeting people at random</li>
<li>But all these 3 models seems to be performing very similarly.</li>
</ul>
<p><strong>Model Performance Comparison Dashboard : Ordered by Logloss</strong></p>
<p>On a similar note, we can also choose the best model based on minimal
logloss</p>
<pre class="r"><code>automl_models_h2o@leaderboard %&gt;%
    plot_h2o_performance(newdata = test_tbl, order_by = &quot;logloss&quot;, 
                         size = 1, max_models = 4)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-82-1.png" /></p>
<p>How does lift apply to Attrition ?</p>
<ul>
<li>Example: Providing stock options strategically to high performers
that are high risk<br />
</li>
<li>Could strategically focus on those in the high risk category that
are working time<br />
</li>
<li>Stock options Assignment would be 3x more effective with the model</li>
</ul>
<p><strong>Communicating to Executive</strong></p>
<ul>
<li>If 35 people are expected to quit, we can obtain 69% gain by
targeting the top 25% of high risk people.<br />
</li>
<li>This reduces costs by 1/3rd versus random selection because we only
need to offer stock options to high risk candidates.</li>
</ul>
</div>
<div id="explaining-black-box-classification-models-with-lime" class="section level3">
<h3>Explaining Black-Box Classification Models With LIME</h3>
<ul>
<li>Models that tend to be highly predictive like stacked ensembles are
not interpretable by normal means which is a serious issue.<br />
</li>
<li>Explanations are critical to business</li>
<li>Key question here is why is churn happening ?<br />
</li>
<li>Knowledge empowers to make better decisions</li>
</ul>
<p><strong>LIME - Local Interpretable Model-Agnostic Explanations package</strong></p>
<ul>
<li>Used to determine which features contribute to prediction (&amp; by how
much) for a single observation (i.e local).<br />
</li>
<li>Every complex model is linear at local level.<br />
</li>
<li>Performs localized regression 1000x and returns results which are
then averaged. Features with highest importance becomes the
expalanation.<br />
</li>
<li>LIME can be used to explain Black-Box classification models e.g deep
learning, stacked ensembles and random forest.<br />
</li>
<li>AutoML + LIME can be applied to any classification problem in
business like customer churn, fraud detection.</li>
</ul>
<pre class="r"><code>automl_leader &lt;- h2o.loadModel(&quot;04_Modeling/h20_models/StackedEnsemble_BestOfFamily_AutoML_20190325_101852&quot;)

# Making Predictions

predictions_tbl &lt;- automl_leader %&gt;% 
  h2o.predict(newdata = as.h2o(test_tbl)) %&gt;%
  as.tibble() %&gt;%
  bind_cols(
    test_tbl %&gt;%
      select(Attrition, EmployeeNumber)
  )</code></pre>
<p><strong>Steps Involved in LIME</strong></p>
<ol style="list-style-type: decimal">
<li>Build explainer with lime()<br />
</li>
<li>Create an explanation with explain()</li>
</ol>
<p><strong>Multiple Explanations</strong></p>
<p><strong>Using lime::explain() function to build explainer</strong></p>
<ul>
<li>Use bin_continuous to bin the features. It makes it easy to detect
what causes the continuous feature to have a high feature weight
explanation</li>
<li>Using n_bins to tell how many bins you want</li>
<li>Using quantile_bins to tell how to distribute observations with the
bins. If True, cuts will be selected to evenly distribute the total
observations within each of bins</li>
</ul>
<pre class="r"><code>explainer &lt;- train_tbl %&gt;%
  select(-Attrition) %&gt;%
  lime(
    model           =  automl_leader,
    bin_continuous  =  TRUE,
    n_bins          =  4, 
    quantile_bins   = TRUE
    )

explainer</code></pre>
<pre><code>## $model
## Model Details:
## ==============
## 
## H2OBinomialModel: stackedensemble
## Model ID:  StackedEnsemble_BestOfFamily_AutoML_20190325_101852 
## NULL
## 
## 
## H2OBinomialMetrics: stackedensemble
## ** Reported on training data. **
## 
## MSE:  0.0425205
## RMSE:  0.206205
## LogLoss:  0.1623063
## Mean Per-Class Error:  0.1141125
## AUC:  0.9829226
## pr_auc:  0.9185428
## Gini:  0.9658452
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##         No Yes    Error      Rate
## No     873  16 0.017998   =16/889
## Yes     37 139 0.210227   =37/176
## Totals 910 155 0.049765  =53/1065
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.409975 0.839879 127
## 2                       max f2  0.140350 0.877551 225
## 3                 max f0point5  0.523454 0.885714 105
## 4                 max accuracy  0.409975 0.950235 127
## 5                max precision  0.986089 1.000000   0
## 6                   max recall  0.094748 1.000000 264
## 7              max specificity  0.986089 1.000000   0
## 8             max absolute_mcc  0.409975 0.812852 127
## 9   max min_per_class_accuracy  0.185882 0.914773 194
## 10 max mean_per_class_accuracy  0.140350 0.930144 225
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## H2OBinomialMetrics: stackedensemble
## ** Reported on validation data. **
## 
## MSE:  0.08123465
## RMSE:  0.2850169
## LogLoss:  0.2863561
## Mean Per-Class Error:  0.245
## AUC:  0.813
## pr_auc:  0.5554092
## Gini:  0.626
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##         No Yes    Error     Rate
## No     152   8 0.050000   =8/160
## Yes     11  14 0.440000   =11/25
## Totals 163  22 0.102703  =19/185
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.346963 0.595745  21
## 2                       max f2  0.229984 0.629630  34
## 3                 max f0point5  0.678260 0.660377   6
## 4                 max accuracy  0.678260 0.902703   6
## 5                max precision  0.978007 1.000000   0
## 6                   max recall  0.035646 1.000000 158
## 7              max specificity  0.978007 1.000000   0
## 8             max absolute_mcc  0.346963 0.538636  21
## 9   max min_per_class_accuracy  0.092737 0.737500  60
## 10 max mean_per_class_accuracy  0.229984 0.783750  34
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## H2OBinomialMetrics: stackedensemble
## ** Reported on cross-validation data. **
## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
## 
## MSE:  0.09294898
## RMSE:  0.3048753
## LogLoss:  0.3169679
## Mean Per-Class Error:  0.2331143
## AUC:  0.8346425
## pr_auc:  0.6259658
## Gini:  0.6692849
## 
## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
##         No Yes    Error       Rate
## No     818  71 0.079865    =71/889
## Yes     68 108 0.386364    =68/176
## Totals 886 179 0.130516  =139/1065
## 
## Maximum Metrics: Maximum metrics at their respective thresholds
##                         metric threshold    value idx
## 1                       max f1  0.292277 0.608451 146
## 2                       max f2  0.141569 0.642570 219
## 3                 max f0point5  0.492226 0.658784  90
## 4                 max accuracy  0.492226 0.883568  90
## 5                max precision  0.986183 1.000000   0
## 6                   max recall  0.026760 1.000000 396
## 7              max specificity  0.986183 1.000000   0
## 8             max absolute_mcc  0.292277 0.530175 146
## 9   max min_per_class_accuracy  0.110456 0.750000 249
## 10 max mean_per_class_accuracy  0.141569 0.771398 219
## 
## Gains/Lift Table: Extract with `h2o.gainsLift(&lt;model&gt;, &lt;data&gt;)` or `h2o.gainsLift(&lt;model&gt;, valid=&lt;T/F&gt;, xval=&lt;T/F&gt;)`
## 
## $bin_continuous
## [1] TRUE
## 
## $n_bins
## [1] 4
## 
## $quantile_bins
## [1] TRUE
## 
## $use_density
## [1] TRUE
## 
## $feature_type
##                      Age           BusinessTravel                DailyRate 
##                &quot;numeric&quot;                 &quot;factor&quot;                &quot;numeric&quot; 
##               Department         DistanceFromHome                Education 
##                 &quot;factor&quot;                &quot;numeric&quot;                 &quot;factor&quot; 
##           EducationField           EmployeeNumber  EnvironmentSatisfaction 
##                 &quot;factor&quot;                &quot;numeric&quot;                 &quot;factor&quot; 
##                   Gender               HourlyRate           JobInvolvement 
##                 &quot;factor&quot;                &quot;numeric&quot;                 &quot;factor&quot; 
##                 JobLevel                  JobRole          JobSatisfaction 
##                 &quot;factor&quot;                 &quot;factor&quot;                 &quot;factor&quot; 
##            MaritalStatus            MonthlyIncome              MonthlyRate 
##                 &quot;factor&quot;                &quot;numeric&quot;                &quot;numeric&quot; 
##       NumCompaniesWorked                 OverTime        PercentSalaryHike 
##                &quot;numeric&quot;                 &quot;factor&quot;                &quot;numeric&quot; 
##        PerformanceRating RelationshipSatisfaction         StockOptionLevel 
##                 &quot;factor&quot;                 &quot;factor&quot;                 &quot;factor&quot; 
##        TotalWorkingYears    TrainingTimesLastYear          WorkLifeBalance 
##                &quot;numeric&quot;                &quot;numeric&quot;                 &quot;factor&quot; 
##           YearsAtCompany       YearsInCurrentRole  YearsSinceLastPromotion 
##                &quot;numeric&quot;                &quot;numeric&quot;                &quot;numeric&quot; 
##     YearsWithCurrManager 
##                &quot;numeric&quot; 
## 
## $bin_cuts
## $bin_cuts$Age
##   0%  25%  50%  75% 100% 
##   18   30   36   43   60 
## 
## $bin_cuts$BusinessTravel
## NULL
## 
## $bin_cuts$DailyRate
##   0%  25%  50%  75% 100% 
##  102  464  798 1156 1498 
## 
## $bin_cuts$Department
## NULL
## 
## $bin_cuts$DistanceFromHome
##    0%   25%   50%   75%  100% 
##  1.00  2.00  7.00 13.75 29.00 
## 
## $bin_cuts$Education
## NULL
## 
## $bin_cuts$EducationField
## NULL
## 
## $bin_cuts$EmployeeNumber
##      0%     25%     50%     75%    100% 
##    1.00  486.25 1020.50 1553.75 2068.00 
## 
## $bin_cuts$EnvironmentSatisfaction
## NULL
## 
## $bin_cuts$Gender
## NULL
## 
## $bin_cuts$HourlyRate
##   0%  25%  50%  75% 100% 
##   30   48   66   83  100 
## 
## $bin_cuts$JobInvolvement
## NULL
## 
## $bin_cuts$JobLevel
## NULL
## 
## $bin_cuts$JobRole
## NULL
## 
## $bin_cuts$JobSatisfaction
## NULL
## 
## $bin_cuts$MaritalStatus
## NULL
## 
## $bin_cuts$MonthlyIncome
##       0%      25%      50%      75%     100% 
##  1009.00  2935.25  4903.50  8395.00 19999.00 
## 
## $bin_cuts$MonthlyRate
##       0%      25%      50%      75%     100% 
##  2097.00  8191.25 14328.00 20337.25 26999.00 
## 
## $bin_cuts$NumCompaniesWorked
##   0%  25%  50%  75% 100% 
##    0    1    2    4    9 
## 
## $bin_cuts$OverTime
## NULL
## 
## $bin_cuts$PercentSalaryHike
##   0%  25%  50%  75% 100% 
##   11   12   14   18   25 
## 
## $bin_cuts$PerformanceRating
## NULL
## 
## $bin_cuts$RelationshipSatisfaction
## NULL
## 
## $bin_cuts$StockOptionLevel
## NULL
## 
## $bin_cuts$TotalWorkingYears
##   0%  25%  50%  75% 100% 
##    0    6   10   15   40 
## 
## $bin_cuts$TrainingTimesLastYear
##   0%  25%  50% 100% 
##    0    2    3    6 
## 
## $bin_cuts$WorkLifeBalance
## NULL
## 
## $bin_cuts$YearsAtCompany
##   0%  25%  50%  75% 100% 
##    0    3    5   10   40 
## 
## $bin_cuts$YearsInCurrentRole
##   0%  25%  50%  75% 100% 
##    0    2    3    7   18 
## 
## $bin_cuts$YearsSinceLastPromotion
##   0%  50%  75% 100% 
##    0    1    3   15 
## 
## $bin_cuts$YearsWithCurrManager
##   0%  25%  50%  75% 100% 
##    0    2    3    7   17 
## 
## 
## $feature_distribution
## $feature_distribution$Age
## 
##      1      2      3      4 
## 0.2608 0.2824 0.2176 0.2392 
## 
## $feature_distribution$BusinessTravel
## 
##        Non-Travel     Travel_Rarely Travel_Frequently 
##            0.0992            0.7096            0.1912 
## 
## $feature_distribution$DailyRate
## 
##      1      2      3      4 
## 0.2512 0.2496 0.2488 0.2504 
## 
## $feature_distribution$Department
## 
##        Human Resources Research &amp; Development                  Sales 
##                 0.0392                 0.6656                 0.2952 
## 
## $feature_distribution$DistanceFromHome
## 
##      1      2      3      4 
## 0.2856 0.2488 0.2152 0.2504 
## 
## $feature_distribution$Education
## 
## Below College       College      Bachelor        Master        Doctor 
##        0.1192        0.1928        0.3840        0.2712        0.0328 
## 
## $feature_distribution$EducationField
## 
##  Human Resources    Life Sciences        Marketing          Medical 
##           0.0184           0.4160           0.1080           0.3136 
##            Other Technical Degree 
##           0.0528           0.0912 
## 
## $feature_distribution$EmployeeNumber
## 
##      1      2      3      4 
## 0.2504 0.2496 0.2496 0.2504 
## 
## $feature_distribution$EnvironmentSatisfaction
## 
##       Low    Medium      High Very High 
##    0.1960    0.1936    0.3104    0.3000 
## 
## $feature_distribution$Gender
## 
## Female   Male 
## 0.3928 0.6072 
## 
## $feature_distribution$HourlyRate
## 
##      1      2      3      4 
## 0.2512 0.2632 0.2440 0.2416 
## 
## $feature_distribution$JobInvolvement
## 
##       Low    Medium      High Very High 
##    0.0536    0.2488    0.5944    0.1032 
## 
## $feature_distribution$JobLevel
## 
##      1      2      3      4      5 
## 0.3680 0.3608 0.1456 0.0752 0.0504 
## 
## $feature_distribution$JobRole
## 
## Healthcare Representative           Human Resources 
##                    0.0840                    0.0312 
##     Laboratory Technician                   Manager 
##                    0.1792                    0.0712 
##    Manufacturing Director         Research Director 
##                    0.0984                    0.0584 
##        Research Scientist           Sales Executive 
##                    0.2072                    0.2184 
##      Sales Representative 
##                    0.0520 
## 
## $feature_distribution$JobSatisfaction
## 
##       Low    Medium      High Very High 
##    0.2016    0.1848    0.3000    0.3136 
## 
## $feature_distribution$MaritalStatus
## 
##   Single  Married Divorced 
##   0.3192   0.4608   0.2200 
## 
## $feature_distribution$MonthlyIncome
## 
##      1      2      3      4 
## 0.2504 0.2496 0.2496 0.2504 
## 
## $feature_distribution$MonthlyRate
## 
##      1      2      3      4 
## 0.2504 0.2496 0.2496 0.2504 
## 
## $feature_distribution$NumCompaniesWorked
## 
##      1      2      3      4 
## 0.4848 0.1000 0.2048 0.2104 </code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/Snip20190325_8.png" />
<img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/Snip20190325_12.png" /></p>
<p>Creating an explanation with explain</p>
<pre class="r"><code>explanation &lt;- test_tbl %&gt;%
    slice(1:20) %&gt;%
    select(-Attrition) %&gt;%
    lime::explain(
        explainer = explainer,
        n_labels   = 1,
        n_features = 8,
        n_permutations = 5000,
        kernel_width   = 1
    )</code></pre>
<pre class="r"><code>explanation %&gt;%
    as.tibble()</code></pre>
<pre><code>## # A tibble: 160 x 13
##    model_type     case  label label_prob model_r2 model_intercept
##    &lt;chr&gt;          &lt;chr&gt; &lt;chr&gt;      &lt;dbl&gt;    &lt;dbl&gt;           &lt;dbl&gt;
##  1 classification 1     No         0.964    0.212           0.670
##  2 classification 1     No         0.964    0.212           0.670
##  3 classification 1     No         0.964    0.212           0.670
##  4 classification 1     No         0.964    0.212           0.670
##  5 classification 1     No         0.964    0.212           0.670
##  6 classification 1     No         0.964    0.212           0.670
##  7 classification 1     No         0.964    0.212           0.670
##  8 classification 1     No         0.964    0.212           0.670
##  9 classification 2     No         0.856    0.338           0.764
## 10 classification 2     No         0.856    0.338           0.764
##    model_prediction feature                 feature_value feature_weight
##               &lt;dbl&gt; &lt;chr&gt;                           &lt;dbl&gt;          &lt;dbl&gt;
##  1            0.896 OverTime                            1         0.168 
##  2            0.896 Department                          3        -0.0797
##  3            0.896 YearsSinceLastPromotion             7        -0.0703
##  4            0.896 MonthlyIncome                    9419         0.0705
##  5            0.896 StockOptionLevel                    2         0.0638
##  6            0.896 YearsInCurrentRole                  9         0.0627
##  7            0.896 JobSatisfaction                     4         0.0624
##  8            0.896 Age                                30        -0.0515
##  9            0.664 YearsAtCompany                     33        -0.227 
## 10            0.664 OverTime                            2        -0.172 
##    feature_desc                data        prediction
##    &lt;chr&gt;                       &lt;list&gt;      &lt;list&gt;    
##  1 OverTime = No               &lt;list [31]&gt; &lt;list [2]&gt;
##  2 Department = Sales          &lt;list [31]&gt; &lt;list [2]&gt;
##  3 3 &lt; YearsSinceLastPromotion &lt;list [31]&gt; &lt;list [2]&gt;
##  4 8395 &lt; MonthlyIncome        &lt;list [31]&gt; &lt;list [2]&gt;
##  5 StockOptionLevel = 1        &lt;list [31]&gt; &lt;list [2]&gt;
##  6 7 &lt; YearsInCurrentRole      &lt;list [31]&gt; &lt;list [2]&gt;
##  7 JobSatisfaction = Very High &lt;list [31]&gt; &lt;list [2]&gt;
##  8 Age &lt;= 30                   &lt;list [31]&gt; &lt;list [2]&gt;
##  9 10 &lt; YearsAtCompany         &lt;list [31]&gt; &lt;list [2]&gt;
## 10 OverTime = Yes              &lt;list [31]&gt; &lt;list [2]&gt;
## # … with 150 more rows</code></pre>
<div id="recreating-plot-features-with-lime" class="section level4">
<h4>Recreating Plot Features with LIME</h4>
<p>Function <strong>plot_features_tq</strong> create a visualization containing
individual plot for each observations</p>
<ul>
<li>It plots the 8 most influential variables that best explain the
linear model in that observations local region.<br />
</li>
<li>Label variable if it causes an increase in the probability as
<strong>supports</strong> or if it causes decrease in the probability
<strong>contradicts</strong>.<br />
</li>
<li>It provides us with the model fit labelled as Explanation fit for
each model, which allows us to see how well that model explains the
local region.</li>
</ul>
<pre class="r"><code>plot_features_tq &lt;- function(explanation, ncol) {
  
  data_transformed &lt;- explanation %&gt;%
    as.tibble() %&gt;%
    mutate(
      feature_desc = as_factor(feature_desc) %&gt;%
        fct_reorder(abs(feature_weight), .desc = FALSE),
      key = ifelse( feature_weight &gt; 0, &quot;Supports&quot;, &quot;Contradicts&quot;) %&gt;%
        fct_relevel(&quot;Supports&quot;),
      case_text = glue(&quot;Case : {case}&quot;),
      label_text = glue(&quot;Label : {label}&quot;),
      prob_text  = glue(&quot;Probability: {round(label_prob, 2)}&quot;),
      r2_text    = glue(&quot;Explanation Fit : {model_r2 %&gt;% round(2)}&quot;)
    ) %&gt;%
    select(feature_desc, feature_weight, key, case_text:r2_text)
  
  
  data_transformed %&gt;%
    ggplot(aes(feature_desc, feature_weight, fill = key)) +
    geom_bar(stat = &quot;identity&quot;) +
    coord_flip() +
    theme_tq() +
    scale_fill_tq() +
    labs(y = &quot;Weight&quot; , x = &quot;Feature&quot;) +
    facet_wrap(~ case_text + label_text + prob_text + r2_text, 
               ncol = ncol , scales = &quot;free&quot;)
  
} </code></pre>
<p>We can infer that <strong>Case 5</strong> has the highest probability of leaving the
company and variables that appear to be influencing this high
probability include OverTime (Yes), NumCompaniesWorked (&gt;4), Joblevel
(1), Business Travel (Frequently) &amp; DistanceFromHome (13.8 miles).</p>
<pre class="r"><code>explanation %&gt;%
  filter(case %in% 1:6) %&gt;%
  plot_features_tq(ncol = 2)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-87-1.png" /></p>
</div>
<div id="recreating-plot-explanation-with-lime" class="section level4">
<h4>Recreating Plot explanation with LIME</h4>
<pre class="r"><code>plot_explanations_tq &lt;- function(explanation) {
  
  
  data_transformed &lt;- explanation %&gt;%
    as.tibble() %&gt;%
    mutate(
      case = as_factor(case),
      order_1 = rank(feature)
    ) %&gt;%
    group_by(feature) %&gt;%
    mutate(
      order_2 = rank(feature_value)
    ) %&gt;%
    ungroup() %&gt;%
    mutate(
      order = order_1 * 1000 + order_2
    ) %&gt;%
    mutate(
      feature_desc  = as.factor(feature_desc) %&gt;%
        fct_reorder(order, .desc = T)
    ) %&gt;%
    select(case, feature_desc, feature_weight, label)
  
  
  data_transformed %&gt;%
    ggplot(aes(case, feature_desc)) +
    geom_tile(aes(fill = feature_weight)) + 
    facet_wrap(~ label) +
    theme_tq() + 
    scale_fill_gradient2(low = palette_light()[[2]],
                         mid = &quot;white&quot;, 
                         high = palette_light()[[1]]) +
    theme(
      panel.grid = element_blank(),
      legend.position = &quot;right&quot;,
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)
    ) + 
    labs(y = &quot;Feature&quot;, x = &quot;Case&quot;, 
         fill  = glue(&quot;Feature 
                      Weight&quot;))
  
}</code></pre>
<ul>
<li>Heatmap showing how the different features selected across all the
observations influence each case</li>
<li>This plot is useful if we are trying to find common features that
influence all the observations</li>
</ul>
<pre class="r"><code>plot_explanations_tq(explanation)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-89-1.png" /></p>
</div>
</div>
<div id="phase-5-evaluation" class="section level3">
<h3>Phase 5 : Evaluation</h3>
<ul>
<li>In this phase, we try to evaluate degree to which data science
insights meets the business objectives<br />
</li>
<li>Success of data science project depends upon ROI.<br />
</li>
<li>We must always focus to present a business case for making any
changes, then your organization.<br />
decides whether or not to proceed with some or all recommendations
based on ROI.</li>
</ul>
<p><strong>Expected Value Framework</strong></p>
<ul>
<li>A way of assigning value to a decision using both the probability of
its occurence and potential benefit of outcome<br />
</li>
<li>Decisions becomes based on probability rather than intuition</li>
</ul>
<p><strong>Step 2: Tie financial value of individual decisions to optimize for
profit</strong></p>
<ul>
<li><em>MonthlyIncome</em>: Used later as a way to calculate cost of policy
change</li>
<li><em>OverTime</em> : Needed to determine whether or not the employee is
working overtime</li>
<li><em>Attrition cost</em> is only occurred if employee leaves. The ‘Yes’
column is the likelihood of leaving. When combined, we can get an
expected attrition cost .</li>
</ul>
<p><strong>Calculating the Expected ROI Of a Policy Change</strong></p>
<p><strong>Policy I: No OverTime for Everyone</strong></p>
<ul>
<li>“No OT Policy” i.e Threshold = 0, anyone with over time is targeted
, analysis of test set (15% of Total Population)</li>
<li><strong>Baseline</strong>: Do nothing results in no additional costs (e.g no cost
incurred from reduction in OT)</li>
</ul>
<blockquote>
<p>Expected Cost = Yes * (Total Potential Cost) + No * (Policy change
cost)</p>
</blockquote>
<ul>
<li>For Baseline : Expected cost equation simplifies further since there
is no policy change</li>
</ul>
<blockquote>
<p>Expected Cost = Yes * (Attrition Cost)</p>
</blockquote>
<p><strong>Calculating Expected Value With OT</strong></p>
<pre class="r"><code>predictions_with_OT_tbl &lt;- automl_leader %&gt;%
  h2o.predict(newdata = as.h2o(test_tbl)) %&gt;%
  as.tibble() %&gt;%
  bind_cols(
    test_tbl %&gt;%
      select(EmployeeNumber, MonthlyIncome, OverTime)
  )</code></pre>
<pre class="r"><code>ev_with_OT_tbl &lt;- predictions_with_OT_tbl %&gt;%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %&gt;%
  mutate(
    cost_of_policy_change = 0
  ) %&gt;%
  mutate(
    expected_attrition_cost = 
      Yes * (attrition_cost + cost_of_policy_change) +
      No  * (cost_of_policy_change)
  )

total_ev_with_OT_tbl &lt;- ev_with_OT_tbl %&gt;%
  summarise(
    total_expected_attrition_cost_0 = sum(expected_attrition_cost)
  )</code></pre>
<p>We observed that total expected cost with Overtime policy is $3,191,435.</p>
<pre class="r"><code>total_ev_with_OT_tbl</code></pre>
<pre><code>## # A tibble: 1 x 1
##   total_expected_attrition_cost_0
##                             &lt;dbl&gt;
## 1                        3159744.</code></pre>
<p><strong>Calculating Expected Value Without OT</strong></p>
<ul>
<li><strong>Policy Change</strong>: The goal is to update the calculation to reflect
a new policy based on your realistic parameters you and your
organisation are potentially experiencing.<br />
</li>
<li>We are interested in showing savings with no overtime policy, so we
will be switching Overtime NO to Yes on test set and then predicting
their prob again using h2o.predict().<br />
</li>
<li>Adjusting Overtime had a pretty big impact on likelihood of leaving.
Let’s say organization has an average overtime of 10%.</li>
</ul>
<pre class="r"><code>test_without_OT_tbl &lt;- test_tbl %&gt;%
  mutate(OverTime = fct_recode(OverTime, &quot;No&quot; = &quot;Yes&quot;))

predictions_without_OT_tbl &lt;- automl_leader %&gt;%
  h2o.predict(newdata = as.h2o(test_without_OT_tbl)) %&gt;%
  as.tibble() %&gt;%
  bind_cols(
    test_tbl %&gt;%
      select(EmployeeNumber, MonthlyIncome, OverTime),
    test_without_OT_tbl %&gt;%
      select(OverTime)
  ) %&gt;%
  rename(
    OverTime_0 = OverTime,
    OverTime_1 = OverTime1
  )</code></pre>
<pre class="r"><code>avg_overtime_pct &lt;- 0.10


ev_without_OT_tbl&lt;- predictions_without_OT_tbl %&gt;%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %&gt;%
  mutate(
    cost_of_policy_change = case_when(
      OverTime_0 == &quot;Yes&quot; &amp; OverTime_1 == &quot;No&quot; ~ avg_overtime_pct * attrition_cost,
      TRUE ~ 0
    )
  ) %&gt;%
  mutate(
    expected_attrition_cost = 
      Yes * (attrition_cost + cost_of_policy_change) +
      No  * (cost_of_policy_change)
  )

total_ev_without_OT_tbl &lt;- ev_without_OT_tbl %&gt;%
  summarise(
    total_expected_attrition_cost_1 = sum(expected_attrition_cost)
  )</code></pre>
<p>We observed that total expected cost with Overtime policy is $2,795,000.
This implies cost is going down ( $3,191,435 to $2,795,000) after
implementing the policy.</p>
<pre class="r"><code>total_ev_without_OT_tbl</code></pre>
<pre><code>## # A tibble: 1 x 1
##   total_expected_attrition_cost_1
##                             &lt;dbl&gt;
## 1                        2762888.</code></pre>
<p><em>Expected Savings Calculation</em></p>
<ul>
<li>We can potentially save <strong>$396k</strong> for the organization i.e 12.4%
savings.</li>
<li>To annualize the savings, multiply the factor of total observations
to test observations.<br />
(train + test) / test = (1250 + 220) / 220 = 6.7.<br />
</li>
<li>Annualizing Savings: $396k X 6.7 = $2.6M.</li>
</ul>
<pre class="r"><code>bind_cols(
  total_ev_with_OT_tbl,
  total_ev_without_OT_tbl
) %&gt;%
  mutate(
    savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
    pct_savings = savings / total_expected_attrition_cost_0
  )</code></pre>
<pre><code>## # A tibble: 1 x 4
##   total_expected_attrition_cost_0 total_expected_attrition_cost_1 savings
##                             &lt;dbl&gt;                           &lt;dbl&gt;   &lt;dbl&gt;
## 1                        3159744.                        2762888. 396856.
##   pct_savings
##         &lt;dbl&gt;
## 1       0.126</code></pre>
<p>So, we can potentially save the organization $2.6M per year if we
implement No overtime policy for everyone.<br />
<strong>Targeting by Threshold Primer</strong></p>
<ul>
<li>In this case, we <strong>target by Threshold</strong>, if threshold = 30%, only
employees with prob_leave &gt;= 0.30 are targeted.<br />
</li>
<li>Most Critical : Don’t want to miss FNs. Typically cost is higher for
FNs.</li>
</ul>
<p><strong>Comparing FNs vs FPs</strong></p>
<ul>
<li>FN’s are more costly than FPs<br />
</li>
<li><strong>False Negatives</strong> - We predict employee stays but actually leaves,
we fail to target and do not reduce OT for 100% of attrition cost.<br />
</li>
<li><strong>False Positives</strong> - We predict leaves but stays, we target and
reduce OT for 10-30% of attrition cost.<br />
</li>
<li>For example: Lets say FN = $100 k , FP = $30K ,FN/FP = 3X more
costly. In this case, when FN’s = 3X FP’s, lower threshold (lower
the threshold from 28% to 13% ,catch the people that have 13% or
more probability of quitting.<br />
</li>
<li>Expected Savings Increases 13.9% ((549 - 482) / 482 * 100), $67k
Savings / 0.15 = $446k per year additional savings for full
population.</li>
</ul>
<pre class="r"><code>performance_h2o &lt;- automl_leader %&gt;%
  h2o.performance(newdata = as.h2o(test_tbl))</code></pre>
<pre class="r"><code>performance_h2o %&gt;%
  h2o.confusionMatrix()</code></pre>
<pre><code>## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.323813276742962:
##         No Yes    Error     Rate
## No     173  11 0.059783  =11/184
## Yes     11  25 0.305556   =11/36
## Totals 184  36 0.100000  =22/220</code></pre>
<p>Most Critical : Don’t want to miss False Negatives. Typically cost is
higher for FNs.</p>
<pre class="r"><code>rates_by_threshold_tbl &lt;- performance_h2o %&gt;%
  h2o.metric() %&gt;%
  as.tibble()

rates_by_threshold_tbl %&gt;%
  glimpse()</code></pre>
<pre><code>## Observations: 220
## Variables: 20
## $ threshold               &lt;dbl&gt; 0.9567169, 0.9191046, 0.9025171, 0.89106…
## $ f1                      &lt;dbl&gt; 0.05405405, 0.10526316, 0.15384615, 0.20…
## $ f2                      &lt;dbl&gt; 0.03448276, 0.06849315, 0.10204082, 0.13…
## $ f0point5                &lt;dbl&gt; 0.1250000, 0.2272727, 0.3125000, 0.38461…
## $ accuracy                &lt;dbl&gt; 0.8409091, 0.8454545, 0.8500000, 0.85454…
## $ precision               &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.00000…
## $ recall                  &lt;dbl&gt; 0.02777778, 0.05555556, 0.08333333, 0.11…
## $ specificity             &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.00000…
## $ absolute_mcc            &lt;dbl&gt; 0.1527691, 0.2165431, 0.2658205, 0.30765…
## $ min_per_class_accuracy  &lt;dbl&gt; 0.02777778, 0.05555556, 0.08333333, 0.11…
## $ mean_per_class_accuracy &lt;dbl&gt; 0.5138889, 0.5277778, 0.5416667, 0.55555…
## $ tns                     &lt;dbl&gt; 184, 184, 184, 184, 184, 184, 183, 183, …
## $ fns                     &lt;dbl&gt; 35, 34, 33, 32, 31, 30, 30, 29, 28, 27, …
## $ fps                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2…
## $ tps                     &lt;dbl&gt; 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11, 12…
## $ tnr                     &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.00000…
## $ fnr                     &lt;dbl&gt; 0.9722222, 0.9444444, 0.9166667, 0.88888…
## $ fpr                     &lt;dbl&gt; 0.000000000, 0.000000000, 0.000000000, 0…
## $ tpr                     &lt;dbl&gt; 0.02777778, 0.05555556, 0.08333333, 0.11…
## $ idx                     &lt;int&gt; 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…</code></pre>
<p>F1 is the optimal balance between Precision &amp; Recall.However, the
threshold <span class="citation">@Max</span> F1 is not typically the optimal value for the business
case because FNs are typically more costly than FPs.</p>
<p><strong>tnr &amp; fpr properties</strong></p>
<ol style="list-style-type: decimal">
<li>Probability of correctly classifying a negative<br />
</li>
<li>When actual is negative, we just sometimes classify it as positive<br />
</li>
<li>tnr + fpr = 1</li>
</ol>
<p><strong>tpr &amp; fnr properties</strong></p>
<ol style="list-style-type: decimal">
<li>Probability of correctly classifying a positive<br />
</li>
<li>When actual is positive, we just sometimes classify it as negative<br />
</li>
<li>tpr + fnr = 1</li>
</ol>
<pre class="r"><code>rates_by_threshold_tbl %&gt;%
  select(threshold, f1, tnr:tpr) %&gt;%
  filter( f1 == max(f1)) %&gt;%
  slice(1)  </code></pre>
<pre><code>## # A tibble: 1 x 6
##   threshold    f1   tnr   fnr    fpr   tpr
##       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1     0.324 0.694 0.940 0.306 0.0598 0.694</code></pre>
<p><strong>Visualizing Expected Rates</strong></p>
<ul>
<li>Fundamentally, expected rates are the probability of getting the
model prediction correct or incorrect at a given threshold.<br />
</li>
<li>Idea is to understand how expected rates interact as threshold
increases.<br />
</li>
<li><strong>Cost Tradeoff</strong> : In a perfect world, our model would never make
mistakes &amp; we could target TPs perfectly. We do not live in a
perfect world, FPs and FNs results in costs.</li>
</ul>
<pre class="r"><code>rates_by_threshold_tbl %&gt;%
  select(threshold, f1, tnr:tpr) %&gt;%
  gather(key = key, value = value, tnr:tpr, factor_key = TRUE) %&gt;%
  mutate(key = fct_reorder2(key, threshold, value)) %&gt;%
  ggplot(aes(threshold, value, color = key)) +
  geom_point() + 
  geom_smooth() +
  theme_tq() +
  scale_color_tq() +
  theme(legend.position = &quot;right&quot;) +
  labs(
    title = &quot;Expected Rates&quot;,
    y = &quot;Value&quot;,
    x = &quot;Threshold&quot;
  )</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-98-1.png" /></p>
<ul>
<li>We can notice from the plot that at any point on given threshold tnr
and fpr are related, these rates together add up to 1<br />
</li>
<li>We can notice from the plot that at any point on given threshold tpr
and fnr are related , these rates together add up to 1</li>
</ul>
<pre class="r"><code>predictions_with_OT_tbl &lt;- automl_leader %&gt;%
  h2o.predict(newdata = as.h2o(test_tbl)) %&gt;%
  as.tibble() %&gt;%
  bind_cols(
    test_tbl %&gt;%
      select(EmployeeNumber, MonthlyIncome, OverTime)
  )</code></pre>
<pre class="r"><code>predictions_with_OT_tbl </code></pre>
<pre><code>## # A tibble: 220 x 6
##    predict    No    Yes EmployeeNumber MonthlyIncome OverTime
##    &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;   
##  1 No      0.964 0.0359            228          9419 No      
##  2 No      0.856 0.144            1278         13577 Yes     
##  3 No      0.970 0.0298           1250         17779 No      
##  4 No      0.954 0.0456           2065          5390 No      
##  5 Yes     0.200 0.800            1767          2437 Yes     
##  6 No      0.927 0.0728           1308          2372 No      
##  7 No      0.951 0.0490             18          2661 No      
##  8 No      0.967 0.0331            460          6347 No      
##  9 No      0.970 0.0303           1369          5363 No      
## 10 No      0.966 0.0344           1040          6347 No      
## # … with 210 more rows</code></pre>
<pre class="r"><code>ev_with_OT_tbl &lt;- predictions_with_OT_tbl %&gt;%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %&gt;%
  mutate(
    cost_of_policy_change = 0
  ) %&gt;%
  mutate(
    expected_attrition_cost = 
      Yes * (attrition_cost + cost_of_policy_change) +
      No  * (cost_of_policy_change)
  )

ev_with_OT_tbl</code></pre>
<pre><code>## # A tibble: 220 x 9
##    predict    No    Yes EmployeeNumber MonthlyIncome OverTime
##    &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;   
##  1 No      0.964 0.0359            228          9419 No      
##  2 No      0.856 0.144            1278         13577 Yes     
##  3 No      0.970 0.0298           1250         17779 No      
##  4 No      0.954 0.0456           2065          5390 No      
##  5 Yes     0.200 0.800            1767          2437 Yes     
##  6 No      0.927 0.0728           1308          2372 No      
##  7 No      0.951 0.0490             18          2661 No      
##  8 No      0.967 0.0331            460          6347 No      
##  9 No      0.970 0.0303           1369          5363 No      
## 10 No      0.966 0.0344           1040          6347 No      
##    attrition_cost cost_of_policy_change expected_attrition_cost
##             &lt;dbl&gt;                 &lt;dbl&gt;                   &lt;dbl&gt;
##  1         72979.                     0                   2618.
##  2         64663.                     0                   9298.
##  3         56259.                     0                   1677.
##  4         81037.                     0                   3698.
##  5         86943.                     0                  69570.
##  6         87073.                     0                   6341.
##  7         86495.                     0                   4242.
##  8         79123.                     0                   2616.
##  9         81091.                     0                   2458.
## 10         79123.                     0                   2723.
## # … with 210 more rows</code></pre>
<pre class="r"><code>total_ev_with_OT_tbl &lt;- ev_with_OT_tbl %&gt;%
  summarise(
    total_expected_attrition_cost_0 = sum(expected_attrition_cost)
  )

total_ev_with_OT_tbl</code></pre>
<pre><code>## # A tibble: 1 x 1
##   total_expected_attrition_cost_0
##                             &lt;dbl&gt;
## 1                        3159744.</code></pre>
<p><strong>Calculating Expected Value With Targeted OT</strong></p>
<pre class="r"><code>max_f1_tbl &lt;- rates_by_threshold_tbl %&gt;%
  select(threshold, f1, tnr:tpr) %&gt;%
  filter(f1 == max(f1)) %&gt;%
  slice(1)

max_f1_tbl</code></pre>
<pre><code>## # A tibble: 1 x 6
##   threshold    f1   tnr   fnr    fpr   tpr
##       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1     0.324 0.694 0.940 0.306 0.0598 0.694</code></pre>
<pre class="r"><code>tnr &lt;- max_f1_tbl$tnr
fnr &lt;- max_f1_tbl$fnr
fpr &lt;- max_f1_tbl$fpr
tpr &lt;- max_f1_tbl$tpr

threshold &lt;- max_f1_tbl$threshold
threshold </code></pre>
<pre><code>## [1] 0.3238133</code></pre>
<p>Anyone with Yes &gt; = 0.3125333 and OverTime = Yes had their OT toggled
to No.</p>
<pre class="r"><code>test_targeted_OT_tbl &lt;- test_tbl %&gt;%
  add_column(Yes = predictions_with_OT_tbl$Yes) %&gt;%
  mutate(
    OverTime = case_when(
      Yes &gt;= threshold ~ factor(&quot;No&quot;, levels = levels(test_tbl$OverTime)) ,
      TRUE ~ OverTime
     )
  ) %&gt;%
  select(-Yes)

test_targeted_OT_tbl</code></pre>
<pre><code>## # A tibble: 220 x 32
##      Age Attrition BusinessTravel    DailyRate Department            
##    &lt;dbl&gt; &lt;fct&gt;     &lt;fct&gt;                 &lt;dbl&gt; &lt;fct&gt;                 
##  1    30 No        Travel_Rarely          1339 Sales                 
##  2    55 No        Non-Travel              177 Research &amp; Development
##  3    54 No        Travel_Rarely           685 Research &amp; Development
##  4    49 No        Travel_Frequently      1023 Sales                 
##  5    43 Yes       Travel_Frequently       807 Research &amp; Development
##  6    58 No        Travel_Rarely           848 Research &amp; Development
##  7    34 No        Travel_Rarely          1346 Research &amp; Development
##  8    37 No        Travel_Rarely          1192 Research &amp; Development
##  9    35 No        Travel_Rarely           817 Research &amp; Development
## 10    50 No        Non-Travel              145 Sales                 
##    DistanceFromHome Education     EducationField   EmployeeNumber
##               &lt;dbl&gt; &lt;fct&gt;         &lt;fct&gt;                     &lt;dbl&gt;
##  1                5 Bachelor      Life Sciences               228
##  2                8 Below College Medical                    1278
##  3                3 Bachelor      Life Sciences              1250
##  4                2 Bachelor      Medical                    2065
##  5               17 Bachelor      Technical Degree           1767
##  6               23 Master        Life Sciences              1308
##  7               19 College       Medical                      18
##  8                5 College       Medical                     460
##  9                1 Bachelor      Medical                    1369
## 10                1 Bachelor      Life Sciences              1040
##    EnvironmentSatisfaction Gender HourlyRate JobInvolvement JobLevel
##    &lt;fct&gt;                   &lt;fct&gt;       &lt;dbl&gt; &lt;fct&gt;          &lt;fct&gt;   
##  1 Medium                  Female         41 High           3       
##  2 Very High               Male           37 Medium         4       
##  3 Very High               Male           85 High           4       
##  4 Very High               Male           63 Medium         2       
##  5 High                    Male           38 Medium         1       
##  6 Low                     Male           88 High           1       
##  7 Medium                  Male           93 High           1       
##  8 Very High               Male           61 High           2       
##  9 Very High               Female         60 Medium         2       
## 10 Very High               Female         95 High           2       
##    JobRole                   JobSatisfaction MaritalStatus MonthlyIncome
##    &lt;fct&gt;                     &lt;fct&gt;           &lt;fct&gt;                 &lt;dbl&gt;
##  1 Sales Executive           Very High       Married                9419
##  2 Healthcare Representative Medium          Divorced              13577
##  3 Research Director         Very High       Married               17779
##  4 Sales Executive           Medium          Married                5390
##  5 Research Scientist        High            Married                2437
##  6 Research Scientist        High            Divorced               2372
##  7 Laboratory Technician     Very High       Divorced               2661
##  8 Manufacturing Director    Very High       Divorced               6347
##  9 Laboratory Technician     Very High       Married                5363
## 10 Sales Executive           High            Married                6347
##    MonthlyRate NumCompaniesWorked OverTime PercentSalaryHike
##          &lt;dbl&gt;              &lt;dbl&gt; &lt;fct&gt;                &lt;dbl&gt;
##  1        8053                  2 No                      12
##  2       25592                  1 Yes                     15
##  3       23474                  3 No                      14
##  4       13243                  2 No                      14
##  5       15587                  9 No                      16
##  6       26076                  1 No                      12
##  7        8758                  0 No                      11
##  8       23177                  7 No                      16
##  9       10846                  0 No                      12
## 10       24920                  0 No                      12
##    PerformanceRating RelationshipSatisfaction StockOptionLevel
##    &lt;fct&gt;             &lt;fct&gt;                    &lt;fct&gt;           
##  1 Excellent         High                     1               
##  2 Excellent         Very High                1               
##  3 Excellent         Low                      0               
##  4 Excellent         Very High                0               
##  5 Excellent         Very High                1               
##  6 Excellent         Very High                2               
##  7 Excellent         High                     1               
##  8 Excellent         High                     2               
##  9 Excellent         Medium                   1               
## 10 Excellent         Low                      1               
##    TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany
##                &lt;dbl&gt;                 &lt;dbl&gt; &lt;fct&gt;                    &lt;dbl&gt;
##  1                12                     2 Better                      10
##  2                34                     3 Better                      33
##  3                36                     2 Better                      10
##  4                17                     3 Good                         9
##  5                 6                     4 Better                       1
##  6                 2                     3 Better                       2
##  7                 3                     2 Better                       2
##  8                 8                     2 Good                         6
##  9                10                     0 Better                       9
## 10                19                     3 Better                      18
##    YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager
##                 &lt;dbl&gt;                   &lt;dbl&gt;                &lt;dbl&gt;
##  1                  9                       7                    4
##  2                  9                      15                    0
##  3                  9                       0                    9
##  4                  6                       0                    8
##  5                  0                       0                    0
##  6                  2                       2                    2
##  7                  2                       1                    2
##  8                  2                       0                    4
##  9                  7                       0                    0
## 10                  7                       0                   13
## # … with 210 more rows</code></pre>
<p>Running h2o.predict() on the modified data will give us the
probabilities of attrition = Yes i.e implementing the policy change.</p>
<pre class="r"><code>predictions_targeted_OT_tbl &lt;- automl_leader %&gt;%
  h2o.predict(newdata = as.h2o(test_targeted_OT_tbl)) %&gt;%
  as.tibble() %&gt;%
  bind_cols(
    test_tbl %&gt;%
      select(EmployeeNumber, MonthlyIncome, OverTime),
    test_targeted_OT_tbl %&gt;%
      select(OverTime)
  ) %&gt;%
  rename(
    OverTime_0 = OverTime,
    OverTime_1 = OverTime1
  )</code></pre>
<pre class="r"><code>predictions_targeted_OT_tbl</code></pre>
<pre><code>## # A tibble: 220 x 7
##    predict    No    Yes EmployeeNumber MonthlyIncome OverTime_0 OverTime_1
##    &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;     
##  1 No      0.964 0.0359            228          9419 No         No        
##  2 No      0.856 0.144            1278         13577 Yes        Yes       
##  3 No      0.970 0.0298           1250         17779 No         No        
##  4 No      0.954 0.0456           2065          5390 No         No        
##  5 No      0.795 0.205            1767          2437 Yes        No        
##  6 No      0.927 0.0728           1308          2372 No         No        
##  7 No      0.951 0.0490             18          2661 No         No        
##  8 No      0.967 0.0331            460          6347 No         No        
##  9 No      0.970 0.0303           1369          5363 No         No        
## 10 No      0.966 0.0344           1040          6347 No         No        
## # … with 210 more rows</code></pre>
<pre class="r"><code>avg_overtime_pct &lt;- 0.10


ev_targeted_OT_tbl &lt;- predictions_targeted_OT_tbl %&gt;%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %&gt;%
  mutate(
    cost_of_policy_change = case_when(
      OverTime_0 == &quot;Yes&quot; &amp; OverTime_1 == &quot;No&quot; ~ attrition_cost * avg_overtime_pct,
      TRUE ~ 0
    )
  ) %&gt;%
  mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
  )


ev_targeted_OT_tbl</code></pre>
<pre><code>## # A tibble: 220 x 14
##    predict    No    Yes EmployeeNumber MonthlyIncome OverTime_0 OverTime_1
##    &lt;fct&gt;   &lt;dbl&gt;  &lt;dbl&gt;          &lt;dbl&gt;         &lt;dbl&gt; &lt;fct&gt;      &lt;fct&gt;     
##  1 No      0.964 0.0359            228          9419 No         No        
##  2 No      0.856 0.144            1278         13577 Yes        Yes       
##  3 No      0.970 0.0298           1250         17779 No         No        
##  4 No      0.954 0.0456           2065          5390 No         No        
##  5 No      0.795 0.205            1767          2437 Yes        No        
##  6 No      0.927 0.0728           1308          2372 No         No        
##  7 No      0.951 0.0490             18          2661 No         No        
##  8 No      0.967 0.0331            460          6347 No         No        
##  9 No      0.970 0.0303           1369          5363 No         No        
## 10 No      0.966 0.0344           1040          6347 No         No        
##    attrition_cost cost_of_policy_change cb_tn cb_fp  cb_tp  cb_fn
##             &lt;dbl&gt;                 &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;
##  1         72979.                    0     0     0  72979. 72979.
##  2         64663.                    0     0     0  64663. 64663.
##  3         56259.                    0     0     0  56259. 56259.
##  4         81037.                    0     0     0  81037. 81037.
##  5         86943.                 8694. 8694. 8694. 95637. 95637.
##  6         87073.                    0     0     0  87073. 87073.
##  7         86495.                    0     0     0  86495. 86495.
##  8         79123.                    0     0     0  79123. 79123.
##  9         81091.                    0     0     0  81091. 81091.
## 10         79123.                    0     0     0  79123. 79123.
##    expected_attrition_cost
##                      &lt;dbl&gt;
##  1                   2618.
##  2                   9298.
##  3                   1677.
##  4                   3698.
##  5                  26547.
##  6                   6341.
##  7                   4242.
##  8                   2616.
##  9                   2458.
## 10                   2723.
## # … with 210 more rows</code></pre>
<pre class="r"><code>total_ev_targeted_OT_tbl &lt;- ev_targeted_OT_tbl %&gt;%
  summarise(total_expected_attrition_cost_1 = sum(expected_attrition_cost))

total_ev_targeted_OT_tbl</code></pre>
<pre><code>## # A tibble: 1 x 1
##   total_expected_attrition_cost_1
##                             &lt;dbl&gt;
## 1                        2746695.</code></pre>
<p><strong>Savings Calculation</strong></p>
<p>Percentage savings goes up to 14.2% from 12.4% , So targted OT makes
more sense</p>
<pre class="r"><code>savings_tbl &lt;- bind_cols(
  total_ev_with_OT_tbl,
  total_ev_targeted_OT_tbl
) %&gt;%
  mutate(
    savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
    pct_savings = savings / total_expected_attrition_cost_0
  )

savings_tbl</code></pre>
<pre><code>## # A tibble: 1 x 4
##   total_expected_attrition_cost_0 total_expected_attrition_cost_1 savings
##                             &lt;dbl&gt;                           &lt;dbl&gt;   &lt;dbl&gt;
## 1                        3159744.                        2746695. 413049.
##   pct_savings
##         &lt;dbl&gt;
## 1       0.131</code></pre>
<p><strong>Threshold Optimization</strong></p>
<ul>
<li>An Iterative approach to expected value.<br />
</li>
<li>Critical for maximizing profitability of a policy change.</li>
</ul>
<p><strong>calculate_savings_by_threshold()</strong></p>
<ul>
<li>A function that calculates &amp; returns the savings when the user
provides data, h2o model, threshold and expected rates (tnr, fpr,
fnr, tpr)<br />
</li>
<li>Default: The function is set up for a <em>“No OT Policy”</em> because the
threshold is so low anyone with OT gets converted to No OT</li>
</ul>
<p><strong>Savings By Threshold</strong></p>
<p>Function that can be used on a single threshold to calculate savings</p>
<pre class="r"><code>calculate_savings_by_threshold &lt;- function(data, h2o_model, threshold = 0, 
                                           tnr = 0, fpr = 1, fnr = 0, tpr = 1) {
  
  data_0_tbl &lt;- as.tibble(data)
  
  # Expected Value
  
  # Calculating Expected Value with OT
  
  pred_0_tbl &lt;- h2o_model %&gt;%
    h2o.predict(newdata = as.h2o(data_0_tbl)) %&gt;%
    as.tibble() %&gt;%
    bind_cols(
      data_0_tbl %&gt;%
        select(EmployeeNumber, MonthlyIncome, OverTime)
    )
  
  ev_0_tbl &lt;- pred_0_tbl %&gt;%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = 250000
      )
    ) %&gt;%
    mutate(
      cost_of_policy_change = 0
    ) %&gt;%
    mutate(
      expected_attrition_cost = 
        Yes * (attrition_cost + cost_of_policy_change) +
        No  * (cost_of_policy_change)
    )
  
  total_ev_0_tbl &lt;- ev_0_tbl %&gt;%
    summarise(
      total_expected_attrition_cost_0 = sum(expected_attrition_cost)
    )
  
  # Calculating Expected Value with Targeted OT
  data_1_tbl &lt;- data_0_tbl %&gt;%
    add_column(Yes = pred_0_tbl$Yes) %&gt;%
    mutate(
      OverTime = case_when(
        Yes &gt;= threshold ~ factor(&quot;No&quot;, levels = levels(data_0_tbl$OverTime)) ,
        TRUE ~ OverTime
      )
    ) %&gt;%
    select(-Yes)
  
  pred_1_tbl &lt;- h2o_model %&gt;%
    h2o.predict(newdata = as.h2o(data_1_tbl)) %&gt;%
    as.tibble() %&gt;%
    bind_cols(
      data_0_tbl %&gt;%
        select(EmployeeNumber, MonthlyIncome, OverTime),
      data_1_tbl %&gt;%
        select(OverTime)
    ) %&gt;%
    rename(
      OverTime_0 = OverTime,
      OverTime_1 = OverTime1
    )
  
  avg_overtime_pct &lt;- 0.10
  
  ev_1_tbl &lt;- pred_1_tbl %&gt;%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = 250000
      )
    ) %&gt;%
    mutate(
      cost_of_policy_change = case_when(
        OverTime_0 == &quot;Yes&quot; &amp; OverTime_1 == &quot;No&quot; ~ attrition_cost * avg_overtime_pct,
        TRUE ~ 0
      )
    ) %&gt;%
    mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
    )
  
  total_ev_1_tbl &lt;- ev_1_tbl %&gt;%
    summarise(
      total_expected_attrition_cost_1 = sum(expected_attrition_cost)
    )
  
  # Savings Calculation
  savings_tbl &lt;- bind_cols(
    total_ev_0_tbl,
    total_ev_1_tbl
  ) %&gt;%
    mutate(
      savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
      pct_savings = savings / total_expected_attrition_cost_0
    )
  
  return(savings_tbl$savings)
  
}</code></pre>
<p><strong>Max F1 Score : Threshold @ Max F1 i.e Target employees by balancing
FN’s and FP’s</strong></p>
<p>Adjusting threshold to max F1 i.e targeting high risk employees which
gives us savings which is bit more than what we had with No OT policy</p>
<pre class="r"><code># Threshold @ Max F1

rates_by_threshold_tbl %&gt;%
  select(threshold, f1, tnr:tpr) %&gt;%
  filter(f1 == max(f1))</code></pre>
<pre><code>## # A tibble: 1 x 6
##   threshold    f1   tnr   fnr    fpr   tpr
##       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;
## 1     0.324 0.694 0.940 0.306 0.0598 0.694</code></pre>
<p>Threshold @ Max F1 = 0.313</p>
<pre class="r"><code>max_f1_savings &lt;- calculate_savings_by_threshold(test_tbl, automl_leader, 
                                                 threshold = max_f1_tbl$threshold,
                                                 tnr = max_f1_tbl$tnr,
                                                 fnr = max_f1_tbl$fnr,
                                                 fpr = max_f1_tbl$fpr,
                                                 tpr = max_f1_tbl$tpr)</code></pre>
<pre class="r"><code>max_f1_savings</code></pre>
<pre><code>## [1] 413049.3</code></pre>
<p>Savings in this case is $453K with threshold @ Max F1, more than No OT
policy.</p>
<p><strong>No OT Policy, Threshold = 0</strong></p>
<ul>
<li>Anyone With OT Targeted, analysis of test set (15% of Total
Population).</li>
<li>$396k / 0.15 = $2.6M per year savings.</li>
</ul>
<pre class="r"><code># No OT Policy

test_tbl %&gt;%
  calculate_savings_by_threshold(automl_leader, threshold = 0,
                                 tnr = 0, fnr = 0, tpr = 1, fpr = 1)</code></pre>
<pre><code>## [1] 396856.1</code></pre>
<p><strong>Do Nothing Policy i.e threshold = 1</strong></p>
<pre class="r"><code> # Do Nothing Policy

test_tbl %&gt;%
  calculate_savings_by_threshold(automl_leader, threshold = 1,
                                 tnr = 1, fnr = 1, tpr = 0, fpr = 0)</code></pre>
<pre><code>## [1] 0</code></pre>
<ul>
<li>No Savings in this case because threshold is so high that no one is
targeted (No one has 100% probability of leaving)</li>
</ul>
<p><strong>Optimize by Threshold</strong> : <strong>Maximizing Expected ROI</strong></p>
<p><strong>Threshold @ Max Savings</strong> : Target employees by weighted analysis of
cost of FN and cost of FP</p>
<ul>
<li>Generally, threshold @ F1 Max is default threshold for most
classification algorithms</li>
<li>But it is not optimized for business case. F1 treats False positives
&amp; False negatives equally. They are not FN’s are in many cases 3X+
more costly to organization<br />
</li>
<li><strong>False Negatives</strong> - We predict employee stays but actually leaves,
we fail to target and do not reduce OT for 100% of attrition cost<br />
</li>
<li><strong>False Positives</strong> - We predict leaves but stays, we target and
reduce OT for 10-30% of attrition cost<br />
</li>
<li>FN’s are more costly than FPs Comparing FNs vs FPs</li>
<li>For example: Lets say FN = $100 k , FP = $30K ,FN/FP = 3X more
costly. In this case, when FN’s = 3X FP’s, lower threshold (lower
the threshold from 28% to 13% ,catch the people that have 13% or
more probability of quitting</li>
</ul>
<p><strong>Sampling Indices in test data</strong></p>
<blockquote>
<p>When we perform an iterative process, this often takes a lot of time.
We can sample the indices to reduce the number of iterations from 220
(no .of obs in test data) to 20 (sampled data) which reduces our
iteration time by a factor of 11x.</p>
</blockquote>
<p><strong>Iteratively Applying the function rates_by_threshold_optimized_tbl
to optimize</strong></p>
<pre class="r"><code>smpl &lt;-  seq(1, 220, length.out = 20) %&gt;% round(digits = 0)

rates_by_threshold_optimized_tbl &lt;- rates_by_threshold_tbl %&gt;%
  select(threshold, tnr:tpr) %&gt;%
  slice(smpl) %&gt;%
  mutate(
    savings = pmap_dbl(.l = list(
      threshold = threshold,
      tnr = tnr,
      fnr = fnr,
      fpr = fpr,
      tpr = tpr
    ),
    .f = partial(calculate_savings_by_threshold, data = test_tbl, h2o_model = automl_leader)
    )
  )</code></pre>
<pre class="r"><code>rates_by_threshold_optimized_tbl</code></pre>
<pre><code>## # A tibble: 20 x 6
##    threshold    tnr    fnr     fpr    tpr savings
##        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
##  1    0.957  1      0.972  0       0.0278   5420.
##  2    0.740  0.995  0.667  0.00543 0.333  256614.
##  3    0.456  0.967  0.5    0.0326  0.5    331490.
##  4    0.324  0.940  0.306  0.0598  0.694  413049.
##  5    0.261  0.880  0.306  0.120   0.694  482191.
##  6    0.180  0.821  0.278  0.179   0.722  518469.
##  7    0.140  0.772  0.222  0.228   0.778  516766.
##  8    0.113  0.712  0.194  0.288   0.806  511841.
##  9    0.0956 0.658  0.167  0.342   0.833  499782.
## 10    0.0763 0.598  0.139  0.402   0.861  490380.
## 11    0.0659 0.549  0.0833 0.451   0.917  474196.
## 12    0.0574 0.489  0.0556 0.511   0.944  461748.
## 13    0.0524 0.435  0.0278 0.565   0.972  441870.
## 14    0.0481 0.375  0      0.625   1      429102.
## 15    0.0444 0.315  0      0.685   1      415447.
## 16    0.0408 0.25   0      0.75    1      408216.
## 17    0.0382 0.190  0      0.810   1      408216.
## 18    0.0344 0.125  0      0.875   1      402298.
## 19    0.0315 0.0652 0      0.935   1      402298.
## 20    0.0296 0      0      1       1      396856.</code></pre>
<p><strong>Visualizing Optimized Savings</strong></p>
<pre class="r"><code>rates_by_threshold_optimized_tbl %&gt;%
  ggplot(aes(threshold, savings)) +
  #Vlines
  geom_vline(xintercept = max_f1_tbl$threshold, 
             color = palette_light()[[5]] , size = 2) +
  geom_vline(aes(xintercept = threshold), 
             color = palette_light()[[3]] , size = 2,
            data =  rates_by_threshold_optimized_tbl %&gt;%
              filter(savings ==  max(savings))
  ) +
  # Points
  geom_line(color = palette_light()[[1]]) +
  geom_point(color = palette_light()[[1]]) +
  
  # Optimal Point 
  geom_point(shape = 21, size = 5, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl %&gt;%
               filter(savings == max(savings))) +
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -1, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl %&gt;%
               filter(savings == max(savings))) +
  # F1 Max
  geom_vline(xintercept = max_f1_tbl$threshold,
             color = palette_light()[[5]], size = 2) +
  annotate(geom = &quot;label&quot;, label = scales::dollar(max_f1_savings),
           x = max_f1_tbl$threshold, y = max_f1_savings, vjust = -1,
           color = palette_light()[[1]]) +
  
  # No OT Policy
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %&gt;%
               filter(threshold == min(threshold))) +
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %&gt;%
               filter(threshold == min(threshold))) +
  
  # Do Nothing
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %&gt;%
               filter(threshold == max(threshold))) +
  geom_label(aes(label = scales::dollar(round(savings,0))), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %&gt;%
               filter(threshold == max(threshold))) +
  
  # Aesthestics 
  theme_tq() +
  expand_limits(x = c(-.1, 1.1), y = 8e5) + 
  scale_x_continuous(labels  = scales::percent,
                     breaks = seq(0, 1, by = 0.2)) +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = &quot;Optimization Results : Expected Savings Maximized At 13.3% &quot;,
       x = &quot;Threshold (%)&quot;, 
       y = &quot;Savings&quot;)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-111-1.png" /></p>
<ul>
<li>Lower the threshold from 31% to 14.2%: Catch the people that have
14.2% or more probability of quitting.</li>
<li>Expected Savings Increases 17.4% ((532 - 453) / 453 * 100).</li>
<li>$79k Savings / 0.15 = $526k per year additional savings for full
population.</li>
<li>Targeting employees with Yes &gt; = 14.2% will maximize expected
savings i.e <strong>$532,600</strong>.</li>
<li>Big Savings : Targeting employees with Yes &gt; = 14.2% , Increased
savings by 17.4% versus F1.</li>
</ul>
<p><strong>Sensitivity Analysis</strong></p>
<p><strong>Modelling Assumptions</strong>:</p>
<ol style="list-style-type: decimal">
<li>You will never have perfect information and therefore assumptions
must be made.<br />
</li>
<li>Always test you assumptions: Even though you need to test some
assumptions, you can still try different values to test your
assumptions. This is why we do <strong>Sensitivity Analysis</strong>.</li>
</ol>
<pre class="r"><code>calculate_savings_by_threshold_2 &lt;- function(data, h2o_model, threshold = 0, 
                                             tnr = 0, fpr = 1, fnr = 0, tpr = 1,
                                             avg_overtime_pct = 0.10,
                                             net_revenue_per_employee = 250000) {
  
  data_0_tbl &lt;- as.tibble(data)
  
 
  # Expected Value
  
  # Calculating Expected Value with OT
  
  pred_0_tbl &lt;- h2o_model %&gt;%
    h2o.predict(newdata = as.h2o(data_0_tbl)) %&gt;%
    as.tibble() %&gt;%
    bind_cols(
      data_0_tbl %&gt;%
        select(EmployeeNumber, MonthlyIncome, OverTime)
    )
  
  ev_0_tbl &lt;- pred_0_tbl %&gt;%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        # changed in _2 -----
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %&gt;%
    mutate(
      cost_of_policy_change = 0
    ) %&gt;%
    mutate(
      expected_attrition_cost = 
        Yes * (attrition_cost + cost_of_policy_change) +
        No  * (cost_of_policy_change)
    )
  
  total_ev_0_tbl &lt;- ev_0_tbl %&gt;%
    summarise(
      total_expected_attrition_cost_0 = sum(expected_attrition_cost)
    )
  
  # Calculating Expected Value with Targeted OT
  
  data_1_tbl &lt;- data_0_tbl %&gt;%
    add_column(Yes = pred_0_tbl$Yes) %&gt;%
    mutate(
      OverTime = case_when(
        Yes &gt;= threshold ~ factor(&quot;No&quot;, levels = levels(data_0_tbl$OverTime)) ,
        TRUE ~ OverTime
      )
    ) %&gt;%
    select(-Yes) 
  
  pred_1_tbl &lt;- h2o_model %&gt;%
    h2o.predict(newdata = as.h2o(data_1_tbl)) %&gt;%
    as.tibble() %&gt;%
    bind_cols(
      data_0_tbl %&gt;%
        select(EmployeeNumber, MonthlyIncome, OverTime),
      data_1_tbl %&gt;%
        select(OverTime)
    ) %&gt;%
    rename(
      OverTime_0 = OverTime,
      OverTime_1 = OverTime1
    )
  

  avg_overtime_pct &lt;- avg_overtime_pct 
  
  
  ev_1_tbl &lt;- pred_1_tbl %&gt;%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %&gt;%
    mutate(
      cost_of_policy_change = case_when(
        OverTime_0 == &quot;Yes&quot; &amp; OverTime_1 == &quot;No&quot; ~ attrition_cost * avg_overtime_pct,
        TRUE ~ 0
      )
    ) %&gt;%
    mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
    )
  
  total_ev_1_tbl &lt;- ev_1_tbl %&gt;%
    summarise(
      total_expected_attrition_cost_1 = sum(expected_attrition_cost)
    )
  
  #Savings Calculation
  savings_tbl &lt;- bind_cols(
    total_ev_0_tbl,
    total_ev_1_tbl
  ) %&gt;%
    mutate(
      savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
      pct_savings = savings / total_expected_attrition_cost_0
    )
  
  return(savings_tbl$savings)
  
}</code></pre>
<pre class="r"><code>test_tbl %&gt;%
  calculate_savings_by_threshold_2(automl_leader, avg_overtime_pct = 0.10,
                                   net_revenue_per_employee = 250000)</code></pre>
<pre><code>## [1] 396856.1</code></pre>
<p>Savings are reduced</p>
<pre class="r"><code>test_tbl %&gt;%
  calculate_savings_by_threshold_2(automl_leader, avg_overtime_pct = 0.15,
                                   net_revenue_per_employee = 300000)</code></pre>
<pre><code>## [1] 179246.4</code></pre>
<p><strong>Sensitivity Analysis for OverTime</strong></p>
<p><strong>Classifier Calibration</strong>: This combination of threshold and expected
rates settings has our classifier calibrated to optimum FN / FP Ratio
for max savings. If cost / benefit change, settings need to be
recalibrated (re-optimized)</p>
<pre class="r"><code>max_savings_rates_tbl &lt;- rates_by_threshold_optimized_tbl %&gt;%
  filter(savings == max(savings))

max_savings_rates_tbl</code></pre>
<pre><code>## # A tibble: 1 x 6
##   threshold   tnr   fnr   fpr   tpr savings
##       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;
## 1     0.180 0.821 0.278 0.179 0.722 518469.</code></pre>
<pre class="r"><code>calculate_savings_by_threshold_2(
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_savings_rates_tbl$threshold,
  tnr = max_savings_rates_tbl$tnr,
  fnr =  max_savings_rates_tbl$fnr,
  fpr = max_savings_rates_tbl$fpr,
  tpr =  max_savings_rates_tbl$tpr
)</code></pre>
<pre><code>## [1] 518469.5</code></pre>
<pre class="r"><code># Preloaded Function : Preloads the calibrated settings that optimize threshold &amp;  maximize expected savings 

calculate_savings_by_threshold_2_preloaded &lt;- partial(
  calculate_savings_by_threshold_2,
  # Function Arguments
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_savings_rates_tbl$threshold,
  tnr = max_savings_rates_tbl$tnr,
  fnr =  max_savings_rates_tbl$fnr,
  fpr = max_savings_rates_tbl$fpr,
  tpr =  max_savings_rates_tbl$tpr
  
)


calculate_savings_by_threshold_2_preloaded(
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000)</code></pre>
<pre><code>## [1] 518469.5</code></pre>
<p>Multiple Combinations: We have two inputs that we are simultaneously
going to change:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Average OverTime Percent</strong> : If an employee works 100% they
essentially double their hours. We don’t expect this to be average.
Rather, it’s likely that the worst case is 30% or roughly 30% of 40
= 12 hours per week</p></li>
<li><p><strong>Net Revenue per Employee(NRPE)</strong> : On an income statement, take
the gross revenue minus Costs of Goods sold to get Net Revenue. They
spread this out across every employee to get an estimate of their
financial value to organization.</p></li>
</ol>
<p><strong>Worst Case</strong>: We believe $200,000 for lowest NRPE<br />
<strong>Best Case</strong>: We believe $400,000 for highest NRPE</p>
<pre class="r"><code>sensitivity_tbl &lt;- list(
  avg_overtime_pct = seq(0.05, 0.30, by = 0.05), 
  net_revenue_per_employee = seq(200000, 400000, by = 50000)
) %&gt;%
  cross_df() %&gt;%
  mutate(
    savings = pmap_dbl(
      .l = list(
        avg_overtime_pct = avg_overtime_pct,
        net_revenue_per_employee = net_revenue_per_employee
      ),
      .f = calculate_savings_by_threshold_2_preloaded
    )
  )</code></pre>
<pre class="r"><code>sensitivity_tbl</code></pre>
<pre><code>## # A tibble: 30 x 3
##    avg_overtime_pct net_revenue_per_employee savings
##               &lt;dbl&gt;                    &lt;dbl&gt;   &lt;dbl&gt;
##  1             0.05                   200000 534281.
##  2             0.1                    200000 428535.
##  3             0.15                   200000 322789.
##  4             0.2                    200000 217042.
##  5             0.25                   200000 111296.
##  6             0.3                    200000   5549.
##  7             0.05                   250000 646820.
##  8             0.1                    250000 518469.
##  9             0.15                   250000 390119.
## 10             0.2                    250000 261768.
## # … with 20 more rows</code></pre>
<p><strong>Heat Maps</strong> are a great way to show how two variables interact with a
third variable (the target variable)</p>
<pre class="r"><code>sensitivity_tbl %&gt;%
  ggplot(aes(avg_overtime_pct, net_revenue_per_employee)) +
  geom_tile(aes(fill = savings)) +
  geom_label(aes(label = savings %&gt;% round(0) %&gt;% scales::dollar())) +
  theme_tq() +
  theme(legend.position = &quot;none&quot;) +
  scale_fill_gradient2(
    low = palette_light()[[2]],
    mid = &quot;white&quot;,
    high = palette_light()[[1]],
    midpoint = 0
  ) + 
  scale_x_continuous(
    labels = scales::percent,
    breaks = seq(0.05, 0.30, by = 0.05)
  ) +
  scale_y_continuous(
    labels = scales::dollar
  ) +
  labs(
    title = &quot;Profitability Heatmap : Expected Savings Sensitivity Analysis &quot;,
    subtitle = &quot;How sensitive is savings to net revenue per employee and average overtime percentage ?&quot;,
    x = &quot;Average Overtime Percentage&quot;,
    y = &quot;Net Revenue Per Employee&quot;
  )</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-117-1.png" /></p>
<p>As long as people are not working OverTime more than 25%, we are break
even that is profit.</p>
<p><strong>Threshold Optimization For Stock Options</strong></p>
<p><strong>Challenge : People with no stock options are leaving</strong></p>
<p><strong>Find optimal threshold</strong></p>
<pre class="r"><code>avg_overtime_pct &lt;- 0.10
net_revenue_per_employee &lt;- 250000
stock_option_cost &lt;- 5000

data &lt;- test_tbl 
h2o_model &lt;- automl_leader

calculate_savings_by_threshold_3 &lt;- function(data, h2o_model, threshold = 0, 
                                             tnr = 0, fpr = 1, fnr = 0, tpr = 1,
                                             avg_overtime_pct = 0.10,
                                             net_revenue_per_employee = 250000,
                                             stock_option_cost = 5000) {
  
  data_0_tbl &lt;- as.tibble(data)
  
  # Expected Value
  
  # Calculating Expected Value with OT
  
  pred_0_tbl &lt;- h2o_model %&gt;%
    h2o.predict(newdata = as.h2o(data_0_tbl)) %&gt;%
    as.tibble() %&gt;%
    bind_cols(
      data_0_tbl %&gt;%
        select(EmployeeNumber, MonthlyIncome, OverTime, StockOptionLevel)
    )
  
  ev_0_tbl &lt;- pred_0_tbl %&gt;%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %&gt;%
    mutate(
      cost_of_policy_change = 0
    ) %&gt;%
    mutate(
      expected_attrition_cost = 
        Yes * (attrition_cost + cost_of_policy_change) +
        No  * (cost_of_policy_change)
    )
  
  total_ev_0_tbl &lt;- ev_0_tbl %&gt;%
    summarise(
      total_expected_attrition_cost_0 = sum(expected_attrition_cost)
    )
  
  # Calculating Expected Value with Targeted OT &amp; Stock Option Policy
  
  data_1_tbl &lt;- data_0_tbl %&gt;%
    add_column(Yes = pred_0_tbl$Yes) %&gt;%
    mutate(
      OverTime = case_when(
        Yes &gt;= threshold ~ factor(&quot;No&quot;, levels = levels(data_0_tbl$OverTime)) ,
        TRUE ~ OverTime
      )
    ) %&gt;%
    mutate(
      StockOptionLevel = case_when(
        Yes &gt;= threshold &amp; StockOptionLevel == 0 
        ~ factor(&quot;1&quot;, levels = levels(data_0_tbl$StockOptionLevel)) ,
        TRUE ~ StockOptionLevel
      )
    ) %&gt;%
    select(-Yes) 
  
  pred_1_tbl &lt;- h2o_model %&gt;%
    h2o.predict(newdata = as.h2o(data_1_tbl)) %&gt;%
    as.tibble() %&gt;%
    bind_cols(
      data_0_tbl %&gt;%
        select(EmployeeNumber, MonthlyIncome, OverTime, StockOptionLevel),
      data_1_tbl %&gt;%
        select(OverTime, StockOptionLevel)
    ) %&gt;%
    rename(
      OverTime_0 = OverTime,
      OverTime_1 = OverTime1,
      StockOptionLevel_0 = StockOptionLevel,
      StockOptionLevel_1 = StockOptionLevel1   
    )
  

  avg_overtime_pct &lt;- avg_overtime_pct 
  stock_option_cost &lt;- stock_option_cost 
  
  
  ev_1_tbl &lt;- pred_1_tbl %&gt;%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %&gt;%
    # cost OT
    mutate(
      cost_OT = case_when(
        OverTime_0 == &quot;Yes&quot; &amp; OverTime_1 == &quot;No&quot;
        ~  avg_overtime_pct * MonthlyIncome * 12,
        TRUE ~ 0
      )
    ) %&gt;%
    # cost Stock Option
    mutate(
      cost_SO = case_when(
        StockOptionLevel_1 == &quot;1&quot; &amp; StockOptionLevel_0 == &quot;0&quot;
        ~ stock_option_cost,
        TRUE ~ 0
      )
    ) %&gt;%
    mutate(cost_of_policy_change = cost_OT + cost_SO) %&gt;%
    mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
    )
  
  total_ev_1_tbl &lt;- ev_1_tbl %&gt;%
    summarise(
      total_expected_attrition_cost_1 = sum(expected_attrition_cost)
    )
  
  # Savings Calculation
  savings_tbl &lt;- bind_cols(
    total_ev_0_tbl,
    total_ev_1_tbl
  ) %&gt;%
    mutate(
      savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
      pct_savings = savings / total_expected_attrition_cost_0
    )
  
  return(savings_tbl$savings)
  
}</code></pre>
<pre class="r"><code>max_f1_tbl &lt;- rates_by_threshold_tbl %&gt;%
  select(threshold, f1, tnr:tpr) %&gt;%
  filter(f1 == max(f1))

max_f1_savings &lt;- calculate_savings_by_threshold_3(
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_f1_tbl$threshold,
  tnr = max_f1_tbl$tnr,
  fnr =  max_f1_tbl$fnr,
  fpr = max_f1_tbl$fpr,
  tpr =  max_f1_tbl$tpr,
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000,
  stock_option_cost = 5000
)</code></pre>
<p><strong>Optimisation</strong></p>
<pre class="r"><code>smpl &lt;- seq(1, 220, length.out = 20) %&gt;% round(digits = 0)
  
calculate_savings_by_threshold_3_preloaded &lt;- partial(
  calculate_savings_by_threshold_3,
  # Function Arguments
  data = test_tbl,
  h2o_model = automl_leader,
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000,
  stock_option_cost = 5000)

rates_by_threshold_optimized_tbl_3 &lt;- rates_by_threshold_tbl %&gt;%
  select(threshold, tnr:tpr) %&gt;%
  slice(smpl) %&gt;%
  mutate(
    savings = pmap_dbl(.l = list(
      threshold = threshold,
      tnr = tnr,
      fnr = fnr,
      fpr = fpr,
      tpr = tpr
    ),
    .f = calculate_savings_by_threshold_3_preloaded
    )
  )</code></pre>
<pre class="r"><code>rates_by_threshold_optimized_tbl_3</code></pre>
<pre><code>## # A tibble: 20 x 6
##    threshold    tnr    fnr     fpr    tpr  savings
##        &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;
##  1    0.957  1      0.972  0       0.0278   31319.
##  2    0.740  0.995  0.667  0.00543 0.333   469050.
##  3    0.456  0.967  0.5    0.0326  0.5     709481.
##  4    0.324  0.940  0.306  0.0598  0.694   909679.
##  5    0.261  0.880  0.306  0.120   0.694  1025848.
##  6    0.180  0.821  0.278  0.179   0.722  1052881.
##  7    0.140  0.772  0.222  0.228   0.778  1061113.
##  8    0.113  0.712  0.194  0.288   0.806  1044772.
##  9    0.0956 0.658  0.167  0.342   0.833  1027305.
## 10    0.0763 0.598  0.139  0.402   0.861   997224.
## 11    0.0659 0.549  0.0833 0.451   0.917   952199.
## 12    0.0574 0.489  0.0556 0.511   0.944   920897.
## 13    0.0524 0.435  0.0278 0.565   0.972   894789.
## 14    0.0481 0.375  0      0.625   1       867870.
## 15    0.0444 0.315  0      0.685   1       832960.
## 16    0.0408 0.25   0      0.75    1       804168.
## 17    0.0382 0.190  0      0.810   1       804168.
## 18    0.0344 0.125  0      0.875   1       786774.
## 19    0.0315 0.0652 0      0.935   1       786774.
## 20    0.0296 0      0      1       1       753165.</code></pre>
<pre class="r"><code>rates_by_threshold_optimized_tbl_3 %&gt;%
  filter(savings == max(savings))</code></pre>
<pre><code>## # A tibble: 1 x 6
##   threshold   tnr   fnr   fpr   tpr  savings
##       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1     0.140 0.772 0.222 0.228 0.778 1061113.</code></pre>
<pre class="r"><code>rates_by_threshold_optimized_tbl_3 %&gt;%
  ggplot(aes(threshold, savings)) +
  
  #Vlines
  geom_vline(xintercept = max_f1_tbl$threshold, 
             color = palette_light()[[5]] , size = 2) +
  geom_vline(aes(xintercept = threshold), 
             color = palette_light()[[3]] , size = 2,
            data =  rates_by_threshold_optimized_tbl_3 %&gt;%
              filter(savings ==  max(savings))
  ) +
  # Points
  geom_line(color = palette_light()[[1]]) +
  geom_point(color = palette_light()[[1]]) +
  
  # F1 Max
  annotate(geom = &quot;label&quot;, label = scales::dollar(max_f1_savings),
           x = max_f1_tbl$threshold, y = max_f1_savings, vjust = -1,
           color = palette_light()[[1]]) +
  
  # Optimal Point 
  geom_point(shape = 21, size = 5, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl_3 %&gt;%
               filter(savings == max(savings))) +
  
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -2, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl_3 %&gt;%
               filter(savings == max(savings))) +

  # No OT Policy
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %&gt;%
               filter(threshold == min(threshold))) +
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %&gt;%
               filter(threshold == min(threshold))) +
  
  # Do Nothing
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %&gt;%
               filter(threshold == max(threshold))) +
  geom_label(aes(label = scales::dollar(round(savings,0))), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %&gt;%
               filter(threshold == max(threshold))) +
  
  # Aesthestics 
  theme_tq() +
  expand_limits(x = c(-.1, 1.1), y = 12e5) + 
  scale_x_continuous(labels  = scales::percent,
                     breaks = seq(0, 1, by = 0.2)) +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = &quot;Optimization Results : Expected Savings Maximized At 26.7% &quot;,
       x = &quot;Threshold (%)&quot;, 
       y = &quot;Savings&quot;)</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-122-1.png" /></p>
<p><strong>Sensitivity Analysis at Optimal Threshold</strong></p>
<pre class="r"><code>net_revenue_per_employee &lt;- 250000
avg_overtime_pct &lt;- seq(0.05, 0.30, by = 0.05)
stock_option_cost &lt;- seq(5000, 25000, by = 5000)

max_savings_rates_tbl_3 &lt;- rates_by_threshold_optimized_tbl_3 %&gt;%
filter(savings == max(savings))

max_savings_rates_tbl_3</code></pre>
<pre><code>## # A tibble: 1 x 6
##   threshold   tnr   fnr   fpr   tpr  savings
##       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;
## 1     0.140 0.772 0.222 0.228 0.778 1061113.</code></pre>
<pre class="r"><code>calculate_savings_by_threshold_3_preloaded &lt;- partial(
  calculate_savings_by_threshold_3,
  # Function Arguments
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_savings_rates_tbl$threshold,
  tnr = max_savings_rates_tbl_3$tnr,
  fnr =  max_savings_rates_tbl_3$fnr,
  fpr = max_savings_rates_tbl_3$fpr,
  tpr =  max_savings_rates_tbl_3$tpr
  
)</code></pre>
<pre class="r"><code>calculate_savings_by_threshold_3_preloaded(
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000,
  stock_option_cost = 5000)  </code></pre>
<pre><code>## [1] 1052881</code></pre>
<pre class="r"><code>sensitivity_tbl_3 &lt;- list(
  avg_overtime_pct  = seq(0.05, 0.30, by = 0.05),
  net_revenue_per_employee = 250000,
  stock_option_cost = seq(5000, 25000, by = 5000)
) %&gt;%
  cross_df() %&gt;%
  mutate(
    savings = pmap_dbl(
      .l = list(
        avg_overtime_pct = avg_overtime_pct,
        net_revenue_per_employee = net_revenue_per_employee,
        stock_option_cost = stock_option_cost
      ),
      .f = calculate_savings_by_threshold_3_preloaded
    )
  )</code></pre>
<pre class="r"><code>sensitivity_tbl_3</code></pre>
<pre><code>## # A tibble: 30 x 4
##    avg_overtime_pct net_revenue_per_employee stock_option_cost  savings
##               &lt;dbl&gt;                    &lt;dbl&gt;             &lt;dbl&gt;    &lt;dbl&gt;
##  1             0.05                   250000              5000 1136672.
##  2             0.1                    250000              5000 1052881.
##  3             0.15                   250000              5000  969090.
##  4             0.2                    250000              5000  885299.
##  5             0.25                   250000              5000  801508.
##  6             0.3                    250000              5000  717716.
##  7             0.05                   250000             10000  911672.
##  8             0.1                    250000             10000  827881.
##  9             0.15                   250000             10000  744090.
## 10             0.2                    250000             10000  660299.
## # … with 20 more rows</code></pre>
<pre class="r"><code>sensitivity_tbl_3 %&gt;%
  ggplot(aes(avg_overtime_pct, stock_option_cost)) +
  geom_tile(aes(fill = savings)) +
  geom_label(aes(label = savings %&gt;% round(0) %&gt;% scales::dollar())) +
  theme_tq() +
  theme(legend.position = &quot;none&quot;) +
  scale_fill_gradient2(
    low = palette_light()[[2]],
    mid = &quot;white&quot;,
    high = palette_light()[[1]],
    midpoint = 0
  ) + 
  scale_x_continuous(
    labels = scales::percent,
    breaks = seq(0.05, 0.30, by = 0.05)
  ) +
  scale_y_continuous(
    labels = scales::dollar,
    breaks = seq(5000, 25000, by = 5000)
  ) +
  labs(
    title = &quot;Profitability Heatmap : Expected Savings Sensitivity Analysis &quot;,
    subtitle = &quot;How sensitive is savings to stock options cost and average overtime percentage ?&quot;,
    x = &quot;Average Overtime Percentage&quot;,
    y = &quot;Average Stock Options Cost&quot;
  )</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-127-1.png" /></p>
<p>As longs as average overtime percentage is less than 18% and avg stocks
options cost less than $20,000, we are break even that is profit.</p>
<p><strong>Recommendation Algorithm Strategy</strong></p>
<p><strong>Purpose</strong> To enable immediate manager to reduce attrition using an
automated recommendation system</p>
<p><strong>Strategy Logic:</strong></p>
<ul>
<li>Uses Correlation Analysis to determine logical strategies that can
be adjusted via stakeholder feedback<br />
</li>
<li>Investigate your strategies and try to put them into 2 or 3 groups.
This helps managers focus on specific areas.</li>
</ul>
<p>! <span class="math display">\[Alt text\]</span> (/Untitled/Users/raj/Pictures/Snip20190109_2.png⁩)</p>
<p><strong>Strategy Groups</strong></p>
<ul>
<li><strong>Work Life</strong> : OverTime, BusinessTravel</li>
<li><strong>Personal Development</strong> : Traning &amp; Mentoring Employees, Sponsoring
Higher Education</li>
<li><strong>Professional Development</strong>: Promotion based on experience &amp; Job
level</li>
</ul>
<p><strong>Recipes for Correlation Analysis</strong></p>
<p>After applying recipe, we get a discretized Output i.e all are binary
features 0’s and 1’s only. This discretization of features will help us
in identifying strategies.</p>
<pre class="r"><code>recipe_obj &lt;- recipe(Attrition ~ . , data = train_readable_tbl) %&gt;%
  step_zv(all_predictors()) %&gt;%
  step_num2factor(factor_names) %&gt;%
  step_discretize(all_numeric(), options = list(min_unique = 1)) %&gt;%
  step_dummy(all_nominal(), one_hot = TRUE) %&gt;%
  prep()</code></pre>
<pre class="r"><code>train_corr_tbl &lt;- bake(recipe_obj, new_data = train_readable_tbl)
train_corr_tbl %&gt;% glimpse()</code></pre>
<pre><code>## Observations: 1,250
## Variables: 141
## $ Age_bin_missing                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ Age_bin1                            &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0…
## $ Age_bin2                            &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 0, 0, 1…
## $ Age_bin3                            &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 1, 0…
## $ Age_bin4                            &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 0…
## $ BusinessTravel_Non.Travel           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ BusinessTravel_Travel_Rarely        &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 1, 1, 0, 1…
## $ BusinessTravel_Travel_Frequently    &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 0, 0, 1, 0…
## $ DailyRate_bin_missing               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ DailyRate_bin1                      &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 1, 0…
## $ DailyRate_bin2                      &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…
## $ DailyRate_bin3                      &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0…
## $ DailyRate_bin4                      &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 1, 1, 0, 1…
## $ Department_Human.Resources          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ Department_Research...Development   &lt;dbl&gt; 0, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ Department_Sales                    &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ DistanceFromHome_bin_missing        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ DistanceFromHome_bin1               &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 0, 0, 0, 0…
## $ DistanceFromHome_bin2               &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 1, 0, 0, 0…
## $ DistanceFromHome_bin3               &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…
## $ DistanceFromHome_bin4               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 1, 1…
## $ Education_Below.College             &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 0…
## $ Education_College                   &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 0, 0, 0…
## $ Education_Bachelor                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 1, 1…
## $ Education_Master                    &lt;dbl&gt; 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…
## $ Education_Doctor                    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ EducationField_Human.Resources      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ EducationField_Life.Sciences        &lt;dbl&gt; 1, 1, 0, 1, 0, 1, 0, 1, 1, 0…
## $ EducationField_Marketing            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ EducationField_Medical              &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 1, 0, 0, 1…
## $ EducationField_Other                &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…
## $ EducationField_Technical.Degree     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ EmployeeNumber_bin_missing          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ EmployeeNumber_bin1                 &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
## $ EmployeeNumber_bin2                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ EmployeeNumber_bin3                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ EmployeeNumber_bin4                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ EnvironmentSatisfaction_Low         &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…
## $ EnvironmentSatisfaction_Medium      &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ EnvironmentSatisfaction_High        &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 0, 0, 1…
## $ EnvironmentSatisfaction_Very.High   &lt;dbl&gt; 0, 0, 1, 1, 0, 1, 0, 1, 1, 0…
## $ Gender_Female                       &lt;dbl&gt; 1, 0, 0, 1, 0, 0, 1, 0, 0, 0…
## $ Gender_Male                         &lt;dbl&gt; 0, 1, 1, 0, 1, 1, 0, 1, 1, 1…
## $ HourlyRate_bin_missing              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ HourlyRate_bin1                     &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 1, 0…
## $ HourlyRate_bin2                     &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0…
## $ HourlyRate_bin3                     &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 1, 0, 0…
## $ HourlyRate_bin4                     &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 1…
## $ JobInvolvement_Low                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ JobInvolvement_Medium               &lt;dbl&gt; 0, 1, 1, 0, 0, 0, 0, 0, 1, 0…
## $ JobInvolvement_High                 &lt;dbl&gt; 1, 0, 0, 1, 1, 1, 0, 1, 0, 1…
## $ JobInvolvement_Very.High            &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…
## $ JobLevel_X1                         &lt;dbl&gt; 0, 0, 1, 1, 1, 1, 1, 1, 0, 0…
## $ JobLevel_X2                         &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 1…
## $ JobLevel_X3                         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…
## $ JobLevel_X4                         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ JobLevel_X5                         &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ JobRole_Healthcare.Representative   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…
## $ JobRole_Human.Resources             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ JobRole_Laboratory.Technician       &lt;dbl&gt; 0, 0, 1, 0, 1, 1, 1, 1, 0, 0…
## $ JobRole_Manager                     &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ JobRole_Manufacturing.Director      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…
## $ JobRole_Research.Director           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ JobRole_Research.Scientist          &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0…
## $ JobRole_Sales.Executive             &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ JobRole_Sales.Representative        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ JobSatisfaction_Low                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…
## $ JobSatisfaction_Medium              &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0…
## $ JobSatisfaction_High                &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 0, 1, 1, 1…
## $ JobSatisfaction_Very.High           &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 0, 0, 0…
## $ MaritalStatus_Single                &lt;dbl&gt; 1, 0, 1, 0, 0, 1, 0, 0, 1, 0…
## $ MaritalStatus_Married               &lt;dbl&gt; 0, 1, 0, 1, 1, 0, 1, 0, 0, 1…
## $ MaritalStatus_Divorced              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…
## $ MonthlyIncome_bin_missing           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ MonthlyIncome_bin1                  &lt;dbl&gt; 0, 0, 1, 1, 0, 0, 1, 1, 0, 0…
## $ MonthlyIncome_bin2                  &lt;dbl&gt; 0, 0, 0, 0, 1, 1, 0, 0, 0, 0…
## $ MonthlyIncome_bin3                  &lt;dbl&gt; 1, 1, 0, 0, 0, 0, 0, 0, 0, 1…
## $ MonthlyIncome_bin4                  &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…
## $ MonthlyRate_bin_missing             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ MonthlyRate_bin1                    &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…
## $ MonthlyRate_bin2                    &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 1, 1, 0…
## $ MonthlyRate_bin3                    &lt;dbl&gt; 1, 0, 0, 0, 1, 0, 0, 0, 0, 1…
## $ MonthlyRate_bin4                    &lt;dbl&gt; 0, 1, 0, 1, 0, 0, 0, 0, 0, 0…
## $ NumCompaniesWorked_bin_missing      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ NumCompaniesWorked_bin1             &lt;dbl&gt; 0, 1, 0, 1, 0, 1, 0, 1, 1, 0…
## $ NumCompaniesWorked_bin2             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ NumCompaniesWorked_bin3             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…
## $ NumCompaniesWorked_bin4             &lt;dbl&gt; 1, 0, 1, 0, 1, 0, 0, 0, 0, 1…
## $ OverTime_No                         &lt;dbl&gt; 0, 1, 0, 0, 1, 1, 0, 1, 1, 1…
## $ OverTime_Yes                        &lt;dbl&gt; 1, 0, 1, 1, 0, 0, 1, 0, 0, 0…
## $ PercentSalaryHike_bin_missing       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ PercentSalaryHike_bin1              &lt;dbl&gt; 1, 0, 0, 1, 1, 0, 0, 0, 0, 0…
## $ PercentSalaryHike_bin2              &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 0, 0, 0, 1…
## $ PercentSalaryHike_bin3              &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…
## $ PercentSalaryHike_bin4              &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 1, 1, 0…
## $ PerformanceRating_Low               &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ PerformanceRating_Good              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ PerformanceRating_Excellent         &lt;dbl&gt; 1, 0, 1, 1, 1, 1, 0, 0, 0, 1…
## $ PerformanceRating_Outstanding       &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 1, 1, 1, 0…
## $ RelationshipSatisfaction_Low        &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 1, 0, 0, 0…
## $ RelationshipSatisfaction_Medium     &lt;dbl&gt; 0, 0, 1, 0, 0, 0, 0, 1, 1, 1…
## $ RelationshipSatisfaction_High       &lt;dbl&gt; 0, 0, 0, 1, 0, 1, 0, 0, 0, 0…
## $ RelationshipSatisfaction_Very.High  &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 0, 0, 0…
## $ StockOptionLevel_X0                 &lt;dbl&gt; 1, 0, 1, 1, 0, 1, 0, 0, 1, 0…
## $ StockOptionLevel_X1                 &lt;dbl&gt; 0, 1, 0, 0, 1, 0, 0, 1, 0, 0…
## $ StockOptionLevel_X2                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…
## $ StockOptionLevel_X3                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…
## $ TotalWorkingYears_bin_missing       &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ TotalWorkingYears_bin1              &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 1, 0, 0…
## $ TotalWorkingYears_bin2              &lt;dbl&gt; 1, 1, 1, 1, 0, 1, 0, 0, 1, 0…
## $ TotalWorkingYears_bin3              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…
## $ TotalWorkingYears_bin4              &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…
## $ TrainingTimesLastYear_bin_missing   &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ TrainingTimesLastYear_bin1          &lt;dbl&gt; 1, 0, 0, 0, 0, 1, 0, 1, 1, 0…
## $ TrainingTimesLastYear_bin2          &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 1, 0, 0, 1…
## $ TrainingTimesLastYear_bin3          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ WorkLifeBalance_Bad                 &lt;dbl&gt; 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ WorkLifeBalance_Good                &lt;dbl&gt; 0, 0, 0, 0, 0, 1, 1, 0, 0, 1…
## $ WorkLifeBalance_Better              &lt;dbl&gt; 0, 1, 1, 1, 1, 0, 0, 1, 1, 0…
## $ WorkLifeBalance_Best                &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ YearsAtCompany_bin_missing          &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ YearsAtCompany_bin1                 &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 1, 0, 0…
## $ YearsAtCompany_bin2                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ YearsAtCompany_bin3                 &lt;dbl&gt; 1, 1, 0, 1, 0, 1, 0, 0, 1, 1…
## $ YearsAtCompany_bin4                 &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ YearsInCurrentRole_bin_missing      &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ YearsInCurrentRole_bin1             &lt;dbl&gt; 0, 0, 1, 0, 1, 0, 1, 1, 0, 0…
## $ YearsInCurrentRole_bin2             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ YearsInCurrentRole_bin3             &lt;dbl&gt; 1, 1, 0, 1, 0, 1, 0, 0, 1, 1…
## $ YearsInCurrentRole_bin4             &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ YearsSinceLastPromotion_bin_missing &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ YearsSinceLastPromotion_bin1        &lt;dbl&gt; 1, 1, 1, 0, 0, 0, 1, 1, 1, 0…
## $ YearsSinceLastPromotion_bin2        &lt;dbl&gt; 0, 0, 0, 1, 1, 1, 0, 0, 0, 0…
## $ YearsSinceLastPromotion_bin3        &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…
## $ YearsWithCurrManager_bin_missing    &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ YearsWithCurrManager_bin1           &lt;dbl&gt; 0, 0, 1, 1, 1, 0, 1, 1, 0, 0…
## $ YearsWithCurrManager_bin2           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
## $ YearsWithCurrManager_bin3           &lt;dbl&gt; 1, 1, 0, 0, 0, 1, 0, 0, 0, 1…
## $ YearsWithCurrManager_bin4           &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…
## $ Attrition_No                        &lt;dbl&gt; 0, 1, 0, 1, 1, 1, 1, 1, 1, 1…
## $ Attrition_Yes                       &lt;dbl&gt; 1, 0, 1, 0, 0, 0, 0, 0, 0, 0…</code></pre>
<p><strong>Data Manipulation for Correlation Visualization</strong></p>
<pre class="r"><code>cor_level &lt;- 0.06

correlation_results_tbl &lt;- train_corr_tbl %&gt;%
  select(-Attrition_No) %&gt;%
  get_cor(Attrition_Yes, fct_reorder = TRUE, fct_rev = TRUE) %&gt;%
  filter(abs(Attrition_Yes) &gt;= cor_level) %&gt;%
  mutate(
    relationship = case_when(
      Attrition_Yes &gt; 0 ~ &quot;Supports&quot;,
      TRUE ~ &quot;Contradicts&quot;
    )
  ) %&gt;%
  mutate(feature_text = as.character(feature)) %&gt;%
  separate(feature_text, into = &quot;feature_base&quot;, sep = &quot;_&quot;, extra = &quot;drop&quot;) %&gt;%
  mutate(feature_base = as_factor(feature_base) %&gt;% fct_rev())


length_unique_groups &lt;- correlation_results_tbl %&gt;%
  pull(feature_base) %&gt;% 
  unique() %&gt;%
  length()</code></pre>
<pre class="r"><code>correlation_results_tbl %&gt;%
  ggplot(aes(Attrition_Yes, feature_base, color = relationship)) +
  geom_point() + 
  geom_label(aes(label = feature), vjust = -0.5) +
  expand_limits(x = c(-0.3, 0.3), y = c(1,length_unique_groups + 1)) +
  theme_tq() +
  scale_color_tq() +
  labs(
    title = &quot;Correlation Analysis : Recommendation Strategy Development&quot;,
    subtitle =  &quot;Discretizing feature to help identify a strategy&quot;
    )</code></pre>
<p><img src="https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-131-1.png" /></p>
<p><strong>Recommendation Strategy Development</strong></p>
<ul>
<li>Implementing Strategies into code<br />
</li>
<li>We will be using <strong>Good Better Best Approach</strong> for building a
recommendation strategy development</li>
</ul>
<p>Function <strong>recommend_strategies</strong> evaluates recommendation startegies
given an employee number</p>
<pre class="r"><code>recommend_strategies &lt;- function(data, employee_number){
  
  data %&gt;%
    filter(EmployeeNumber == employee_number) %&gt;%
    mutate_if(is.factor, as.numeric) %&gt;%
    
    # Personal Development Strategy 
    mutate(
      personal_development_strategy =  case_when(
        
        # (Worst Case) Create Personal Development Plan: Job Involvement, JobSatisfaction, PerformanceRating 
        PerformanceRating == 1 | 
          JobSatisfaction == 1 |
          JobInvolvement  &lt;= 2      ~  &quot;Create Personal Development Plan&quot;,
        # (Better Case) Promote Training &amp; Formation: YearsAtCompany, TotalWorkingYears 
        YearsAtCompany &lt; 3 |
          TotalWorkingYears &lt; 6    ~  &quot;Promote Training &amp; Formation&quot; ,
        
        # (Best Case 1) Seek Mentorship Role: YearsInCurrentRole, YearsAtCompany, PerformanceRating, JobSatisfaction
        (YearsInCurrentRole &gt; 3 |  YearsAtCompany &gt;= 5) &amp; 
          PerformanceRating &gt;= 3 &amp; 
          JobSatisfaction == 4  ~  &quot;Seek Mentorship Role&quot;,
        
        
        # (Best Case 2) Seek Leadership Role : JobInvolvement, JobSatisfaction, PerformanceRating
        JobInvolvement &gt;= 3 &amp; 
          JobSatisfaction &gt;= 3 &amp;
          PerformanceRating&gt;= 3  ~ &quot;Seek Leadership Role&quot;,
        
        # Catch All
        TRUE ~ &quot;Retain and Maintain&quot;
      )
    ) %&gt;%
    # select(EmployeeNumber, personal_development_strategy)
  
  
    # Professional Development Strategy
  mutate(
    professional_development_strategy =  case_when(
      
      #  Ready for Rotation: YearsInCurrentRole,  JobSatisfaction (LOW)
      YearsInCurrentRole &gt;= 2 &amp;
        JobSatisfaction &lt;= 2                 ~&quot;Ready for Rotation&quot;,
      
      #  Ready for Promotion Level 2: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 1 &amp;
        YearsInCurrentRole &gt;=2 &amp;
        JobInvolvement &gt;=3 &amp;
        PerformanceRating &gt;=3                 ~ &quot;Ready for Promotion&quot;,   
      
      #  Ready for Promotion Level 3: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 2 &amp;
        YearsInCurrentRole &gt;=2 &amp;
        JobInvolvement &gt;=4 &amp;
        PerformanceRating &gt;=3                 ~ &quot;Ready for Promotion&quot;,   
      
      #  Ready for Promotion Level 4: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 3 &amp;
        YearsInCurrentRole &gt;=3 &amp;
        JobInvolvement &gt;=4 &amp;
        PerformanceRating &gt;=3                 ~ &quot;Ready for Promotion&quot;,   
      
      #  Ready for Promotion Level 5: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 4 &amp;
        YearsInCurrentRole &gt;=4 &amp;
        JobInvolvement &gt;=4 &amp;
        PerformanceRating &gt;=3                 ~ &quot;Ready for Promotion&quot;,   
      
      #  Incentivize Specialization : YearsInCurrentRole, JobSatisfaction, PerformanceRating
      YearsInCurrentRole &gt;= 4 &amp; 
        JobSatisfaction &gt;= 4 &amp;
        PerformanceRating &gt;= 3         ~ &quot;Incentivize Specialization&quot;,
      
      # Catch All
      TRUE ~ &quot;Retain and Maintain&quot;
    )
  ) %&gt;%
  #select(EmployeeNumber, personal_development_strategy,professional_development_strategy)  
    
   # Work-Environment Strategy
    mutate(
      work_environment_strategy =  case_when(
        
        # Improve Work - Life Balance : OverTime, WorkLifeBalance
        OverTime == 2 | 
          WorkLifeBalance == 1                 ~ &quot;Improve Work-Life Balance&quot;,
        
        # Monitor Business Travel: BusinessTravel, DistanceFromHome, WorkLifeBalance    
        ( BusinessTravel == 3|
            DistanceFromHome &gt;= 10) &amp;
          WorkLifeBalance == 2                 ~ &quot;Monitor Business Travel&quot;,
        
        
        # Review Job Assignment: EnvironmentSatisfaction, YearsInCurrentRole 
        EnvironmentSatisfaction == 1 &amp;
          YearsInCurrentRole &gt;= 2             ~ &quot;Review Job Assignment&quot;,
        
        # Promote Job Engagement: JobInvolvement
        JobInvolvement &lt;= 2   ~ &quot;Promote Job Engagement&quot;,
        
        # Catch All
        TRUE ~ &quot;Retain and Maintain&quot;
      )
    ) %&gt;%
    select(EmployeeNumber, personal_development_strategy,professional_development_strategy, work_environment_strategy)
}</code></pre>
<p>Now, we will evaluate strategies for employees both in training and test
data.</p>
<p>Recommendation strategies for <em>Employee number 4</em> from training data:</p>
<pre class="r"><code>train_readable_tbl %&gt;%
  recommend_strategies(4)</code></pre>
<pre><code>## # A tibble: 1 x 4
##   EmployeeNumber personal_development_strategy   
##            &lt;dbl&gt; &lt;chr&gt;                           
## 1              4 Create Personal Development Plan
##   professional_development_strategy work_environment_strategy
##   &lt;chr&gt;                             &lt;chr&gt;                    
## 1 Retain and Maintain               Improve Work-Life Balance</code></pre>
<p>Recommendation strategies for <em>Employee number 228</em> from test data :</p>
<pre class="r"><code>test_readable_tbl %&gt;%
  recommend_strategies(228)</code></pre>
<pre><code>## # A tibble: 1 x 4
##   EmployeeNumber personal_development_strategy
##            &lt;dbl&gt; &lt;chr&gt;                        
## 1            228 Seek Mentorship Role         
##   professional_development_strategy work_environment_strategy
##   &lt;chr&gt;                             &lt;chr&gt;                    
## 1 Incentivize Specialization        Retain and Maintain</code></pre>
</div>
<div id="phase-6-deployment" class="section level3">
<h3>Phase 6 : Deployment</h3>
<p>In this phase, we can deploy a nice formatted dashboard or shiny apps
for stakeholders to take decisions given an Employee number (To Be
Continued…)</p>
</div>
<div id="inspired-from" class="section level3">
<h3>Inspired From</h3>
<ul>
<li>Matt Dancho - <a href="https://www.business-science.io/">Business Science
University</a></li>
</ul>
</div>
<div id="disclaimer" class="section level3">
<h3>Disclaimer</h3>
<p>I am not authorized, endorsed by, or in any way officially connected
with H2O.ai. The views are mine not my employers.</p>
</div>
</div>
