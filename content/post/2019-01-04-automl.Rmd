---
title: "AutoML in Practice"
author: RAJ KUMAR
date: '2019-01-04'
slug: automl-with-h2o
#lastmod: '`r Sys.Date()`'
categories: []
tags:
  - automl
  - R
banner: 'banners/employee_churn.jpeg'
description: "Predicting Employee Churn using H2O and LIME"
images: ['banners/employee_churn.jpeg']
menu: ''    
---


## Introduction: The Problem

Dataset Source:

[<https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/>\]

## Required packages

``` r
library(h2o)          # High performance machine learning
library(lime)         # Explaining black-box models
library(recipes)      # Creating ML preprocessing recipes
library(tidyverse)    # Set of pkgs for data science: dplyr, ggplot2, purrr, tidyr, ...
library(tidyquant)    # Financial time series pkg - Used for theme_tq ggplot2 theme
library(glue)         # Pasting text
library(cowplot)      # Handling multiple ggplots
library(GGally)       # Data understanding - visualizations
library(skimr)        # Data understanding - summary information
library(fs)           # Working with the file system - directory structure
library(readxl)       # Reading excel files
library(forcats)      # Tools for working with categorical variables
library(writexl)      # Writing to excel files
```

**Reading dataset in R**

``` r
path_train <- "00_Data/telco_train.xlsx"
train_raw_tbl<-read_excel(path_train,sheet = 1)
colnames(train_raw_tbl)
```

    ##  [1] "Age"                      "Attrition"               
    ##  [3] "BusinessTravel"           "DailyRate"               
    ##  [5] "Department"               "DistanceFromHome"        
    ##  [7] "Education"                "EducationField"          
    ##  [9] "EmployeeCount"            "EmployeeNumber"          
    ## [11] "EnvironmentSatisfaction"  "Gender"                  
    ## [13] "HourlyRate"               "JobInvolvement"          
    ## [15] "JobLevel"                 "JobRole"                 
    ## [17] "JobSatisfaction"          "MaritalStatus"           
    ## [19] "MonthlyIncome"            "MonthlyRate"             
    ## [21] "NumCompaniesWorked"       "Over18"                  
    ## [23] "OverTime"                 "PercentSalaryHike"       
    ## [25] "PerformanceRating"        "RelationshipSatisfaction"
    ## [27] "StandardHours"            "StockOptionLevel"        
    ## [29] "TotalWorkingYears"        "TrainingTimesLastYear"   
    ## [31] "WorkLifeBalance"          "YearsAtCompany"          
    ## [33] "YearsInCurrentRole"       "YearsSinceLastPromotion" 
    ## [35] "YearsWithCurrManager"

## Business Science Problem Framework

![Credits: Business Science
University](https://www.business-science.io/assets/2018-06-19_BSPF/bspf_top.PNG)

### Phase 1 : Business Understanding

#### View Business as Machine

**Step 1: Isolate Business Units**

In realistic situation, we may not have the data on all the columns
listed, so subsetting data here for fields of interest like
EmployeeNumber, Department, JobRole, Attrition

``` r
dept_job_role_tbl <- train_raw_tbl %>% select(EmployeeNumber,Department,JobRole,Attrition)

dept_job_role_tbl
```

    ## # A tibble: 1,250 x 4
    ##    EmployeeNumber Department             JobRole                  
    ##             <dbl> <chr>                  <chr>                    
    ##  1              1 Sales                  Sales Executive          
    ##  2              2 Research & Development Research Scientist       
    ##  3              4 Research & Development Laboratory Technician    
    ##  4              5 Research & Development Research Scientist       
    ##  5              7 Research & Development Laboratory Technician    
    ##  6              8 Research & Development Laboratory Technician    
    ##  7             10 Research & Development Laboratory Technician    
    ##  8             11 Research & Development Laboratory Technician    
    ##  9             12 Research & Development Manufacturing Director   
    ## 10             13 Research & Development Healthcare Representative
    ##    Attrition
    ##    <chr>    
    ##  1 Yes      
    ##  2 No       
    ##  3 Yes      
    ##  4 No       
    ##  5 No       
    ##  6 No       
    ##  7 No       
    ##  8 No       
    ##  9 No       
    ## 10 No       
    ## # … with 1,240 more rows

In this problem, potential candidates of business units are
**Department** and **Job Roles**

**Step 2: Objectives**

Our business objective is to “**Retain High Performers**”

``` r
dept_job_role_tbl %>% 
  group_by(Attrition) %>% 
  summarize(n=n()) %>% 
  ungroup() %>% 
  mutate(pct=n/sum(n))
```

    ## # A tibble: 2 x 3
    ##   Attrition     n   pct
    ##   <chr>     <int> <dbl>
    ## 1 No         1049 0.839
    ## 2 Yes         201 0.161

**Step 3: Collect outcomes in terms of feedback. Feedback identifies
problems**

This implies 16% Attrition. We should understand that not all attrition
is bad, employee productivity varies. Some attrition is necessary, lets
say poor job employee fit and some attrition is bad, say loosing a high
performer. We need to assess whether this 16% is good attrition or bad
attrition

#### Understand the Drivers

**Step 1: Investigate if objectives are being met**

Organization has 16% Attrition rate

**Step 2: Synthesize outcomes**

**High counts and High Percentages**

``` r
options(dplyr.width = Inf)

dept_job_role_tbl %>% 
  
  group_by(Department,Attrition) %>% 
  summarize(n=n()) %>% 
  ungroup() %>% 
  
  group_by(Department) %>%
  mutate(pct=n/sum(n))
```

    ## # A tibble: 6 x 4
    ## # Groups:   Department [3]
    ##   Department             Attrition     n   pct
    ##   <chr>                  <chr>     <int> <dbl>
    ## 1 Human Resources        No           37 0.755
    ## 2 Human Resources        Yes          12 0.245
    ## 3 Research & Development No          721 0.867
    ## 4 Research & Development Yes         111 0.133
    ## 5 Sales                  No          291 0.789
    ## 6 Sales                  Yes          78 0.211

We observe that there may be something going on in department since
there are different percentages of attrition within different
departments.

**Attrition by Department & Job Role**

``` r
dept_job_role_tbl %>% 
  
  group_by(Department,JobRole, Attrition) %>% 
  summarize(n=n()) %>% 
  ungroup() %>% 
  
  group_by(Department,JobRole) %>%
  mutate(pct=n/sum(n)) %>%
  ungroup() %>%
  
  filter(Attrition %in% "Yes" )
```

    ## # A tibble: 10 x 5
    ##    Department             JobRole                   Attrition     n    pct
    ##    <chr>                  <chr>                     <chr>     <int>  <dbl>
    ##  1 Human Resources        Human Resources           Yes          12 0.308 
    ##  2 Research & Development Healthcare Representative Yes           8 0.0762
    ##  3 Research & Development Laboratory Technician     Yes          49 0.219 
    ##  4 Research & Development Manager                   Yes           2 0.0417
    ##  5 Research & Development Manufacturing Director    Yes           7 0.0569
    ##  6 Research & Development Research Director         Yes           2 0.0274
    ##  7 Research & Development Research Scientist        Yes          43 0.166 
    ##  8 Sales                  Manager                   Yes           2 0.0645
    ##  9 Sales                  Sales Executive           Yes          50 0.183 
    ## 10 Sales                  Sales Representative      Yes          26 0.4

We observe high counts and high percentages by Job Role and Departments,
for ex: high counts in sales department

**Step 3: Hypothesize drivers**

In this step, we should work with either subject matter expert or
stakeholders to decide what drives the business. In this case, we have
**Job Role** and **Department**

#### Measure the Drivers

**Step 1: Collect information on employee attrition : Ongoing**

In this case, we should ask following:

-   Is data available ?  
-   Treat information as assets  
-   Build Strategic Databases

**Step 2: Develop KPI’s**

-   In this step, we can either use **benchmark industry metrics** if
    available or  
-   Develop **internal metrics** using goals
-   **Industry KPI: 8.8% attrition rate**  
-   We will consider this as a conservative KPI that indicates a major
    problem if exceeded

#### Uncover Problems and opportunities

**Step 1: Evaluate performance vs KPIs**

**Attrition Cost** : Cost incurred when an employee leaves

Calculating **Attrition cost** which can be broken down into following:

-   Direct Costs  
-   Lost Productivity Costs  
-   Savings of salary and Benefits (Cost Reduction)

Cost per employee = Direct Costs + Lost Productivity Costs + Savings of
salary and Benefits

Total Attrition Cost = No. of employees \* Cost per employee

``` r
calculate_attrition_cost<- function (
  #Employee  
  n                  = 1,
  salary             = 80000,
  
  
  #Direct Costs
  separation_cost          = 500,
  vacancy_cost             = 10000,
  acquisition_cost         = 4900,
  placement_cost           = 3500, 
  
  
  #Productivity Costs
  net_revenue_per_employee = 250000, 
  workdays_per_year        = 240,
  workdays_position_open   = 40,
  workdays_onboarding      = 60, 
  onboarding_efficiency    = 0.50
  
){
  
  #Direct Costs
  direct_cost <- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)
  
  #Lost Productivity Costs
  productivity_cost <- net_revenue_per_employee /workdays_per_year *
    (workdays_position_open + workdays_onboarding * onboarding_efficiency)
  
  # Savings of salary and Benefits (Cost Reduction)
  salary_benefit_reduction <- salary / workdays_per_year * workdays_position_open
  
  #Estimated Turnover per Employee 
  cost_per_employee <- direct_cost + productivity_cost - salary_benefit_reduction
  
  # Total Cost of Employee Turnover 
  total_cost <- n * cost_per_employee
  
  return(total_cost)
}
```

``` r
calculate_attrition_cost()
```

    ## [1] 78483.33

**Attriton Cost per person is : 78483.33**

**Attriton Cost for 200 employees is : 15,696,667 ~ 15.7 million**

This implies that it is a **$15.7 million/year** problem for an
organisation that loses 200 employees per year

**Workflow of Attrition**

``` r
count_to_pct <-  function(data,...,col=n){
  
  grouping_vars_expr <- quos(...) 
  col_expr <- enquo(col)  
  
  ret <- data %>%
    group_by(!!! grouping_vars_expr) %>%
    mutate(pct = (!! col_expr) /sum(!! col_expr)) %>%
    ungroup()
  
  return(ret)
}

assess_attrition <- function(data, attrition_col, attrition_value, baseline_pct) {
      
      attrition_col_expr <- enquo(attrition_col)
        
      data %>%
            filter((!! attrition_col_expr) %in% attrition_value ) %>%
            arrange(desc(pct)) %>%
            mutate (
                above_industry_avg =  case_when(
                pct> baseline_pct ~ "Yes" ,
                TRUE ~ "No"
            )
          )
            
}
```

**Step 2: Highlight potential problem areas**

Calculating Attrition cost by **Department** & **JobRole**

``` r
dept_job_role_tbl %>% 
  
  count(Department, JobRole, Attrition) %>% 
  
  count_to_pct(Department, JobRole)%>%
  
  assess_attrition(Attrition, attrition_value = "Yes", baseline_pct = 0.088) %>% 
  
  mutate(cost_of_attrition= calculate_attrition_cost(n=n,80000))
```

    ## # A tibble: 10 x 7
    ##    Department             JobRole                   Attrition     n    pct
    ##    <chr>                  <chr>                     <chr>     <int>  <dbl>
    ##  1 Sales                  Sales Representative      Yes          26 0.4   
    ##  2 Human Resources        Human Resources           Yes          12 0.308 
    ##  3 Research & Development Laboratory Technician     Yes          49 0.219 
    ##  4 Sales                  Sales Executive           Yes          50 0.183 
    ##  5 Research & Development Research Scientist        Yes          43 0.166 
    ##  6 Research & Development Healthcare Representative Yes           8 0.0762
    ##  7 Sales                  Manager                   Yes           2 0.0645
    ##  8 Research & Development Manufacturing Director    Yes           7 0.0569
    ##  9 Research & Development Manager                   Yes           2 0.0417
    ## 10 Research & Development Research Director         Yes           2 0.0274
    ##    above_industry_avg cost_of_attrition
    ##    <chr>                          <dbl>
    ##  1 Yes                         2040567.
    ##  2 Yes                          941800.
    ##  3 Yes                         3845683.
    ##  4 Yes                         3924167.
    ##  5 Yes                         3374783.
    ##  6 No                           627867.
    ##  7 No                           156967.
    ##  8 No                           549383.
    ##  9 No                           156967.
    ## 10 No                           156967.

We can observe that **Sales Representative** (~2 million) and
**Laboratory Technician** (~3.8 million) have higher cost of attrition
within Sales and Research & Development department even with lower
attrition rate.

10% reduction in turnover saves the organization **$1.6 million /
year**. So, this problem is worth solving for an organization.

**Step 3: Review process and consider what could be missed or needed to
answer questions**

**Visualizing of Attrition cost by JobRole**

``` r
plot_attrition <- function(data,...,.value,
                           fct_reorder = TRUE,
                           fct_rev = FALSE,
                           include_lbl = TRUE,
                           color = palette_light()[[1]],
                           units = c("0", "K", "M")) {
  
  
  #Inputs
  group_vars_expr <- quos(...)
  if(length(group_vars_expr)==0)
    group_vars_expr <- quos(rlang::sym(colnames(data)[[1]]))
  
  value_expr <- enquo(.value)
  value_name <- quo_name(value_expr)
  
  units_val <- switch(units[[1]],
                      "M" = 1e6,
                      "K" = 1e3,
                      "0" = 1) 
  if(units[[1]] == "0") units <- ""
  
  #Data Manipulation
  usd <- scales::dollar_format(prefix = "$" , largest_with_cents = 1e3)
  
  data_manipulated <- data %>%
    mutate(name = str_c(!!! group_vars_expr, sep = ": ") %>% as_factor()) %>%
    mutate(value_text = str_c(usd(!! value_expr / units_val ), units[[1]] , sep = ""))
  
  if(fct_reorder){
    data_manipulated <-  data_manipulated %>% 
      mutate(name = forcats::fct_reorder(name, !! value_expr)) %>% 
      arrange(name)
  }
    
  if(fct_rev){
    data_manipulated <-  data_manipulated %>% 
      mutate(name = forcats::fct_rev(name)) %>% 
      arrange(name)
  }
  
  
  #Visualization
  
  g <- data_manipulated  %>%
    ggplot(aes_string(x = value_name, y = "name")) +
    geom_segment(aes(xend = 0, yend = name), color = color) +
    geom_point(aes_string(size = value_name), color = color) +
    scale_x_continuous(labels = scales::dollar) +
    theme_tq() +
    scale_size(range = c(3, 5)) +
    theme(legend.position = "none")
  
  
  if(include_lbl){
    g <- g + 
    geom_label(
      aes_string(label = "value_text", size = value_name),
      hjust = "inward",
      color = color
    )
  } 
  
  return(g)

}
```

It looks like **Sales Executive** and **Laboratory Technician** are
biggest sales driver of cost

``` r
dept_job_role_tbl %>% 
  
  count(JobRole, Attrition) %>% 
  count_to_pct(JobRole)%>%
  assess_attrition(Attrition, attrition_value = "Yes", baseline_pct = 0.088) %>% 
  mutate(cost_of_attrition= calculate_attrition_cost(n=n,80000)
  ) %>% 
  
  plot_attrition(JobRole, .value = cost_of_attrition, units= "M") +
  labs(title = "Estimated Cost of Attrition By Job Role",
       x =  "Cost of Attrition" ,
       y="",
       subtitle = "Looks like Sales Executive and Laboratory Technician are biggest sales driver of cost")
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-12-1.png)

### Phase 2 : Data Understanding

**Loading datasets**

``` r
#Load Data
path_train <- "00_Data/telco_train.xlsx"
path_test <- "00_Data/telco_test.xlsx"

path_data_definitions <- "00_Data/telco_data_definitions.xlsx"

train_raw_tbl <- read_excel(path_train, sheet=1)
test_raw_tbl <- read_excel(path_test, sheet=1)

definitions_raw_tbl <- read_excel(path_data_definitions, sheet = 1, col_names = FALSE)
```

**Reading data definitions table**

``` r
definitions_raw_tbl 
```

    ## # A tibble: 35 x 2
    ##    X__1                    X__2             
    ##    <chr>                   <chr>            
    ##  1 Education               1 'Below College'
    ##  2 <NA>                    2 'College'      
    ##  3 <NA>                    3 'Bachelor'     
    ##  4 <NA>                    4 'Master'       
    ##  5 <NA>                    5 'Doctor'       
    ##  6 <NA>                    <NA>             
    ##  7 EnvironmentSatisfaction 1 'Low'          
    ##  8 <NA>                    2 'Medium'       
    ##  9 <NA>                    3 'High'         
    ## 10 <NA>                    4 'Very High'    
    ## # … with 25 more rows

**Reading training dataset**

``` r
train_raw_tbl %>% glimpse()
```

    ## Observations: 1,250
    ## Variables: 35
    ## $ Age                      <dbl> 41, 49, 37, 33, 27, 32, 59, 30, 38, 36,…
    ## $ Attrition                <chr> "Yes", "No", "Yes", "No", "No", "No", "…
    ## $ BusinessTravel           <chr> "Travel_Rarely", "Travel_Frequently", "…
    ## $ DailyRate                <dbl> 1102, 279, 1373, 1392, 591, 1005, 1324,…
    ## $ Department               <chr> "Sales", "Research & Development", "Res…
    ## $ DistanceFromHome         <dbl> 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15…
    ## $ Education                <dbl> 2, 1, 2, 4, 1, 2, 3, 1, 3, 3, 3, 2, 1, …
    ## $ EducationField           <chr> "Life Sciences", "Life Sciences", "Othe…
    ## $ EmployeeCount            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
    ## $ EmployeeNumber           <dbl> 1, 2, 4, 5, 7, 8, 10, 11, 12, 13, 14, 1…
    ## $ EnvironmentSatisfaction  <dbl> 2, 3, 4, 4, 1, 4, 3, 4, 4, 3, 1, 4, 1, …
    ## $ Gender                   <chr> "Female", "Male", "Male", "Female", "Ma…
    ## $ HourlyRate               <dbl> 94, 61, 92, 56, 40, 79, 81, 67, 44, 94,…
    ## $ JobInvolvement           <dbl> 3, 2, 2, 3, 3, 3, 4, 3, 2, 3, 4, 2, 3, …
    ## $ JobLevel                 <dbl> 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, …
    ## $ JobRole                  <chr> "Sales Executive", "Research Scientist"…
    ## $ JobSatisfaction          <dbl> 4, 2, 3, 3, 2, 4, 1, 3, 3, 3, 2, 3, 3, …
    ## $ MaritalStatus            <chr> "Single", "Married", "Single", "Married…
    ## $ MonthlyIncome            <dbl> 5993, 5130, 2090, 2909, 3468, 3068, 267…
    ## $ MonthlyRate              <dbl> 19479, 24907, 2396, 23159, 16632, 11864…
    ## $ NumCompaniesWorked       <dbl> 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, …
    ## $ Over18                   <chr> "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y",…
    ## $ OverTime                 <chr> "Yes", "No", "Yes", "Yes", "No", "No", …
    ## $ PercentSalaryHike        <dbl> 11, 23, 15, 11, 12, 13, 20, 22, 21, 13,…
    ## $ PerformanceRating        <dbl> 3, 4, 3, 3, 3, 3, 4, 4, 4, 3, 3, 3, 3, …
    ## $ RelationshipSatisfaction <dbl> 1, 4, 2, 3, 4, 3, 1, 2, 2, 2, 3, 4, 4, …
    ## $ StandardHours            <dbl> 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,…
    ## $ StockOptionLevel         <dbl> 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, …
    ## $ TotalWorkingYears        <dbl> 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10…
    ## $ TrainingTimesLastYear    <dbl> 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, …
    ## $ WorkLifeBalance          <dbl> 1, 3, 3, 3, 3, 2, 2, 3, 3, 2, 3, 3, 2, …
    ## $ YearsAtCompany           <dbl> 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5,…
    ## $ YearsInCurrentRole       <dbl> 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, …
    ## $ YearsSinceLastPromotion  <dbl> 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, …
    ## $ YearsWithCurrManager     <dbl> 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, …

**Reading test dataset**

``` r
test_raw_tbl  %>% glimpse()
```

    ## Observations: 220
    ## Variables: 35
    ## $ Age                      <dbl> 30, 55, 54, 49, 43, 58, 34, 37, 35, 50,…
    ## $ Attrition                <chr> "No", "No", "No", "No", "Yes", "No", "N…
    ## $ BusinessTravel           <chr> "Travel_Rarely", "Non-Travel", "Travel_…
    ## $ DailyRate                <dbl> 1339, 177, 685, 1023, 807, 848, 1346, 1…
    ## $ Department               <chr> "Sales", "Research & Development", "Res…
    ## $ DistanceFromHome         <dbl> 5, 8, 3, 2, 17, 23, 19, 5, 1, 1, 1, 10,…
    ## $ Education                <dbl> 3, 1, 3, 3, 3, 4, 2, 2, 3, 3, 4, 4, 3, …
    ## $ EducationField           <chr> "Life Sciences", "Medical", "Life Scien…
    ## $ EmployeeCount            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
    ## $ EmployeeNumber           <dbl> 228, 1278, 1250, 2065, 1767, 1308, 18, …
    ## $ EnvironmentSatisfaction  <dbl> 2, 4, 4, 4, 3, 1, 2, 4, 4, 4, 2, 4, 3, …
    ## $ Gender                   <chr> "Female", "Male", "Male", "Male", "Male…
    ## $ HourlyRate               <dbl> 41, 37, 85, 63, 38, 88, 93, 61, 60, 95,…
    ## $ JobInvolvement           <dbl> 3, 2, 3, 2, 2, 3, 3, 3, 2, 3, 1, 2, 3, …
    ## $ JobLevel                 <dbl> 3, 4, 4, 2, 1, 1, 1, 2, 2, 2, 1, 2, 2, …
    ## $ JobRole                  <chr> "Sales Executive", "Healthcare Represen…
    ## $ JobSatisfaction          <dbl> 4, 2, 4, 2, 3, 3, 4, 4, 4, 3, 3, 4, 3, …
    ## $ MaritalStatus            <chr> "Married", "Divorced", "Married", "Marr…
    ## $ MonthlyIncome            <dbl> 9419, 13577, 17779, 5390, 2437, 2372, 2…
    ## $ MonthlyRate              <dbl> 8053, 25592, 23474, 13243, 15587, 26076…
    ## $ NumCompaniesWorked       <dbl> 2, 1, 3, 2, 9, 1, 0, 7, 0, 0, 1, 2, 2, …
    ## $ Over18                   <chr> "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y",…
    ## $ OverTime                 <chr> "No", "Yes", "No", "No", "Yes", "No", "…
    ## $ PercentSalaryHike        <dbl> 12, 15, 14, 14, 16, 12, 11, 16, 12, 12,…
    ## $ PerformanceRating        <dbl> 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, …
    ## $ RelationshipSatisfaction <dbl> 3, 4, 1, 4, 4, 4, 3, 3, 2, 1, 2, 3, 3, …
    ## $ StandardHours            <dbl> 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,…
    ## $ StockOptionLevel         <dbl> 1, 1, 0, 0, 1, 2, 1, 2, 1, 1, 0, 3, 1, …
    ## $ TotalWorkingYears        <dbl> 12, 34, 36, 17, 6, 2, 3, 8, 10, 19, 1, …
    ## $ TrainingTimesLastYear    <dbl> 2, 3, 2, 3, 4, 3, 2, 2, 0, 3, 3, 5, 5, …
    ## $ WorkLifeBalance          <dbl> 3, 3, 3, 2, 3, 3, 3, 2, 3, 3, 3, 3, 2, …
    ## $ YearsAtCompany           <dbl> 10, 33, 10, 9, 1, 2, 2, 6, 9, 18, 1, 1,…
    ## $ YearsInCurrentRole       <dbl> 9, 9, 9, 6, 0, 2, 2, 2, 7, 7, 0, 0, 7, …
    ## $ YearsSinceLastPromotion  <dbl> 7, 15, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0, 0,…
    ## $ YearsWithCurrManager     <dbl> 4, 0, 9, 8, 0, 2, 2, 4, 0, 13, 0, 0, 8,…

### Exploratory Data Analysis

#### Step 1 : Data Summarisation

**Summarising data with skim**

It allows you to skim useful summary statistics in console or use those
statitics in a pipeable workflow

``` r
skim(train_raw_tbl)
```

    ## Skim summary statistics
    ##  n obs: 1250 
    ##  n variables: 35 
    ## 
    ## ── Variable type:character ───────────────────────────────────────────────────────────────────────────────────────────
    ##        variable missing complete    n min max empty n_unique
    ##       Attrition       0     1250 1250   2   3     0        2
    ##  BusinessTravel       0     1250 1250  10  17     0        3
    ##      Department       0     1250 1250   5  22     0        3
    ##  EducationField       0     1250 1250   5  16     0        6
    ##          Gender       0     1250 1250   4   6     0        2
    ##         JobRole       0     1250 1250   7  25     0        9
    ##   MaritalStatus       0     1250 1250   6   8     0        3
    ##          Over18       0     1250 1250   1   1     0        1
    ##        OverTime       0     1250 1250   2   3     0        2
    ## 
    ## ── Variable type:numeric ─────────────────────────────────────────────────────────────────────────────────────────────
    ##                  variable missing complete    n     mean      sd   p0
    ##                       Age       0     1250 1250    36.95    9.08   18
    ##                 DailyRate       0     1250 1250   799.74  403.52  102
    ##          DistanceFromHome       0     1250 1250     9.1     8.1     1
    ##                 Education       0     1250 1250     2.91    1.03    1
    ##             EmployeeCount       0     1250 1250     1       0       1
    ##            EmployeeNumber       0     1250 1250  1022.71  600.42    1
    ##   EnvironmentSatisfaction       0     1250 1250     2.71    1.09    1
    ##                HourlyRate       0     1250 1250    65.82   20.14   30
    ##            JobInvolvement       0     1250 1250     2.75    0.71    1
    ##                  JobLevel       0     1250 1250     2.08    1.12    1
    ##           JobSatisfaction       0     1250 1250     2.73    1.11    1
    ##             MonthlyIncome       0     1250 1250  6569.4  4777.89 1009
    ##               MonthlyRate       0     1250 1250 14294.5  7071.61 2097
    ##        NumCompaniesWorked       0     1250 1250     2.71    2.49    0
    ##         PercentSalaryHike       0     1250 1250    15.23    3.69   11
    ##         PerformanceRating       0     1250 1250     3.16    0.36    3
    ##  RelationshipSatisfaction       0     1250 1250     2.71    1.09    1
    ##             StandardHours       0     1250 1250    80       0      80
    ##          StockOptionLevel       0     1250 1250     0.8     0.86    0
    ##         TotalWorkingYears       0     1250 1250    11.4     7.79    0
    ##     TrainingTimesLastYear       0     1250 1250     2.81    1.29    0
    ##           WorkLifeBalance       0     1250 1250     2.76    0.71    1
    ##            YearsAtCompany       0     1250 1250     7.09    6.21    0
    ##        YearsInCurrentRole       0     1250 1250     4.26    3.66    0
    ##   YearsSinceLastPromotion       0     1250 1250     2.21    3.2     0
    ##      YearsWithCurrManager       0     1250 1250     4.16    3.59    0
    ##      p25     p50      p75  p100     hist
    ##    30       36      43       60 ▂▃▆▇▅▃▂▂
    ##   464      798    1156     1498 ▇▇▇▇▇▇▇▇
    ##     2        7      13.75    29 ▇▅▃▁▁▁▁▁
    ##     2        3       4        5 ▂▅▁▇▁▆▁▁
    ##     1        1       1        1 ▁▁▁▇▁▁▁▁
    ##   486.25  1020.5  1553.75  2068 ▇▇▇▇▇▇▇▇
    ##     2        3       4        4 ▅▁▅▁▁▇▁▇
    ##    48       66      83      100 ▆▇▇▇▆▇▇▇
    ##     2        3       3        4 ▁▁▃▁▁▇▁▂
    ##     1        2       3        5 ▇▇▁▃▁▂▁▁
    ##     2        3       4        4 ▅▁▅▁▁▇▁▇
    ##  2935.25  4903.5  8395    19999 ▇▇▃▂▁▁▁▂
    ##  8191.25 14328   20337.25 26999 ▇▇▇▇▇▇▇▆
    ##     1        2       4        9 ▇▂▂▂▁▁▁▁
    ##    12       14      18       25 ▇▇▃▃▂▂▂▁
    ##     3        3       3        4 ▇▁▁▁▁▁▁▂
    ##     2        3       4        4 ▅▁▆▁▁▇▁▇
    ##    80       80      80       80 ▁▁▁▇▁▁▁▁
    ##     0        1       1        3 ▇▁▇▁▁▂▁▁
    ##     6       10      15       40 ▃▇▂▂▂▁▁▁
    ##     2        3       3        6 ▁▁▇▇▁▂▂▁
    ##     2        3       3        4 ▁▁▃▁▁▇▁▂
    ##     3        5      10       40 ▇▅▁▁▁▁▁▁
    ##     2        3       7       18 ▇▃▁▅▁▁▁▁
    ##     0        1       3       15 ▇▂▁▁▁▁▁▁
    ##     2        3       7       17 ▇▃▁▃▁▁▁▁

Separating your data by data type is a great way to investigate
properties of data :

**1. Character / Categorical**  
**2. Numeric**

**Exploring features which are of Character Data Type**

``` r
# Character Data Type
train_raw_tbl %>% 
  select_if(is.character) %>% 
  glimpse()
```

    ## Observations: 1,250
    ## Variables: 9
    ## $ Attrition      <chr> "Yes", "No", "Yes", "No", "No", "No", "No", "No",…
    ## $ BusinessTravel <chr> "Travel_Rarely", "Travel_Frequently", "Travel_Rar…
    ## $ Department     <chr> "Sales", "Research & Development", "Research & De…
    ## $ EducationField <chr> "Life Sciences", "Life Sciences", "Other", "Life …
    ## $ Gender         <chr> "Female", "Male", "Male", "Female", "Male", "Male…
    ## $ JobRole        <chr> "Sales Executive", "Research Scientist", "Laborat…
    ## $ MaritalStatus  <chr> "Single", "Married", "Single", "Married", "Marrie…
    ## $ Over18         <chr> "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y", "Y",…
    ## $ OverTime       <chr> "Yes", "No", "Yes", "Yes", "No", "No", "Yes", "No…

Unique values in each character type feature and proportion of values in
it

``` r
train_raw_tbl %>% 
  select_if(is.character) %>% 
  map(~ table(.) %>% prop.table())
```

    ## $Attrition
    ## .
    ##     No    Yes 
    ## 0.8392 0.1608 
    ## 
    ## $BusinessTravel
    ## .
    ##        Non-Travel Travel_Frequently     Travel_Rarely 
    ##            0.0992            0.1912            0.7096 
    ## 
    ## $Department
    ## .
    ##        Human Resources Research & Development                  Sales 
    ##                 0.0392                 0.6656                 0.2952 
    ## 
    ## $EducationField
    ## .
    ##  Human Resources    Life Sciences        Marketing          Medical 
    ##           0.0184           0.4160           0.1080           0.3136 
    ##            Other Technical Degree 
    ##           0.0528           0.0912 
    ## 
    ## $Gender
    ## .
    ## Female   Male 
    ## 0.3928 0.6072 
    ## 
    ## $JobRole
    ## .
    ## Healthcare Representative           Human Resources 
    ##                    0.0840                    0.0312 
    ##     Laboratory Technician                   Manager 
    ##                    0.1792                    0.0712 
    ##    Manufacturing Director         Research Director 
    ##                    0.0984                    0.0584 
    ##        Research Scientist           Sales Executive 
    ##                    0.2072                    0.2184 
    ##      Sales Representative 
    ##                    0.0520 
    ## 
    ## $MaritalStatus
    ## .
    ## Divorced  Married   Single 
    ##   0.2200   0.4608   0.3192 
    ## 
    ## $Over18
    ## .
    ## Y 
    ## 1 
    ## 
    ## $OverTime
    ## .
    ##     No    Yes 
    ## 0.7152 0.2848

**Exploring features which are of Numeric Data Type**

``` r
# Numeric Data Type

train_raw_tbl %>% 
  select_if(is.numeric) %>% 
  map(~ unique(.) %>% length())
```

    ## $Age
    ## [1] 43
    ## 
    ## $DailyRate
    ## [1] 808
    ## 
    ## $DistanceFromHome
    ## [1] 29
    ## 
    ## $Education
    ## [1] 5
    ## 
    ## $EmployeeCount
    ## [1] 1
    ## 
    ## $EmployeeNumber
    ## [1] 1250
    ## 
    ## $EnvironmentSatisfaction
    ## [1] 4
    ## 
    ## $HourlyRate
    ## [1] 71
    ## 
    ## $JobInvolvement
    ## [1] 4
    ## 
    ## $JobLevel
    ## [1] 5
    ## 
    ## $JobSatisfaction
    ## [1] 4
    ## 
    ## $MonthlyIncome
    ## [1] 1161
    ## 
    ## $MonthlyRate
    ## [1] 1217
    ## 
    ## $NumCompaniesWorked
    ## [1] 10
    ## 
    ## $PercentSalaryHike
    ## [1] 15
    ## 
    ## $PerformanceRating
    ## [1] 2
    ## 
    ## $RelationshipSatisfaction
    ## [1] 4
    ## 
    ## $StandardHours
    ## [1] 1
    ## 
    ## $StockOptionLevel
    ## [1] 4
    ## 
    ## $TotalWorkingYears
    ## [1] 40
    ## 
    ## $TrainingTimesLastYear
    ## [1] 7
    ## 
    ## $WorkLifeBalance
    ## [1] 4
    ## 
    ## $YearsAtCompany
    ## [1] 37
    ## 
    ## $YearsInCurrentRole
    ## [1] 19
    ## 
    ## $YearsSinceLastPromotion
    ## [1] 16
    ## 
    ## $YearsWithCurrManager
    ## [1] 18

We should note that numeric varibales that are lower in levels are
likely to be discrete and that are higher in levels are likely to be
continuous, filtering features where levels are less than 10, those
features are likely to be factor.

``` r
train_raw_tbl %>% 
  select_if(is.numeric) %>% 
  map_df(~ unique(.) %>% length()) %>%
  gather() %>% 
  arrange(value) %>%
  filter(value <= 10)
```

    ## # A tibble: 13 x 2
    ##    key                      value
    ##    <chr>                    <int>
    ##  1 EmployeeCount                1
    ##  2 StandardHours                1
    ##  3 PerformanceRating            2
    ##  4 EnvironmentSatisfaction      4
    ##  5 JobInvolvement               4
    ##  6 JobSatisfaction              4
    ##  7 RelationshipSatisfaction     4
    ##  8 StockOptionLevel             4
    ##  9 WorkLifeBalance              4
    ## 10 Education                    5
    ## 11 JobLevel                     5
    ## 12 TrainingTimesLastYear        7
    ## 13 NumCompaniesWorked          10

#### Step 2 : Data Visualization

Visualising the feature-target interactions with
[GGally](http://ggobi.github.io/ggally/) package

``` r
plot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {
  
  color_expr <- enquo(color)
  
  if (rlang::quo_is_null(color_expr)) {
    
    g <- data %>%
      ggpairs(lower = "blank") 
    
  } else {
    
    color_name <- quo_name(color_expr)
    
    g <- data %>%
      ggpairs(mapping = aes_string(color = color_name), 
              lower = "blank", legend = 1,
              diag = list(continuous = wrap("densityDiag", 
                                            alpha = density_alpha))) +
      theme(legend.position = "bottom")
  }
  
  return(g)
  
}
```

**Exploring features Visually by Category**

Based on intuition we can divide the features into different categories,
which would helps us doing a better analysis.

**1. Descriptive Features : Age, Gender, MaritalStatus,
NumCompaniesWorked, Over18, DistanceFromHome**

``` r
train_raw_tbl %>% 
  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>% 
  plot_ggpairs(color = Attrition)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-23-1.png)

-   It looks like younger people are leaving the company more so than
    older people.  
-   It seems like people who are living between 20-30 miles away from
    company are leaving  
    as compared to people who are within 10 miles stay at company
    (skewed distribution).  
-   It looks like there is an upward trend of attrition (Yes) in marital
    status in Single, Married and Divorced while on the other side its
    small, large and medium.

**2. Employment Features : Department, JobRole , JobLevel,
JobInvolvement, JobSatisfaction**

``` r
train_raw_tbl %>% 
  select(Attrition, contains("employee"), contains("department"), contains("job") ) %>% 
  plot_ggpairs(color = Attrition)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-24-1.png)

-   Employee count won’t add any value to predictive power of model.  
-   Also, Employee number won’t help but it would help in identifying a
    particular employee.  
-   If you look at Department wise R & D has higher attrition, then
    Sales and then HR.  
-   On the contrary, density for JobInvolvement type 3 has a higher
    spike for people who are staying with relation to people who are
    leaving or conversely JobInvolvement type 3 has less population as
    compared to lower levels. So, people in 1 and 2 type are more likely
    to leave as compared to type 3 & 4.  
-   There are lot more people leaving in Joblevel 1 and lot more people
    are staying who are in Joblevel 2nd
-   We observe that certain JobRoles has higher attrition rate.  
-   It looks like people who are at Jobsatisfaction level 1 are more
    likely to leave than others.

**3. Compensation Features : DailyRate , HourlyRate, MonthlyRate,
MonthlyIncome, PercentSalaryHike, StockOptionLevel**

``` r
train_raw_tbl %>% 
  select(Attrition, contains("income"), contains("rate"), contains("salary"), contains("stock") ) %>% 
  plot_ggpairs(color = Attrition)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-25-1.png)

-   Persons who have less MonthlyIncome are more likely to leave.  
-   Persons who have less DailyRate are more likely to leave.  
-   It looks like people with less percent salary hike are more likely
    to leave.  
-   We observe that people with StockOptionLevel 0 i.e no stocks are
    more likely to leave than others.

**4. Survey Results : EnvironmentSatisfaction, JobSatisfaction ,
RelationshipSatisfaction, WorkLifeBalance**

``` r
train_raw_tbl %>% 
  select(Attrition, contains("satisfaction"), contains("life") ) %>% 
  plot_ggpairs(color = Attrition)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-26-1.png)

-   It looks like density of environment satisfaction 1 has higher spike
    for attrition i.e those people are more likely to leave.  
-   People with less JobSatisfaction are more likely to leave.  
-   People in RelationshipSatisfaction level 1 are more likely to leave
    than the others.  
-   It seems like density for WorkLifeBalance type 2 & 3 has a higher
    spike for people who are staying with relation to people who are
    leaving or conversely WorkLifeBalance type 1 & 4 has less population
    as compared to lower levels. So, people in 1 and 4 type are more
    likely to leave as compared to type 2 & 3.

**5. Performance Data : JobInvolvement, PerformanceRating**

``` r
train_raw_tbl %>% 
  select(Attrition, contains("performance"), contains("involvement")) %>% 
  plot_ggpairs(color = Attrition)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-27-1.png)

-   People either get a performance rating of either 3 and 4.  
-   It looks like average performance rating at the company is 3.  
-   People are more likely to stay in both the cases or less people
    leave who have a rating of 4 (less population with rating 4).

**6. Work - Life Features : BusinessTravel , OverTime**

``` r
train_raw_tbl %>% 
  select(Attrition, contains("overtime"), contains("travel")) %>% 
  plot_ggpairs(color = Attrition)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-28-1.png)

-   People who work overtime are more likely to leave as compared to
    those who don’t.  
-   People who business travel frequently and rarely are more likely to
    leave.

**7. Training & Education : Education, EducationField ,
TrainingTimesLastYear**

``` r
train_raw_tbl %>% 
  select(Attrition, contains("training"), contains("education")) %>% 
  plot_ggpairs(color = Attrition)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-29-1.png)

-   People with TrainingTimesLastYear &gt; 0 are more likely to stay
    than others .Also, in TrainingTimesLastYear type 4 are more likely
    to leave.  
-   People who comes from LifeSciences, Medical, Technical Degree &
    Marketing are more likely to leave the company.  
-   People with Education level 4 and 5 are more likely to leave.

**8. Time - Based Features: TotalWorkingYears , YearsAtCompany ,
YearsInCurrentRole , YearsSinceLastPromotion, YearsWithCurrManager**

``` r
train_raw_tbl %>% 
  select(Attrition, contains("years")) %>% 
  plot_ggpairs(color = Attrition)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-30-1.png)

-   People with greater TotalWorkingYears are more likely to stay.  
-   People who have less YearsAtCompany are more likely to leave.  
-   It looks like people with YearsInCurrentRole has a skewed
    distribution although people with less YearsInCurrentRole are more
    likely to stay.  
-   People with more YearsSinceLastPromotion are more likely to leave.  
-   People whose YearsWithCurrManager are within 2 years are more likely
    to leave.

### Phase 3 : Data Preparation

### Data Preparation - Human Readable

Function **process\_hr\_data\_readable** fills up empty values in data
definition table and re-levels features like Business Travel and Marital
Status in correct order.

``` r
process_hr_data_readable <- function(data, definitions_tbl) {
  
definitions_list <- definitions_tbl %>%
    fill(X__1, .direction = "down") %>%
    filter(!is.na(X__2)) %>%
    separate(X__2,into = c("key", "value"),sep = " '",remove = TRUE) %>%
    rename(column_name = X__1) %>%
    mutate(key = as.numeric(key)) %>%
    mutate(value = value %>% str_replace(pattern = "'", replacement = "")) %>%
    split(.$column_name) %>%
    map( ~ select(.,-column_name)) %>%
    map( ~ mutate(., value = as_factor(value)))
  
  for (i in seq_along(definitions_list)) {
    list_name <- names(definitions_list)[i]
    colnames(definitions_list[[i]]) <-c(list_name, paste0(list_name, "_value"))
  }
  
  data_merged_tbl <- list(HR_Data = data) %>%
    append(definitions_list, after = 1) %>%
    reduce(left_join) %>%
    select(-one_of(names(definitions_list))) %>%
    set_names(str_replace_all(names(.), pattern = "_value", replacement = "")) %>%
    select(sort(names(.))) %>%
    mutate_if(is.character, as.factor) %>%
    mutate(
      BusinessTravel = BusinessTravel %>% fct_relevel("Non-Travel", "Travel_Rarely", "Travel_Frequently"),
      MaritalStatus  = MaritalStatus %>% fct_relevel("Single", "Married", "Divorced")
    )
  
  return(data_merged_tbl)
  
} 

process_hr_data_readable(train_raw_tbl,definitions_tbl = definitions_raw_tbl) %>%
  glimpse()
```

    ## Observations: 1,250
    ## Variables: 35
    ## $ Age                      <dbl> 41, 49, 37, 33, 27, 32, 59, 30, 38, 36,…
    ## $ Attrition                <fct> Yes, No, Yes, No, No, No, No, No, No, N…
    ## $ BusinessTravel           <fct> Travel_Rarely, Travel_Frequently, Trave…
    ## $ DailyRate                <dbl> 1102, 279, 1373, 1392, 591, 1005, 1324,…
    ## $ Department               <fct> Sales, Research & Development, Research…
    ## $ DistanceFromHome         <dbl> 1, 8, 2, 3, 2, 2, 3, 24, 23, 27, 16, 15…
    ## $ Education                <fct> College, Below College, College, Master…
    ## $ EducationField           <fct> Life Sciences, Life Sciences, Other, Li…
    ## $ EmployeeCount            <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …
    ## $ EmployeeNumber           <dbl> 1, 2, 4, 5, 7, 8, 10, 11, 12, 13, 14, 1…
    ## $ EnvironmentSatisfaction  <fct> Medium, High, Very High, Very High, Low…
    ## $ Gender                   <fct> Female, Male, Male, Female, Male, Male,…
    ## $ HourlyRate               <dbl> 94, 61, 92, 56, 40, 79, 81, 67, 44, 94,…
    ## $ JobInvolvement           <fct> High, Medium, Medium, High, High, High,…
    ## $ JobLevel                 <dbl> 2, 2, 1, 1, 1, 1, 1, 1, 3, 2, 1, 2, 1, …
    ## $ JobRole                  <fct> Sales Executive, Research Scientist, La…
    ## $ JobSatisfaction          <fct> Very High, Medium, High, High, Medium, …
    ## $ MaritalStatus            <fct> Single, Married, Single, Married, Marri…
    ## $ MonthlyIncome            <dbl> 5993, 5130, 2090, 2909, 3468, 3068, 267…
    ## $ MonthlyRate              <dbl> 19479, 24907, 2396, 23159, 16632, 11864…
    ## $ NumCompaniesWorked       <dbl> 8, 1, 6, 1, 9, 0, 4, 1, 0, 6, 0, 0, 1, …
    ## $ Over18                   <fct> Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, Y, …
    ## $ OverTime                 <fct> Yes, No, Yes, Yes, No, No, Yes, No, No,…
    ## $ PercentSalaryHike        <dbl> 11, 23, 15, 11, 12, 13, 20, 22, 21, 13,…
    ## $ PerformanceRating        <fct> Excellent, Outstanding, Excellent, Exce…
    ## $ RelationshipSatisfaction <fct> Low, Very High, Medium, High, Very High…
    ## $ StandardHours            <dbl> 80, 80, 80, 80, 80, 80, 80, 80, 80, 80,…
    ## $ StockOptionLevel         <dbl> 0, 1, 0, 0, 1, 0, 3, 1, 0, 2, 1, 0, 1, …
    ## $ TotalWorkingYears        <dbl> 8, 10, 7, 8, 6, 8, 12, 1, 10, 17, 6, 10…
    ## $ TrainingTimesLastYear    <dbl> 0, 3, 3, 3, 3, 2, 3, 2, 2, 3, 5, 3, 1, …
    ## $ WorkLifeBalance          <fct> Bad, Better, Better, Better, Better, Go…
    ## $ YearsAtCompany           <dbl> 6, 10, 0, 8, 2, 7, 1, 1, 9, 7, 5, 9, 5,…
    ## $ YearsInCurrentRole       <dbl> 4, 7, 0, 7, 2, 7, 0, 0, 7, 7, 4, 5, 2, …
    ## $ YearsSinceLastPromotion  <dbl> 0, 1, 0, 3, 2, 3, 0, 0, 1, 7, 0, 0, 4, …
    ## $ YearsWithCurrManager     <dbl> 5, 7, 0, 0, 2, 6, 0, 0, 8, 7, 3, 8, 3, …

### Data Preparation - Machine Readable With Recipes

**Recipes:**

-   Alternative Method for creating and pre-processing design matrices
    that can be used for modeling or visualization.  
-   Creates a template assigning roles to variables within data.
-   Formula sets the stage for subsequent pre-processing steps.  
-   Formula creates the outcomes and predictor roles, assigning each
    variable or feature to a role  
-   Features are transformed into a format that can be digested by ML
    algorithms.

**Faceted Histogram Plotting Function**

-   It would help us in inspecting feature distributions to check
    **Skewness**.
-   Identify transformations needed to get them properly formatted for
    **Correlation Analysis**.

``` r
train_readable_tbl <- process_hr_data_readable(train_raw_tbl, definitions_tbl = definitions_raw_tbl)
test_readable_tbl <- process_hr_data_readable(test_raw_tbl, definitions_tbl = definitions_raw_tbl)


plot_hist_facet <- function(data, bins = 10, ncol = 5, 
                            fct_reorder = FALSE, fct_rev = FALSE,
                            fill = palette_light()[[3]],
                            color = "white", scale = "free") {
  
  data_factored <- data %>%
    mutate_if(is.character, as.factor) %>%
    mutate_if(is.factor, as.numeric) %>%
    gather(key = key, value = value, factor_key = TRUE)
  
  if(fct_reorder) {
    data_factored <- data_factored %>%
      mutate(key = as.character(key) %>% as.factor())
  }
  
  if(fct_rev){
    data_factored <- data_factored %>%
      mutate(key = fct_rev(key))
  }
  
  g <- data_factored %>%
    ggplot(aes(x = value, group = key )) +
    geom_histogram(bins = bins, fill = fill , color = color) +
    facet_wrap(~ key, ncol = ncol, scale = scale) +
    theme_tq()

  
  return(g)
  
}


#Removing zero variance features for facet histogram 
zeroVar <- function(data, useNA = 'ifany') {
  out <- apply(data, 2, function(x) {length(table(x, useNA = useNA))})
  which(out==1)
}

zv <- names(zeroVar(train_raw_tbl))
```

``` r
train_raw_tbl %>%
  select(-zv) %>%
  select(Attrition, everything()) %>%
  plot_hist_facet(bins = 10, ncol = 5, fct_rev = F)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-33-1.png)

-   Features like YearsSinceLastPromotion, YearsAtCompany,
    MonthlyIncome, TotalWorkingYears, NumCompaniesWorked,
    DistanceFromHome, YearsInCurrentRole, PercentSalaryHike,
    YearsWithCurrManager are skewed features, we might want to inspect
    them for transformations.

-   Features like EmployeeCount, Over18, Standard Hours have same value
    i.e zero variance, so its a good idea to not consider these features
    in analysis.

-   Features like Gender, JobLevel, JobSatisfaction, StockOptionLevel
    are factors since histograms works with numeric features we
    converted them to numeric variables for this plot.

**Data Preprocessing with Recipes**

**Recipes are generally a 3-part process**

-   Create the instructions with recipes and steps  
-   Prepare the recipe  
-   Bake the new data

There are quite a few steps available, following guidelines might be
useful in creating a generic checklist for pre-processing data for ML :

-   Impute - Act of filling in missing values within features. Common
    methods include recency (tidyr::fill), similarity (knn impute)  
-   Individual transformation for skewness and other issues  
-   Discretize (if needed and if you have no other choice) i.e making
    continuous variable discrete. For ex, think of turning a variable
    like age into cohorts of less than 20 years, 20-30, 30-40 etc.  
-   Create dummy variable i.e turning categorical data into separate
    columns of zeros and ones. This is important for machine learning to
    detect patterns in unordered data.  
-   Data Normalization steps (centering, scaling, range) . Normalization
    is getting the data onto a consistent scale. Some machine learning
    algorithms (e.g PCA, KNN, Deep learning etc) depend on the data to
    all have same scale
-   Multivariate Transformation (e.g PCA, spatial sign etc…)

**Note**: Discretization can hurt correlations. It’s often best not to
discretize unless there is a specific need to do so

**1. Zero Variance Features**

-   Features that have no variance, and therefore lend nothing the
    predictive quality of model  
-   Remove attributes with a zero variance (i.e all the same value)

``` r
recipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors())

recipe_obj
```

    ## Data Recipe
    ## 
    ## Inputs:
    ## 
    ##       role #variables
    ##    outcome          1
    ##  predictor         34
    ## 
    ## Operations:
    ## 
    ## Zero variance filter on all_predictors()

``` r
recipe_obj %>%
  prep() %>%
  bake(new_data = train_readable_tbl)
```

    ## # A tibble: 1,250 x 32
    ##      Age Attrition BusinessTravel    DailyRate Department            
    ##    <dbl> <fct>     <fct>                 <dbl> <fct>                 
    ##  1    41 Yes       Travel_Rarely          1102 Sales                 
    ##  2    49 No        Travel_Frequently       279 Research & Development
    ##  3    37 Yes       Travel_Rarely          1373 Research & Development
    ##  4    33 No        Travel_Frequently      1392 Research & Development
    ##  5    27 No        Travel_Rarely           591 Research & Development
    ##  6    32 No        Travel_Frequently      1005 Research & Development
    ##  7    59 No        Travel_Rarely          1324 Research & Development
    ##  8    30 No        Travel_Rarely          1358 Research & Development
    ##  9    38 No        Travel_Frequently       216 Research & Development
    ## 10    36 No        Travel_Rarely          1299 Research & Development
    ##    DistanceFromHome Education     EducationField EmployeeNumber
    ##               <dbl> <fct>         <fct>                   <dbl>
    ##  1                1 College       Life Sciences               1
    ##  2                8 Below College Life Sciences               2
    ##  3                2 College       Other                       4
    ##  4                3 Master        Life Sciences               5
    ##  5                2 Below College Medical                     7
    ##  6                2 College       Life Sciences               8
    ##  7                3 Bachelor      Medical                    10
    ##  8               24 Below College Life Sciences              11
    ##  9               23 Bachelor      Life Sciences              12
    ## 10               27 Bachelor      Medical                    13
    ##    EnvironmentSatisfaction Gender HourlyRate JobInvolvement JobLevel
    ##    <fct>                   <fct>       <dbl> <fct>             <dbl>
    ##  1 Medium                  Female         94 High                  2
    ##  2 High                    Male           61 Medium                2
    ##  3 Very High               Male           92 Medium                1
    ##  4 Very High               Female         56 High                  1
    ##  5 Low                     Male           40 High                  1
    ##  6 Very High               Male           79 High                  1
    ##  7 High                    Female         81 Very High             1
    ##  8 Very High               Male           67 High                  1
    ##  9 Very High               Male           44 Medium                3
    ## 10 High                    Male           94 High                  2
    ##    JobRole                   JobSatisfaction MaritalStatus MonthlyIncome
    ##    <fct>                     <fct>           <fct>                 <dbl>
    ##  1 Sales Executive           Very High       Single                 5993
    ##  2 Research Scientist        Medium          Married                5130
    ##  3 Laboratory Technician     High            Single                 2090
    ##  4 Research Scientist        High            Married                2909
    ##  5 Laboratory Technician     Medium          Married                3468
    ##  6 Laboratory Technician     Very High       Single                 3068
    ##  7 Laboratory Technician     Low             Married                2670
    ##  8 Laboratory Technician     High            Divorced               2693
    ##  9 Manufacturing Director    High            Single                 9526
    ## 10 Healthcare Representative High            Married                5237
    ##    MonthlyRate NumCompaniesWorked OverTime PercentSalaryHike
    ##          <dbl>              <dbl> <fct>                <dbl>
    ##  1       19479                  8 Yes                     11
    ##  2       24907                  1 No                      23
    ##  3        2396                  6 Yes                     15
    ##  4       23159                  1 Yes                     11
    ##  5       16632                  9 No                      12
    ##  6       11864                  0 No                      13
    ##  7        9964                  4 Yes                     20
    ##  8       13335                  1 No                      22
    ##  9        8787                  0 No                      21
    ## 10       16577                  6 No                      13
    ##    PerformanceRating RelationshipSatisfaction StockOptionLevel
    ##    <fct>             <fct>                               <dbl>
    ##  1 Excellent         Low                                     0
    ##  2 Outstanding       Very High                               1
    ##  3 Excellent         Medium                                  0
    ##  4 Excellent         High                                    0
    ##  5 Excellent         Very High                               1
    ##  6 Excellent         High                                    0
    ##  7 Outstanding       Low                                     3
    ##  8 Outstanding       Medium                                  1
    ##  9 Outstanding       Medium                                  0
    ## 10 Excellent         Medium                                  2
    ##    TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany
    ##                <dbl>                 <dbl> <fct>                    <dbl>
    ##  1                 8                     0 Bad                          6
    ##  2                10                     3 Better                      10
    ##  3                 7                     3 Better                       0
    ##  4                 8                     3 Better                       8
    ##  5                 6                     3 Better                       2
    ##  6                 8                     2 Good                         7
    ##  7                12                     3 Good                         1
    ##  8                 1                     2 Better                       1
    ##  9                10                     2 Better                       9
    ## 10                17                     3 Good                         7
    ##    YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager
    ##                 <dbl>                   <dbl>                <dbl>
    ##  1                  4                       0                    5
    ##  2                  7                       1                    7
    ##  3                  0                       0                    0
    ##  4                  7                       3                    0
    ##  5                  2                       2                    2
    ##  6                  7                       3                    6
    ##  7                  0                       0                    0
    ##  8                  0                       0                    0
    ##  9                  7                       1                    8
    ## 10                  7                       7                    7
    ## # … with 1,240 more rows

**2. Data Transformations**

-   Changes the data to remove skewness (e.g log), stabilize variance
    (e.g Box Cox) or make stationary (e.g difference for time series)  
-   Computing the skewness of numeric features and filtering which are
    highly skewed

``` r
skewed_feature_names <- train_readable_tbl %>%
  select_if(is.numeric) %>%
  map_df(skewness) %>%
  gather(factor_key = TRUE) %>%
  arrange(desc(value)) %>%
  filter(value >= 0.8) %>%
  pull(key) %>%
  as.character()

# Printing skewed feature names 
skewed_feature_names
```

    ##  [1] "YearsSinceLastPromotion" "YearsAtCompany"         
    ##  [3] "MonthlyIncome"           "TotalWorkingYears"      
    ##  [5] "NumCompaniesWorked"      "JobLevel"               
    ##  [7] "DistanceFromHome"        "StockOptionLevel"       
    ##  [9] "YearsInCurrentRole"      "PercentSalaryHike"      
    ## [11] "YearsWithCurrManager"

Plotting skewed features to identify which needs to be converted into
factors

``` r
train_readable_tbl %>%
  select(skewed_feature_names) %>%
  plot_hist_facet()
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-36-1.png)

Based on plot we observe that **Joblevel** and **StockOptionLevel** are
not numeric. These features needs to be converted to factors and should
not be transformed

``` r
skewed_feature_names <- train_readable_tbl %>%
  select_if(is.numeric) %>%
  map_df(skewness) %>%
  gather(factor_key = TRUE) %>%
  arrange(desc(value)) %>%
  filter(value >= 0.8) %>%
  filter(!key  %in% c("JobLevel", "StockOptionLevel")) %>%
  pull(key) %>%
  as.character()

skewed_feature_names
```

    ## [1] "YearsSinceLastPromotion" "YearsAtCompany"         
    ## [3] "MonthlyIncome"           "TotalWorkingYears"      
    ## [5] "NumCompaniesWorked"      "DistanceFromHome"       
    ## [7] "YearsInCurrentRole"      "PercentSalaryHike"      
    ## [9] "YearsWithCurrManager"

**Yeo-Johnson Transform**

Applying a Yeo-Johnson transform, like a BoxCox, but values can be
negative

``` r
factor_names <-  c("JobLevel", "StockOptionLevel")

recipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_YeoJohnson(skewed_feature_names) %>%
  step_num2factor(factor_names)
```

**Faceted Histogram for Skewed features**

``` r
recipe_obj %>%
  prep() %>%
  bake(train_readable_tbl) %>%
  select(skewed_feature_names) %>%
  plot_hist_facet()
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-39-1.png)

**3. Data Center/Scaling**

Converting numeric data into different scales to be on same scale since
some algorithm requires feature scaling e.g Kmeans, Deep learning, PCA &
SVMs. Its hard to remember which algorithm needs it, it’s better to
center and scale. This typically don’t hurt the predictions.

**Note** : Always center before you scale

``` r
recipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_YeoJohnson(skewed_feature_names) %>%
  step_num2factor(factor_names) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric())

# Before preparing Recipes, mean is NULL
recipe_obj$steps[[4]] 
```

    ## $terms
    ## <list_of<quosure>>
    ## 
    ## [[1]]
    ## <quosure>
    ## expr: ^all_numeric()
    ## env:  0x1314efc10
    ## 
    ## 
    ## $role
    ## [1] NA
    ## 
    ## $trained
    ## [1] FALSE
    ## 
    ## $means
    ## NULL
    ## 
    ## $na_rm
    ## [1] TRUE
    ## 
    ## $skip
    ## [1] FALSE
    ## 
    ## $id
    ## [1] "center_oIgVN"
    ## 
    ## attr(,"class")
    ## [1] "step_center" "step"

``` r
# After preparing recipes, mean is not null 
prepared_recipe <- recipe_obj %>% prep()
prepared_recipe$steps[[4]]
```

    ## $terms
    ## <list_of<quosure>>
    ## 
    ## [[1]]
    ## <quosure>
    ## expr: ^all_numeric()
    ## env:  0x1314efc10
    ## 
    ## 
    ## $role
    ## [1] NA
    ## 
    ## $trained
    ## [1] TRUE
    ## 
    ## $means
    ##                     Age               DailyRate        DistanceFromHome 
    ##            3.695040e+01            7.997448e+02            1.981462e+00 
    ##          EmployeeNumber              HourlyRate           MonthlyIncome 
    ##            1.022712e+03            6.582400e+01            3.769523e+00 
    ##             MonthlyRate      NumCompaniesWorked       PercentSalaryHike 
    ##            1.429450e+04            1.042074e+00            7.045663e-01 
    ##       TotalWorkingYears   TrainingTimesLastYear          YearsAtCompany 
    ##            3.409948e+00            2.813600e+00            2.028353e+00 
    ##      YearsInCurrentRole YearsSinceLastPromotion    YearsWithCurrManager 
    ##            1.748426e+00            5.446626e-01            1.733001e+00 
    ## 
    ## $na_rm
    ## [1] TRUE
    ## 
    ## $skip
    ## [1] FALSE
    ## 
    ## $id
    ## [1] "center_oIgVN"
    ## 
    ## attr(,"class")
    ## [1] "step_center" "step"

**Plotting Features After Transformation**

We observe that features are no more skewed now

``` r
prepared_recipe %>%
  bake(new_data = train_readable_tbl) %>%
  select_if(is.numeric) %>%
  plot_hist_facet()
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-41-1.png)

**4. Dummy Variables**

Expanding categorical features into multiple columns of 0’s and 1’s.
This is important for machine learning algorithms to detect patterns in
unordered data

**Categorical feature before expanding**

We will be looking at histogram of feature which contains **JobRole** in
its name. We observe that it is just one feature before any
transformation

``` r
# Before expanding

recipe_obj %>%
  prep() %>%
  bake(new_data = train_readable_tbl) %>%
  select(contains("JobRole")) %>%
  plot_hist_facet()
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-42-1.png)

**Categorical feature after expanding**

-   We are using step\_dummy which will convert character/factors into
    one or more numeric terms
-   Generally, if factor has 3 levels, the feature is expanded into 2
    columns (1 less than the number of levels)
-   We will obtain the full set of dummy variables using `one_hot`
    argument

``` r
# After expanding  

dummied_recipe_obj  <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_YeoJohnson(skewed_feature_names) %>%
  step_num2factor(factor_names) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_dummy(all_nominal(), one_hot = TRUE)

dummied_recipe_obj %>%
  prep() %>%
  bake(new_data = train_readable_tbl) %>%
  select(contains("JobRole")) %>%
  plot_hist_facet(ncol = 3)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-43-1.png)

We notice that feature **JobRole** now has been expanded to following 9
features :

-   JobRole\_Healthcare.Representative  
-   JobRole\_Human.Resources  
-   JobRole\_Laboratory.Technician  
-   JobRole\_Manager  
-   JobRole\_Manufacturing.Director  
-   JobRole\_Research.Director  
-   JobRole\_Research.Scientist  
-   JobRole\_Sales.Executive  
-   JobRole\_Sales.Representative

**Final Recipe for Correlation Evaluation**

``` r
recipe_obj  <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_YeoJohnson(skewed_feature_names) %>%
  step_num2factor(factor_names) %>%
  step_center(all_numeric()) %>%
  step_scale(all_numeric()) %>%
  step_dummy(all_nominal(),one_hot = TRUE) %>%
  prep()

# Printing Recipe
recipe_obj
```

    ## Data Recipe
    ## 
    ## Inputs:
    ## 
    ##       role #variables
    ##    outcome          1
    ##  predictor         34
    ## 
    ## Training data contained 1250 data points and no missing data.
    ## 
    ## Operations:
    ## 
    ## Zero variance filter removed EmployeeCount, ... [trained]
    ## Yeo-Johnson transformation on 9 items [trained]
    ## Factor variables from JobLevel, StockOptionLevel [trained]
    ## Centering for Age, DailyRate, ... [trained]
    ## Scaling for Age, DailyRate, ... [trained]
    ## Dummy variables from BusinessTravel, Department, ... [trained]

**Training data after applying recipe**

``` r
train_tbl <- bake(recipe_obj, new_data = train_readable_tbl)
train_tbl %>% glimpse()
```

    ## Observations: 1,250
    ## Variables: 83
    ## $ Age                                <dbl> 0.445810757, 1.326511581, 0.0…
    ## $ DailyRate                          <dbl> 0.74904690, -1.29050643, 1.42…
    ## $ DistanceFromHome                   <dbl> -1.49654918, 0.26177716, -1.0…
    ## $ EmployeeNumber                     <dbl> -1.701657, -1.699992, -1.6966…
    ## $ HourlyRate                         <dbl> 1.39874138, -0.23947787, 1.29…
    ## $ MonthlyIncome                      <dbl> 0.28123820, 0.04824053, -1.47…
    ## $ MonthlyRate                        <dbl> 0.7331422, 1.5007180, -1.6825…
    ## $ NumCompaniesWorked                 <dbl> 1.62470821, -0.58603051, 1.26…
    ## $ PercentSalaryHike                  <dbl> -1.4812855, 1.6672467, 0.1960…
    ## $ TotalWorkingYears                  <dbl> -0.26400582, 0.03531548, -0.4…
    ## $ TrainingTimesLastYear              <dbl> -2.1773654, 0.1442497, 0.1442…
    ## $ YearsAtCompany                     <dbl> 0.12937107, 0.74947123, -2.24…
    ## $ YearsInCurrentRole                 <dbl> 0.20272226, 0.87516459, -1.59…
    ## $ YearsSinceLastPromotion            <dbl> -1.11029069, 0.07030875, -1.1…
    ## $ YearsWithCurrManager               <dbl> 0.47708525, 0.89609325, -1.54…
    ## $ BusinessTravel_Non.Travel          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ BusinessTravel_Travel_Rarely       <dbl> 1, 0, 1, 0, 1, 0, 1, 1, 0, 1,…
    ## $ BusinessTravel_Travel_Frequently   <dbl> 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,…
    ## $ Department_Human.Resources         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ Department_Research...Development  <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
    ## $ Department_Sales                   <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ Education_Below.College            <dbl> 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,…
    ## $ Education_College                  <dbl> 1, 0, 1, 0, 0, 1, 0, 0, 0, 0,…
    ## $ Education_Bachelor                 <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,…
    ## $ Education_Master                   <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,…
    ## $ Education_Doctor                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ EducationField_Human.Resources     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ EducationField_Life.Sciences       <dbl> 1, 1, 0, 1, 0, 1, 0, 1, 1, 0,…
    ## $ EducationField_Marketing           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ EducationField_Medical             <dbl> 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,…
    ## $ EducationField_Other               <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…
    ## $ EducationField_Technical.Degree    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ EnvironmentSatisfaction_Low        <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…
    ## $ EnvironmentSatisfaction_Medium     <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ EnvironmentSatisfaction_High       <dbl> 0, 1, 0, 0, 0, 0, 1, 0, 0, 1,…
    ## $ EnvironmentSatisfaction_Very.High  <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,…
    ## $ Gender_Female                      <dbl> 1, 0, 0, 1, 0, 0, 1, 0, 0, 0,…
    ## $ Gender_Male                        <dbl> 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,…
    ## $ JobInvolvement_Low                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobInvolvement_Medium              <dbl> 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,…
    ## $ JobInvolvement_High                <dbl> 1, 0, 0, 1, 1, 1, 0, 1, 0, 1,…
    ## $ JobInvolvement_Very.High           <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…
    ## $ JobLevel_X1                        <dbl> 0, 0, 1, 1, 1, 1, 1, 1, 0, 0,…
    ## $ JobLevel_X2                        <dbl> 1, 1, 0, 0, 0, 0, 0, 0, 0, 1,…
    ## $ JobLevel_X3                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…
    ## $ JobLevel_X4                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobLevel_X5                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobRole_Healthcare.Representative  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…
    ## $ JobRole_Human.Resources            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobRole_Laboratory.Technician      <dbl> 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,…
    ## $ JobRole_Manager                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobRole_Manufacturing.Director     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…
    ## $ JobRole_Research.Director          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobRole_Research.Scientist         <dbl> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,…
    ## $ JobRole_Sales.Executive            <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobRole_Sales.Representative       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobSatisfaction_Low                <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…
    ## $ JobSatisfaction_Medium             <dbl> 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,…
    ## $ JobSatisfaction_High               <dbl> 0, 0, 1, 1, 0, 0, 0, 1, 1, 1,…
    ## $ JobSatisfaction_Very.High          <dbl> 1, 0, 0, 0, 0, 1, 0, 0, 0, 0,…
    ## $ MaritalStatus_Single               <dbl> 1, 0, 1, 0, 0, 1, 0, 0, 1, 0,…
    ## $ MaritalStatus_Married              <dbl> 0, 1, 0, 1, 1, 0, 1, 0, 0, 1,…
    ## $ MaritalStatus_Divorced             <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…
    ## $ OverTime_No                        <dbl> 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,…
    ## $ OverTime_Yes                       <dbl> 1, 0, 1, 1, 0, 0, 1, 0, 0, 0,…
    ## $ PerformanceRating_Low              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ PerformanceRating_Good             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ PerformanceRating_Excellent        <dbl> 1, 0, 1, 1, 1, 1, 0, 0, 0, 1,…
    ## $ PerformanceRating_Outstanding      <dbl> 0, 1, 0, 0, 0, 0, 1, 1, 1, 0,…
    ## $ RelationshipSatisfaction_Low       <dbl> 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,…
    ## $ RelationshipSatisfaction_Medium    <dbl> 0, 0, 1, 0, 0, 0, 0, 1, 1, 1,…
    ## $ RelationshipSatisfaction_High      <dbl> 0, 0, 0, 1, 0, 1, 0, 0, 0, 0,…
    ## $ RelationshipSatisfaction_Very.High <dbl> 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,…
    ## $ StockOptionLevel_X0                <dbl> 1, 0, 1, 1, 0, 1, 0, 0, 1, 0,…
    ## $ StockOptionLevel_X1                <dbl> 0, 1, 0, 0, 1, 0, 0, 1, 0, 0,…
    ## $ StockOptionLevel_X2                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,…
    ## $ StockOptionLevel_X3                <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,…
    ## $ WorkLifeBalance_Bad                <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ WorkLifeBalance_Good               <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 0, 1,…
    ## $ WorkLifeBalance_Better             <dbl> 0, 1, 1, 1, 1, 0, 0, 1, 1, 0,…
    ## $ WorkLifeBalance_Best               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ Attrition_No                       <dbl> 0, 1, 0, 1, 1, 1, 1, 1, 1, 1,…
    ## $ Attrition_Yes                      <dbl> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0,…

**Test data after applying recipe**

``` r
test_tbl <- bake(recipe_obj, new_data = test_readable_tbl)
test_tbl %>% glimpse()
```

    ## Observations: 220
    ## Variables: 83
    ## $ Age                                <dbl> -0.765152876, 1.987037199, 1.…
    ## $ DailyRate                          <dbl> 1.33637879, -1.54328218, -0.2…
    ## $ DistanceFromHome                   <dbl> -0.21324201, 0.26177716, -0.6…
    ## $ EmployeeNumber                     <dbl> -1.32358956, 0.42518111, 0.37…
    ## $ HourlyRate                         <dbl> -1.2323380, -1.4309100, 0.951…
    ## $ MonthlyIncome                      <dbl> 0.91393698, 1.38024738, 1.700…
    ## $ MonthlyRate                        <dbl> -8.826134e-01, 1.597584e+00, …
    ## $ NumCompaniesWorked                 <dbl> 0.03083829, -0.58603051, 0.45…
    ## $ PercentSalaryHike                  <dbl> -0.9455944, 0.1960170, -0.124…
    ## $ TotalWorkingYears                  <dbl> 0.29804725, 2.14458005, 2.264…
    ## $ TrainingTimesLastYear              <dbl> -0.6296220, 0.1442497, -0.629…
    ## $ YearsAtCompany                     <dbl> 0.74947123, 2.42388523, 0.749…
    ## $ YearsInCurrentRole                 <dbl> 1.2224028, 1.2224028, 1.22240…
    ## $ YearsSinceLastPromotion            <dbl> 1.44752633, 1.83567634, -1.11…
    ## $ YearsWithCurrManager               <dbl> 0.22693493, -1.54446392, 1.24…
    ## $ BusinessTravel_Non.Travel          <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,…
    ## $ BusinessTravel_Travel_Rarely       <dbl> 1, 0, 1, 0, 0, 1, 1, 1, 1, 0,…
    ## $ BusinessTravel_Travel_Frequently   <dbl> 0, 0, 0, 1, 1, 0, 0, 0, 0, 0,…
    ## $ Department_Human.Resources         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ Department_Research...Development  <dbl> 0, 1, 1, 0, 1, 1, 1, 1, 1, 0,…
    ## $ Department_Sales                   <dbl> 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,…
    ## $ Education_Below.College            <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ Education_College                  <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0,…
    ## $ Education_Bachelor                 <dbl> 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,…
    ## $ Education_Master                   <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…
    ## $ Education_Doctor                   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ EducationField_Human.Resources     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ EducationField_Life.Sciences       <dbl> 1, 0, 1, 0, 0, 1, 0, 0, 0, 1,…
    ## $ EducationField_Marketing           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ EducationField_Medical             <dbl> 0, 1, 0, 1, 0, 0, 1, 1, 1, 0,…
    ## $ EducationField_Other               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ EducationField_Technical.Degree    <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…
    ## $ EnvironmentSatisfaction_Low        <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 0,…
    ## $ EnvironmentSatisfaction_Medium     <dbl> 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,…
    ## $ EnvironmentSatisfaction_High       <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…
    ## $ EnvironmentSatisfaction_Very.High  <dbl> 0, 1, 1, 1, 0, 0, 0, 1, 1, 1,…
    ## $ Gender_Female                      <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 1, 1,…
    ## $ Gender_Male                        <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,…
    ## $ JobInvolvement_Low                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobInvolvement_Medium              <dbl> 0, 1, 0, 1, 1, 0, 0, 0, 1, 0,…
    ## $ JobInvolvement_High                <dbl> 1, 0, 1, 0, 0, 1, 1, 1, 0, 1,…
    ## $ JobInvolvement_Very.High           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobLevel_X1                        <dbl> 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,…
    ## $ JobLevel_X2                        <dbl> 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,…
    ## $ JobLevel_X3                        <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobLevel_X4                        <dbl> 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobLevel_X5                        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobRole_Healthcare.Representative  <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobRole_Human.Resources            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobRole_Laboratory.Technician      <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 1, 0,…
    ## $ JobRole_Manager                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobRole_Manufacturing.Director     <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0,…
    ## $ JobRole_Research.Director          <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobRole_Research.Scientist         <dbl> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0,…
    ## $ JobRole_Sales.Executive            <dbl> 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,…
    ## $ JobRole_Sales.Representative       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobSatisfaction_Low                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ JobSatisfaction_Medium             <dbl> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0,…
    ## $ JobSatisfaction_High               <dbl> 0, 0, 0, 0, 1, 1, 0, 0, 0, 1,…
    ## $ JobSatisfaction_Very.High          <dbl> 1, 0, 1, 0, 0, 0, 1, 1, 1, 0,…
    ## $ MaritalStatus_Single               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ MaritalStatus_Married              <dbl> 1, 0, 1, 1, 1, 0, 0, 0, 1, 1,…
    ## $ MaritalStatus_Divorced             <dbl> 0, 1, 0, 0, 0, 1, 1, 1, 0, 0,…
    ## $ OverTime_No                        <dbl> 1, 0, 1, 1, 0, 1, 1, 1, 1, 1,…
    ## $ OverTime_Yes                       <dbl> 0, 1, 0, 0, 1, 0, 0, 0, 0, 0,…
    ## $ PerformanceRating_Low              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ PerformanceRating_Good             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ PerformanceRating_Excellent        <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,…
    ## $ PerformanceRating_Outstanding      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ RelationshipSatisfaction_Low       <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,…
    ## $ RelationshipSatisfaction_Medium    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,…
    ## $ RelationshipSatisfaction_High      <dbl> 1, 0, 0, 0, 0, 0, 1, 1, 0, 0,…
    ## $ RelationshipSatisfaction_Very.High <dbl> 0, 1, 0, 1, 1, 1, 0, 0, 0, 0,…
    ## $ StockOptionLevel_X0                <dbl> 0, 0, 1, 1, 0, 0, 0, 0, 0, 0,…
    ## $ StockOptionLevel_X1                <dbl> 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,…
    ## $ StockOptionLevel_X2                <dbl> 0, 0, 0, 0, 0, 1, 0, 1, 0, 0,…
    ## $ StockOptionLevel_X3                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ WorkLifeBalance_Bad                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ WorkLifeBalance_Good               <dbl> 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,…
    ## $ WorkLifeBalance_Better             <dbl> 1, 1, 1, 0, 1, 1, 1, 0, 1, 1,…
    ## $ WorkLifeBalance_Best               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…
    ## $ Attrition_No                       <dbl> 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,…
    ## $ Attrition_Yes                      <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,…

### Correlation Analysis

Correlation Analysis is a great way to determine if you are getting good
features prior to modeling. It is always a good idea to spend your time
on collecting data on good features. If you do not have any feature
exhibiting low correlation, get different data, else analysis isn’t
going to look good.

Features exhibiting high correlation, we should immediately report them
as potential area of focus. Early detection helps stakeholders building
strategies.

Function **get\_cor** takes in data and measure correlation against
targeted feature.

``` r
# fct_reorder: reorders a factor by another column. It changes the level of factors but does not rearrange the data frame. We do this factor level reordering for plotting function, plot_cor()

get_cor <- function(data, target, use = "pairwise.complete.obs",
                     fct_reorder = FALSE, fct_rev = FALSE){
    
   feature_expr <- enquo(target)
  feature_name <- quo_name(feature_expr)
  
  data_cor <-  data %>%
    mutate_if(is.character, as.factor) %>%
    mutate_if(is.factor, as.numeric) %>%
    cor(use = use) %>%
    as.tibble() %>%
    mutate(feature = names(.)) %>%
    select(feature, !! feature_expr) %>%
    filter(!(feature == feature_name)) %>%
    mutate_if(is.character, as_factor)

  if(fct_reorder){
    data_cor <- data_cor %>%
      mutate(feature = fct_reorder(feature, !! feature_expr)) %>%
      arrange(feature)
  }
  
  if(fct_rev){
    data_cor <- data_cor %>%
      mutate(feature = fct_rev(feature)) %>%
      arrange(feature)
  }
  
return(data_cor)  
}
```

Using **train\_tbl** which has been well formatted for Correlation
Analysis

``` r
train_tbl %>% 
  get_cor(Attrition_Yes, fct_reorder = T, fct_rev = T )
```

    ## # A tibble: 82 x 2
    ##    feature                          Attrition_Yes
    ##    <fct>                                    <dbl>
    ##  1 PerformanceRating_Good                  NA    
    ##  2 PerformanceRating_Low                   NA    
    ##  3 OverTime_Yes                             0.240
    ##  4 JobLevel_X1                              0.212
    ##  5 StockOptionLevel_X0                      0.199
    ##  6 MaritalStatus_Single                     0.172
    ##  7 JobRole_Sales.Representative             0.153
    ##  8 EnvironmentSatisfaction_Low              0.119
    ##  9 WorkLifeBalance_Bad                      0.109
    ## 10 BusinessTravel_Travel_Frequently         0.103
    ## # … with 72 more rows

Function **plot\_cor** takes in data and plot correlation against
targeted feature i.e Attrition in this case.

``` r
data         <- train_tbl
feature_expr <- quo(Attrition_Yes)

plot_cor <- function(data, target, fct_reorder = FALSE, fct_rev = FALSE,
                     include_lbl = TRUE, lbl_precision = 2, lbl_position = "outward",
                     size = 2, line_size = 1, vert_size = 1,
                     color_pos = palette_light()[[1]],
                     color_neg = palette_light()[[2]]){

  
  feature_expr <- enquo(target)
  feature_name <- quo_name(feature_expr)

  data_cor <-  data %>%
      get_cor(!! feature_expr, fct_reorder = fct_reorder , fct_rev = fct_rev) %>%
      mutate(feature_name_text = round(!! feature_expr, lbl_precision)) %>%
      mutate(Correlation = case_when(
        (!! feature_expr) >=0 ~ "Positive",
        TRUE                  ~ "Negative") %>% as.factor())
  

      g <- data_cor %>%
        ggplot(aes_string(x = feature_name, y = "feature", group = "feature")) +
        geom_point(aes(color = Correlation), size = size) +
        geom_segment(aes(xend = 0, yend = feature, color = Correlation), size = line_size) +
        geom_vline(xintercept = 0, color = palette_light()[[1]], size = vert_size) +
        expand_limits(x = c(-1,1)) +
        theme_tq() +
        scale_color_manual(values = c(color_neg, color_pos))
      
      
    if(include_lbl) g <- g + geom_label(aes(label = feature_name_text), hjust = lbl_position)
    
    return(g)    
}
```

**Correlation Evaluation : After Transformation**

Variables that contribute most to attrition lies at the top

**Explore features by Category**

**1. Descriptive Features : Age, Gender, MaritalStatus**

``` r
train_tbl %>% 
  select(Attrition_Yes, Age, contains("Gender"), contains("MaritalStatus"), 
         NumCompaniesWorked, contains("Over18"), DistanceFromHome) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-50-1.png)

In this section, we will see how descriptive features correlate with
Attrition:

-   As Age increases, person has lower probability of leaving the
    company (negative correlation is good in this case) compared to a
    person who are in their early 20’s.  
-   There is a positive correlation between DistanceFromHome and
    Attrition which means it has a effect on Attrition. For instance, as
    the distance from home increases from company, there are more
    chances that employee may leave.

**2. Employment Features : Department, JobRole , JobLevel**

``` r
train_tbl %>% 
  select(Attrition_Yes, contains("employee"), contains("department"), contains("job") ) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-51-1.png)

We can say that there is a relationship between Attrition and
Joblevel\_X1. So, its a potential candidate for feature of interest.

**3. Compensation Features : DailyRate ,HourlyRate, MonthlyIncome,
MonthlyRate, PercentSalaryHike, StockOptionLevel**

``` r
train_tbl %>% 
  select(Attrition_Yes, contains("income"), contains("rate"), contains("salary"), contains("stock") ) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-52-1.png)

In this case, we observe that people who have StockOptionLevel\_X0 have
a effect on Attrition. So, they are more likely to leave the company.

**4. Survey Results : EnvironmentSatisfaction, JobSatisfaction ,
RelationshipSatisfaction, WorkLifeBalance**

``` r
train_tbl %>% 
  select(Attrition_Yes, contains("satisfaction"), contains("life") ) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-53-1.png)

We can say that people who have low environment satisfaction level at
company and bad worklife balance are more likely to leave the company
since they exhibit a positive correlation with the Attrition which is
not much significant but for sure has an effect on Attrition.

**5. Performance Data : JobInvolvement, PerformanceRating**

``` r
train_tbl %>% 
  select(Attrition_Yes, contains("performance"), contains("involvement")) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-54-1.png)

People whose JobInvolvement is low are more likely to leave the company.

**6. Work - Life Features : BusinessTravel , OverTime**

``` r
train_tbl %>% 
  select(Attrition_Yes, contains("overtime"), contains("travel")) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-55-1.png)

-   People who work OverTime has a strong effect on Attrition as
    compared to people who don’t. They are more likely to leave the
    company  
-   People who travel for business frequently have a positive
    relationship with Attrition. So, they are more likely to leave the
    company.

**7. Training & Education : Education, EducationField ,
TrainingTimesLastYear**

``` r
train_tbl %>% 
  select(Attrition_Yes, contains("training"), contains("education")) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-56-1.png)

-   We can say that people who have education field as Technical Degree,
    Human Resources & Marketing are more likley to leave the company

**8. Time - Based Features: TotalWorkingYears , YearsAtCompany ,
YearsInCurrentRole , YearsSinceLastPromotion, YearsWithCurrManager**

``` r
train_tbl %>% 
  select(Attrition_Yes, contains("years")) %>% 
plot_cor(target = Attrition_Yes, fct_reorder = T, fct_rev = F)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-57-1.png)

We can say that all the time based features have a negative correlation
with Attrition which is good in our case.

### Phase 4 : Modeling Churn with H2Os AutoML

In this phase, we will see how we turn information into business
insights ?

### Encode Decision Making Algorithms

**Step 1: Develop algorithms to predict target and explain in terms of
business levers**

**First step is to select the technique:**

-   Prediction
-   Classification
-   Regression
-   Clustering
-   Anomaly Detection

In this case, it is a **Binary Classification problem**

-   Employees that are likely to leave are coded as 0  
-   Employees that are likely to leave are coded as 1

**ML Preprocessing with Recipes**

We will cut down some steps in our final recipe before automl since h2o
is a low maintenance algorithm. It handles factor and numeric data and
performs pre-processing internally and transformations are not
necessary.

``` r
recipe_obj <- recipe(Attrition ~ ., data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_num2factor(JobLevel, StockOptionLevel) %>%
  prep()

recipe_obj
```

    ## Data Recipe
    ## 
    ## Inputs:
    ## 
    ##       role #variables
    ##    outcome          1
    ##  predictor         34
    ## 
    ## Training data contained 1250 data points and no missing data.
    ## 
    ## Operations:
    ## 
    ## Zero variance filter removed EmployeeCount, ... [trained]
    ## Factor variables from JobLevel, StockOptionLevel [trained]

``` r
train_tbl <- bake(recipe_obj, new_data = train_readable_tbl)
test_tbl <- bake(recipe_obj, new_data = test_readable_tbl)
```

**Initialising H2O**

``` r
h2o.init()
```

    ##  Connection successful!
    ## 
    ## R is connected to the H2O cluster: 
    ##     H2O cluster uptime:         4 days 20 hours 
    ##     H2O cluster timezone:       Asia/Kolkata 
    ##     H2O data parsing timezone:  UTC 
    ##     H2O cluster version:        3.23.0.4524 
    ##     H2O cluster version age:    2 months and 27 days  
    ##     H2O cluster name:           H2O_started_from_R_raj_fgz785 
    ##     H2O cluster total nodes:    1 
    ##     H2O cluster total memory:   2.84 GB 
    ##     H2O cluster total cores:    8 
    ##     H2O cluster allowed cores:  8 
    ##     H2O cluster healthy:        TRUE 
    ##     H2O Connection ip:          localhost 
    ##     H2O Connection port:        54321 
    ##     H2O Connection proxy:       NA 
    ##     H2O Internal Security:      FALSE 
    ##     H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4 
    ##     R Version:                  R version 3.5.1 (2018-07-02)

``` r
# Splitting the data into train/valid/test and converting tibble to h2o data frame
split_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1234)
```



``` r
train_h2o <- split_h2o[[1]]
valid_h2o <- split_h2o[[2]]
test_h2o <- as.h2o(test_tbl)
```



``` r
# Defining outcome / response variable
y <- "Attrition"

# Defining explanatory variables
x <- setdiff(names(train_h2o),y)
```

**Running AutoML**

-   *Training Frame* : Used to develop model  
-   *Validation Frame* : Used to tune hyperparameters via grid search  
-   *Leaderboard Frame*: Test set completely held out from model
    training & tuning  
-   Use *max\_run\_time\_secs* to minimize modeling time. Once results
    look promising we can always increase the run time to get more
    models with highly tuned parameters.

``` r
automl_models_h2o <- h2o.automl(
    x = x,
    y = y,
    training_frame = train_h2o,
    validation_frame = valid_h2o,
    leaderboard_frame = test_h2o,
    max_runtime_secs = 30,
    nfolds = 5
)
```

Producing Summary of models produced by H2O

``` r
automl_models_h2o@leaderboard
```

In the AutoML leaderboard, we get the models ordered with **highest
AUC** and **minimial Logloss**

**Saving and Loading H2O Models**

``` r
h2o.getModel("DeepLearning_1_AutoML_20190325_101852") %>%
  h2o.saveModel(path = "04_Modeling/h20_models/")

h2o.getModel("GLM_grid_1_AutoML_20190325_101852_model_1") %>%
  h2o.saveModel(path = "04_Modeling/h20_models/")

h2o.getModel("StackedEnsemble_BestOfFamily_AutoML_20190325_101852") %>%
  h2o.saveModel(path = "04_Modeling/h20_models/")

deeplearning_h2o <- h2o.loadModel("04_Modeling/h20_models/DeepLearning_1_AutoML_20190325_101852")

glm_h2o <- h2o.loadModel("04_Modeling/h20_models/GLM_grid_1_AutoML_20190325_101852_model_1") 

stacked_ensemble_h2o <- h2o.loadModel("04_Modeling/h20_models/StackedEnsemble_BestOfFamily_AutoML_20190325_101852")
```

In our case, We will focus more on exploring following leader models :

-   *GLM*
-   *Stacked Ensemble*
-   *Deep Learning*

Produces leader model in the summary by H2O

``` r
automl_models_h2o@leader
```

    ## Model Details:
    ## ==============
    ## 
    ## H2OBinomialModel: stackedensemble
    ## Model ID:  StackedEnsemble_BestOfFamily_AutoML_20190325_101852 
    ## NULL
    ## 
    ## 
    ## H2OBinomialMetrics: stackedensemble
    ## ** Reported on training data. **
    ## 
    ## MSE:  0.0425205
    ## RMSE:  0.206205
    ## LogLoss:  0.1623063
    ## Mean Per-Class Error:  0.1141125
    ## AUC:  0.9829226
    ## pr_auc:  0.9185428
    ## Gini:  0.9658452
    ## 
    ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
    ##         No Yes    Error      Rate
    ## No     873  16 0.017998   =16/889
    ## Yes     37 139 0.210227   =37/176
    ## Totals 910 155 0.049765  =53/1065
    ## 
    ## Maximum Metrics: Maximum metrics at their respective thresholds
    ##                         metric threshold    value idx
    ## 1                       max f1  0.409975 0.839879 127
    ## 2                       max f2  0.140350 0.877551 225
    ## 3                 max f0point5  0.523454 0.885714 105
    ## 4                 max accuracy  0.409975 0.950235 127
    ## 5                max precision  0.986089 1.000000   0
    ## 6                   max recall  0.094748 1.000000 264
    ## 7              max specificity  0.986089 1.000000   0
    ## 8             max absolute_mcc  0.409975 0.812852 127
    ## 9   max min_per_class_accuracy  0.185882 0.914773 194
    ## 10 max mean_per_class_accuracy  0.140350 0.930144 225
    ## 
    ## Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`
    ## H2OBinomialMetrics: stackedensemble
    ## ** Reported on validation data. **
    ## 
    ## MSE:  0.08123465
    ## RMSE:  0.2850169
    ## LogLoss:  0.2863561
    ## Mean Per-Class Error:  0.245
    ## AUC:  0.813
    ## pr_auc:  0.5554092
    ## Gini:  0.626
    ## 
    ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
    ##         No Yes    Error     Rate
    ## No     152   8 0.050000   =8/160
    ## Yes     11  14 0.440000   =11/25
    ## Totals 163  22 0.102703  =19/185
    ## 
    ## Maximum Metrics: Maximum metrics at their respective thresholds
    ##                         metric threshold    value idx
    ## 1                       max f1  0.346963 0.595745  21
    ## 2                       max f2  0.229984 0.629630  34
    ## 3                 max f0point5  0.678260 0.660377   6
    ## 4                 max accuracy  0.678260 0.902703   6
    ## 5                max precision  0.978007 1.000000   0
    ## 6                   max recall  0.035646 1.000000 158
    ## 7              max specificity  0.978007 1.000000   0
    ## 8             max absolute_mcc  0.346963 0.538636  21
    ## 9   max min_per_class_accuracy  0.092737 0.737500  60
    ## 10 max mean_per_class_accuracy  0.229984 0.783750  34
    ## 
    ## Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`
    ## H2OBinomialMetrics: stackedensemble
    ## ** Reported on cross-validation data. **
    ## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
    ## 
    ## MSE:  0.09294898
    ## RMSE:  0.3048753
    ## LogLoss:  0.3169679
    ## Mean Per-Class Error:  0.2331143
    ## AUC:  0.8346425
    ## pr_auc:  0.6259658
    ## Gini:  0.6692849
    ## 
    ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
    ##         No Yes    Error       Rate
    ## No     818  71 0.079865    =71/889
    ## Yes     68 108 0.386364    =68/176
    ## Totals 886 179 0.130516  =139/1065
    ## 
    ## Maximum Metrics: Maximum metrics at their respective thresholds
    ##                         metric threshold    value idx
    ## 1                       max f1  0.292277 0.608451 146
    ## 2                       max f2  0.141569 0.642570 219
    ## 3                 max f0point5  0.492226 0.658784  90
    ## 4                 max accuracy  0.492226 0.883568  90
    ## 5                max precision  0.986183 1.000000   0
    ## 6                   max recall  0.026760 1.000000 396
    ## 7              max specificity  0.986183 1.000000   0
    ## 8             max absolute_mcc  0.292277 0.530175 146
    ## 9   max min_per_class_accuracy  0.110456 0.750000 249
    ## 10 max mean_per_class_accuracy  0.141569 0.771398 219
    ## 
    ## Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`

**Making Predictions**

``` r
predictions <- h2o.predict(deeplearning_h2o, newdata = as.h2o(test_tbl))
```

``` r
# Saving it as a tibble
predictions_tbl <- predictions %>% as.tibble()
predictions_tbl
```

    ## # A tibble: 220 x 3
    ##    predict    No      Yes
    ##    <fct>   <dbl>    <dbl>
    ##  1 No      0.983 0.0167  
    ##  2 No      0.975 0.0250  
    ##  3 No      0.999 0.000648
    ##  4 No      0.984 0.0155  
    ##  5 Yes     0.288 0.712   
    ##  6 No      0.892 0.108   
    ##  7 Yes     0.595 0.405   
    ##  8 No      0.999 0.00117 
    ##  9 No      0.998 0.00228 
    ## 10 No      0.999 0.000512
    ## # … with 210 more rows

**Visualizing Leaderboard**

**Why is there need to Visualize leaderboard ?**

Answer to that is **Model Comparison** . We observe that in leaderboard
we get the model with **highest AUC** and **minimial Logloss** but
minimal logloss may not always be lowest for highest AUC model. So, it’s
a good idea to visualize these numbers which we will do later

-   Here, idea is to see how the models are performing in comparison to
    each other  
-   We can also see if the model position gets altered since we have
    added model position number  
-   Also, each model type is represented with a different color in plot

Function **plot\_h2o\_leaderboard ** takes in H2O leaderboard and
measure correlation against targeted feature

``` r
plot_h2o_leaderboard <- function(h2o_leaderboard, order_by = c("auc", "logloss"),
                                 n_max = 20, size = 4, include_lbl = TRUE) {
  
  #Setup input 
  order_by <- tolower(order_by[[1]])
  
  leaderboard_tbl <- h2o_leaderboard %>%
    as.tibble() %>%
    select(model_id,auc,logloss) %>%
    mutate(model_type = str_split(model_id, '_', simplify = TRUE)[,1]) %>%
    rownames_to_column(var = "rowname") %>%
    mutate(model_id = paste0(rowname, ". ",as.character(model_id)) %>% as_factor())
  
  
  # Transformation 
  if(order_by == "auc" ) {
    
    data_transformed_tbl <- leaderboard_tbl %>%
      slice(1:n_max) %>%
      mutate(
        model_id = as_factor(model_id) %>% reorder(auc),
        model_type = as_factor(model_type)
      ) %>%
      gather(key = key, value = value, 
             -c(model_id, model_type, rowname), factor_key = T) 
  } 
  else if(order_by == "logloss" ){
    data_transformed_tbl <- leaderboard_tbl %>%
      slice(1:n_max) %>%
      mutate(
        model_id = as_factor(model_id) %>% reorder(logloss)%>% fct_rev(),
        model_type = as_factor(model_type)
      ) %>%
      gather(key = key, value = value, 
             -c(model_id, model_type, rowname), factor_key = T)
  } else{
    stop(paste0(" order_by = '", order_by,"' is not a permitted option. "))
  }
  
 g <- data_transformed_tbl %>%
        ggplot(aes(value, model_id, color = model_type)) +
        geom_point(size = size) +
        facet_wrap(~ key, scales = "free_x") +
        theme_tq() +
        scale_color_tq() +
        labs(title = "Leaderboard Metrics",
             subtitle = paste0("Ordered by: ", toupper(order_by)),
             y = "Model Postion, Model ID", x = "")
  
  if(include_lbl) g <- g +  geom_label(aes(label  = round(value, 2), hjust = "inward")) 
  
  return(g)
  
}  
```

**H2O LeaderBoard Performance : Ordered by AUC**

``` r
automl_models_h2o@leaderboard %>%
  plot_h2o_leaderboard(order_by = "auc")
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-67-1.png)

We can see that first 3 models have much better performance than others
w.r.t highest auc and minimal logloss

**H2O LeaderBoard Performance : Ordered by LogLoss**

-   We observe that now model positions have been changed for Xgboost. In some cases, you will 
    also find that top model position changes    
-   After that we see a steep drop between auc and logloss, we might not
    need to inspect those models further

``` r
automl_models_h2o@leaderboard %>%
  plot_h2o_leaderboard(order_by = "logloss")
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-68-1.png)

**Assessing H2O Performance**

**h2o.performance()** : Creates an H2O performance object by providing
model and newdata

``` r
# Assessing Performance 
performance_h2o <- h2o.performance(stacked_ensemble_h2o, newdata =  as.h2o(test_tbl))
```


``` r
# prints all the classifier metrics 
performance_h2o@metrics
```

    ## $model
    ## $model$`__meta`
    ## $model$`__meta`$schema_version
    ## [1] 3
    ## 
    ## $model$`__meta`$schema_name
    ## [1] "ModelKeyV3"
    ## 
    ## $model$`__meta`$schema_type
    ## [1] "Key<Model>"
    ## 
    ## 
    ## $model$name
    ## [1] "StackedEnsemble_BestOfFamily_AutoML_20190325_101852"
    ## 
    ## $model$type
    ## [1] "Key<Model>"
    ## 
    ## $model$URL
    ## [1] "/3/Models/StackedEnsemble_BestOfFamily_AutoML_20190325_101852"
    ## 
    ## 
    ## $model_checksum
    ## [1] 7.207601e+18
    ## 
    ## $frame
    ## $frame$name
    ## [1] "test_tbl_sid_969f_25"
    ## 
    ## 
    ## $frame_checksum
    ## [1] -5.440343e+16
    ## 
    ## $description
    ## NULL
    ## 
    ## $scoring_time
    ## [1] 1.55349e+12
    ## 
    ## $predictions
    ## NULL
    ## 
    ## $MSE
    ## [1] 0.08300179
    ## 
    ## $RMSE
    ## [1] 0.2881003
    ## 
    ## $nobs
    ## [1] 220
    ## 
    ## $custom_metric_name
    ## NULL
    ## 
    ## $custom_metric_value
    ## [1] 0
    ## 
    ## $r2
    ## [1] 0.3935256
    ## 
    ## $logloss
    ## [1] 0.2878677
    ## 
    ## $AUC
    ## [1] 0.8759058
    ## 
    ## $pr_auc
    ## [1] 0.6685171
    ## 
    ## $Gini
    ## [1] 0.7518116
    ## 
    ## $mean_per_class_error
    ## [1] 0.1826691
    ## 
    ## $domain
    ## [1] "No"  "Yes"
    ## 
    ## $cm
    ## $cm$`__meta`
    ## $cm$`__meta`$schema_version
    ## [1] 3
    ## 
    ## $cm$`__meta`$schema_name
    ## [1] "ConfusionMatrixV3"
    ## 
    ## $cm$`__meta`$schema_type
    ## [1] "ConfusionMatrix"
    ## 
    ## 
    ## $cm$table
    ## Confusion Matrix: Row labels: Actual class; Column labels: Predicted class
    ##         No Yes  Error       Rate
    ## No     173  11 0.0598 = 11 / 184
    ## Yes     11  25 0.3056 =  11 / 36
    ## Totals 184  36 0.1000 = 22 / 220
    ## 
    ## 
    ## $thresholds_and_metric_scores
    ## Metrics for Thresholds: Binomial metrics as a function of classification thresholds
    ##   threshold       f1       f2 f0point5 accuracy precision   recall
    ## 1  0.956717 0.054054 0.034483 0.125000 0.840909  1.000000 0.027778
    ## 2  0.919105 0.105263 0.068493 0.227273 0.845455  1.000000 0.055556
    ## 3  0.902517 0.153846 0.102041 0.312500 0.850000  1.000000 0.083333
    ## 4  0.891069 0.200000 0.135135 0.384615 0.854545  1.000000 0.111111
    ## 5  0.877346 0.243902 0.167785 0.446429 0.859091  1.000000 0.138889
    ##   specificity absolute_mcc min_per_class_accuracy mean_per_class_accuracy
    ## 1    1.000000     0.152769               0.027778                0.513889
    ## 2    1.000000     0.216543               0.055556                0.527778
    ## 3    1.000000     0.265820               0.083333                0.541667
    ## 4    1.000000     0.307653               0.111111                0.555556
    ## 5    1.000000     0.344765               0.138889                0.569444
    ##   tns fns fps tps      tnr      fnr      fpr      tpr idx
    ## 1 184  35   0   1 1.000000 0.972222 0.000000 0.027778   0
    ## 2 184  34   0   2 1.000000 0.944444 0.000000 0.055556   1
    ## 3 184  33   0   3 1.000000 0.916667 0.000000 0.083333   2
    ## 4 184  32   0   4 1.000000 0.888889 0.000000 0.111111   3
    ## 5 184  31   0   5 1.000000 0.861111 0.000000 0.138889   4
    ## 
    ## ---
    ##     threshold       f1       f2 f0point5 accuracy precision   recall
    ## 215  0.029937 0.286853 0.501393 0.200893 0.186364  0.167442 1.000000
    ## 216  0.029886 0.285714 0.500000 0.200000 0.181818  0.166667 1.000000
    ## 217  0.029809 0.284585 0.498615 0.199115 0.177273  0.165899 1.000000
    ## 218  0.029739 0.283465 0.497238 0.198238 0.172727  0.165138 1.000000
    ## 219  0.029695 0.282353 0.495868 0.197368 0.168182  0.164384 1.000000
    ## 220  0.029617 0.281250 0.494505 0.196507 0.163636  0.163636 1.000000
    ##     specificity absolute_mcc min_per_class_accuracy
    ## 215    0.027174     0.067454               0.027174
    ## 216    0.021739     0.060193               0.021739
    ## 217    0.016304     0.052008               0.016304
    ## 218    0.010870     0.042367               0.010870
    ## 219    0.005435     0.029890               0.005435
    ## 220    0.000000     0.000000               0.000000
    ##     mean_per_class_accuracy tns fns fps tps      tnr      fnr      fpr
    ## 215                0.513587   5   0 179  36 0.027174 0.000000 0.972826
    ## 216                0.510870   4   0 180  36 0.021739 0.000000 0.978261
    ## 217                0.508152   3   0 181  36 0.016304 0.000000 0.983696
    ## 218                0.505435   2   0 182  36 0.010870 0.000000 0.989130
    ## 219                0.502717   1   0 183  36 0.005435 0.000000 0.994565
    ## 220                0.500000   0   0 184  36 0.000000 0.000000 1.000000
    ##          tpr idx
    ## 215 1.000000 214
    ## 216 1.000000 215
    ## 217 1.000000 216
    ## 218 1.000000 217
    ## 219 1.000000 218
    ## 220 1.000000 219
    ## 
    ## $max_criteria_and_metric_scores
    ## Maximum Metrics: Maximum metrics at their respective thresholds
    ##                         metric threshold    value idx
    ## 1                       max f1  0.323813 0.694444  35
    ## 2                       max f2  0.323813 0.694444  35
    ## 3                 max f0point5  0.525270 0.750000  20
    ## 4                 max accuracy  0.525270 0.904545  20
    ## 5                max precision  0.956717 1.000000   0
    ## 6                   max recall  0.049202 1.000000 147
    ## 7              max specificity  0.956717 1.000000   0
    ## 8             max absolute_mcc  0.323813 0.634662  35
    ## 9   max min_per_class_accuracy  0.147237 0.777778  64
    ## 10 max mean_per_class_accuracy  0.323813 0.817331  35
    ## 
    ## $gains_lift_table
    ## Gains/Lift Table: Avg response rate: 16.36 %, avg score: 17.37 %
    ##    group cumulative_data_fraction lower_threshold     lift cumulative_lift
    ## 1      1               0.01363636        0.900342 6.111111        6.111111
    ## 2      2               0.02272727        0.860247 6.111111        6.111111
    ## 3      3               0.03181818        0.808241 3.055556        5.238095
    ## 4      4               0.04090909        0.777759 6.111111        5.432099
    ## 5      5               0.05000000        0.745066 6.111111        5.555556
    ## 6      6               0.10000000        0.480243 4.444444        5.000000
    ## 7      7               0.15000000        0.333605 2.222222        4.074074
    ## 8      8               0.20000000        0.264923 1.666667        3.472222
    ## 9      9               0.30000000        0.145660 0.833333        2.592593
    ## 10    10               0.40000000        0.101660 0.555556        2.083333
    ## 11    11               0.50000000        0.070095 0.277778        1.722222
    ## 12    12               0.60000000        0.055868 0.833333        1.574074
    ## 13    13               0.70000000        0.046774 0.555556        1.428571
    ## 14    14               0.80000000        0.040402 0.000000        1.250000
    ## 15    15               0.90000000        0.033981 0.000000        1.111111
    ## 16    16               1.00000000        0.029617 0.000000        1.000000
    ##    response_rate    score cumulative_response_rate cumulative_score
    ## 1       1.000000 0.926113                 1.000000         0.926113
    ## 2       1.000000 0.884208                 1.000000         0.909351
    ## 3       0.500000 0.825633                 0.857143         0.885431
    ## 4       1.000000 0.791051                 0.888889         0.864458
    ## 5       1.000000 0.766819                 0.909091         0.846705
    ## 6       0.727273 0.629001                 0.818182         0.737853
    ## 7       0.363636 0.411330                 0.666667         0.629012
    ## 8       0.272727 0.305821                 0.568182         0.548214
    ## 9       0.136364 0.207113                 0.424242         0.434514
    ## 10      0.090909 0.122424                 0.340909         0.356491
    ## 11      0.045455 0.086237                 0.281818         0.302440
    ## 12      0.136364 0.061947                 0.257576         0.262358
    ## 13      0.090909 0.051162                 0.233766         0.232187
    ## 14      0.000000 0.042950                 0.204545         0.208533
    ## 15      0.000000 0.037123                 0.181818         0.189487
    ## 16      0.000000 0.031407                 0.163636         0.173679
    ##    capture_rate cumulative_capture_rate        gain cumulative_gain
    ## 1      0.083333                0.083333  511.111111      511.111111
    ## 2      0.055556                0.138889  511.111111      511.111111
    ## 3      0.027778                0.166667  205.555556      423.809524
    ## 4      0.055556                0.222222  511.111111      443.209877
    ## 5      0.055556                0.277778  511.111111      455.555556
    ## 6      0.222222                0.500000  344.444444      400.000000
    ## 7      0.111111                0.611111  122.222222      307.407407
    ## 8      0.083333                0.694444   66.666667      247.222222
    ## 9      0.083333                0.777778  -16.666667      159.259259
    ## 10     0.055556                0.833333  -44.444444      108.333333
    ## 11     0.027778                0.861111  -72.222222       72.222222
    ## 12     0.083333                0.944444  -16.666667       57.407407
    ## 13     0.055556                1.000000  -44.444444       42.857143
    ## 14     0.000000                1.000000 -100.000000       25.000000
    ## 15     0.000000                1.000000 -100.000000       11.111111
    ## 16     0.000000                1.000000 -100.000000        0.000000
    ## 
    ## $residual_deviance
    ## [1] 126.6618
    ## 
    ## $null_deviance
    ## [1] 196.0906
    ## 
    ## $AIC
    ## [1] 136.6618
    ## 
    ## $null_degrees_of_freedom
    ## [1] 219
    ## 
    ## $residual_degrees_of_freedom
    ## [1] 215

We will now see how to extract some of the above metrics individually
using *performance object*

**Classifier Summary metrics**

**Area Under Curve**  
It’s a way of measuring the performance of a binary classifier by
comparing the False Positive Rate (FPR x-axis) to True positive rate
(TPR y-axis)

``` r
# Retreives area under curve (AUC) for a classifier. If passing an h2O Model, can use the xval argument
# to retreive the average cross validation AUC
h2o.auc(performance_h2o)
```

    ## [1] 0.8759058

We can see that accuracy is 87.6% on test data.

Arguments train, valid and xval only works for models, not for
performance objects.

``` r
h2o.auc(stacked_ensemble_h2o, train = TRUE, valid =  TRUE, xval = TRUE)
```

    ##     train     valid      xval 
    ## 0.9829226 0.8130000 0.8346425

This model has 98% accuracy on training set, 81% accuracy on validation
set & 83% accuracy on cross validation set.

**Gini Coefficient**

-   Gini coefficient is a statistic which measures the ability of a
    scorecard or a characteristic to rank order risk.  
-   A Gini value of 0% means that the characteristic cannot distinguish
    good from bad cases. In general, Gini coeff above 60% is a good
    model.  
-   AUC = (GiniCoeff + 1) / 2

``` r
h2o.giniCoef(performance_h2o)
```

    ## [1] 0.7518116

We have GiniCoeff of 0.75 which implies it is a good model

**Logloss**

-   Log Loss quantifies the accuracy of a classifier by penalising false
    classifications.  
-   Great way to Measure the true performance of classifier by comparing
    the class probability to actual value (1 or 0).  
-   Minimising log loss gives greater accuracy for the classifier.

``` r
h2o.logloss(performance_h2o)
```

    ## [1] 0.2878677

**Confusion Matrix**

-   Used for summarizing the performance of a classification algorithm.
-   It shows the ways in which your classification model is confused
    when making predictions.
-   Critical for *Business Analysis*
-   Confusion matrix varies based on threshold ( value that determines
    which class probability is 1 or 0).

``` r
h2o.confusionMatrix(stacked_ensemble_h2o)
```

    ## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.409975269140079:
    ##         No Yes    Error      Rate
    ## No     873  16 0.017998   =16/889
    ## Yes     37 139 0.210227   =37/176
    ## Totals 910 155 0.049765  =53/1065

``` r
h2o.confusionMatrix(performance_h2o)
```

    ## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.323813276742962:
    ##         No Yes    Error     Rate
    ## No     173  11 0.059783  =11/184
    ## Yes     11  25 0.305556   =11/36
    ## Totals 184  36 0.100000  =22/220

We need to be able to undestand how these model change with thresholds
using **h2o.metric()** (Converts a performance object into a series of
metrics (e.g precision, recall, f1 etc) that vary by threshold)

**Important measures that vary by threshold :**

**Precision**

-   Measures false positives (e.g predicted to leave but actually stayed
    )  
-   Precision = TP / (TP +FP) \# 24 /(24+14) = 0.63
-   In other words, it detects how frequently your algorithm over-picks
    the Yes class  
-   Precision in business context: Precision indicates how often we
    incorrectly say people will leave when they actually will stay

**Recall**

-   Measures false negatives (e.g predicted to stay but actually left)  
-   Recall = TP / (TP + FN) \# 24 /(24+12) = 0.66
-   In other words, it provides a metric for under-picking the Yes
    class  
-   Recall is typically more important than Precision in the business
    context. We would rather give up some false positives (inadeverently
    target stayers) to gain false negatives (accurately predict
    quiters) \* Recall in business context: Recall indicates how often
    we miss people that will leave by incorrectly predicting they will
    stay

**F1**

-   Optimal balance between precision and recall. Typically the
    threshold that maximises F1 is used as threshold cutoff for turning
    class probability into 0/1. However this is not always the best
    case, An expected value optimization is required when costs of false
    positives and false negatives are known  
-   F1 = 2 (precision x recall) / (precision + recall) \# 2 x (0.63 x
    0.66) / (0.63+0.66) = 0.64  
-   In other words, it provides a metric for balancing precision vs
    recall  
-   Max F1 Thresholds: Threshold that optimizes the balance between
    precision and recall

**More Important Measures**

-   True positives (tps), True negatives (tns), False positives (fps),
    and False negatives (fns) are often converted into rates to
    understand cost / benefit of classifier  
-   Rates are included as tpr, tnr, fpr and fnr

``` r
performance_h2o %>%
# h2o.metric(): Returns the classifier performance
  h2o.metric() %>%
  as.tibble() %>%
  glimpse()
```

    ## Observations: 220
    ## Variables: 20
    ## $ threshold               <dbl> 0.9567169, 0.9191046, 0.9025171, 0.89106…
    ## $ f1                      <dbl> 0.05405405, 0.10526316, 0.15384615, 0.20…
    ## $ f2                      <dbl> 0.03448276, 0.06849315, 0.10204082, 0.13…
    ## $ f0point5                <dbl> 0.1250000, 0.2272727, 0.3125000, 0.38461…
    ## $ accuracy                <dbl> 0.8409091, 0.8454545, 0.8500000, 0.85454…
    ## $ precision               <dbl> 1.0000000, 1.0000000, 1.0000000, 1.00000…
    ## $ recall                  <dbl> 0.02777778, 0.05555556, 0.08333333, 0.11…
    ## $ specificity             <dbl> 1.0000000, 1.0000000, 1.0000000, 1.00000…
    ## $ absolute_mcc            <dbl> 0.1527691, 0.2165431, 0.2658205, 0.30765…
    ## $ min_per_class_accuracy  <dbl> 0.02777778, 0.05555556, 0.08333333, 0.11…
    ## $ mean_per_class_accuracy <dbl> 0.5138889, 0.5277778, 0.5416667, 0.55555…
    ## $ tns                     <dbl> 184, 184, 184, 184, 184, 184, 183, 183, …
    ## $ fns                     <dbl> 35, 34, 33, 32, 31, 30, 30, 29, 28, 27, …
    ## $ fps                     <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2…
    ## $ tps                     <dbl> 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11, 12…
    ## $ tnr                     <dbl> 1.0000000, 1.0000000, 1.0000000, 1.00000…
    ## $ fnr                     <dbl> 0.9722222, 0.9444444, 0.9166667, 0.88888…
    ## $ fpr                     <dbl> 0.000000000, 0.000000000, 0.000000000, 0…
    ## $ tpr                     <dbl> 0.02777778, 0.05555556, 0.08333333, 0.11…
    ## $ idx                     <int> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…

If we vary the threshold, metrics completely changed and if we do that
over 1 to 0 we can see how that affects the model across various
metrics.

**Gain & Lift**

Emphasizes how much model improves results

**Ranked Predictions :** By ranking by class probability of Yes, we
assess the models ability to truly detect someone that is leaving

``` r
ranked_predictions_tbl <- predictions_tbl %>%
  bind_cols(test_tbl) %>%
  select(predict:Yes, Attrition) %>%
  arrange(desc(Yes))

ranked_predictions_tbl
```

    ## # A tibble: 220 x 4
    ##    predict     No   Yes Attrition
    ##    <fct>    <dbl> <dbl> <fct>    
    ##  1 Yes     0.0180 0.982 Yes      
    ##  2 Yes     0.0389 0.961 Yes      
    ##  3 Yes     0.0770 0.923 Yes      
    ##  4 Yes     0.103  0.897 Yes      
    ##  5 Yes     0.115  0.885 Yes      
    ##  6 Yes     0.142  0.858 Yes      
    ##  7 Yes     0.143  0.857 Yes      
    ##  8 Yes     0.159  0.841 Yes      
    ##  9 Yes     0.264  0.736 No       
    ## 10 Yes     0.288  0.712 Yes      
    ## # … with 210 more rows

-   In first 10, we have 90% accuracy (9 of 10 people)
-   Without model, we’d only expect the global attrition rate for first
    10 (16% or 1.6 people)
-   If total expected quiters is 220 x 0.16 = 35
-   Gain : If 35 people expected to quit, we gained 9 of 35 or 25.7% in
    first 10 cases
-   Lift : If the expectation is 1.6 people, we beat the expectation by
    9/1.6 = 5.6X in first 10 cases

-   **Grouping By Likelihood:** Grouping into cohorts of most likely to
    least likely groups is as the heart of Gain/Lift chart. When we do
    this, we can immediately show that if candidate has high prob of
    leaving, how likely they are to leave.

-   **ntile:** Thinking of grouping by ntile as splitting up a
    continuious variable into n buckets This allows us to group the
    group the Response (Attrition) based on ntile column.

-   **Cumulative Gain:** Technically this is cumulative gain, because we
    cumulatively sum the pct\_responses

``` r
calculated_gain_lift_tbl <- ranked_predictions_tbl %>%
  mutate(ntile = ntile(Yes, n = 10)) %>%
  group_by(ntile) %>%
  summarise(
    cases = n(),
    responses = sum(Attrition == "Yes")
  ) %>%
  arrange(desc(ntile)) %>%
  mutate(group = row_number()) %>%
  select(group, cases, responses) %>%
  mutate(
    cumulative_responses = cumsum(responses),
    pct_responses        =  responses/sum(responses),
    gain                 =  cumsum(pct_responses),
    cumulative_pct_cases =  cumsum(cases)/sum(cases),
    lift                 =  gain/cumulative_pct_cases,
    gain_baseline        =  cumulative_pct_cases,
    lift_baseline        =  gain_baseline / cumulative_pct_cases
  )
```

We note that the 10th decile i.e this group has the highest class
probabilities for leaving, 18 of 22 actually left.

**Gain**

-   Think of this as what we gained using the model. For example, if we
    focused on first 2 groups , we gain ability to target 69.4% of our
    quiters using our model
-   Cumulative gain baseline is always equal to cumulative ntile
    percentage

**Lift**

-   Think of this as multiplier between what we gained divided by what
    we expected to gain with no model. For example, if we focused on the
    first two decile groups, we gained ability to target 69.4% of our
    quiters but we only expected to be able to target 20% of the quiters
    in 2 decile groups.  
-   Cumulative lift baseline is always equal to 1.  
-   Group: H2o groups into 16 ntiles with the first being the most
    likely to be in the minority (Yes) class 16 n-tiles  
-   220/ 16 n-tiles = 13.75 people / group  
-   16 n-tiles gives great resolution with about 14 people in each group

Defining terms \* Cumulative Capture Rate : This is *cumulative
percentage gain* that we will use  
\* Cumulative Lift : This is the *cumulative lift* that we will use

``` r
gain_lift_tbl <- performance_h2o %>%
   h2o.gainsLift() %>%
   as.tibble()

gain_lift_tbl
```

    ## # A tibble: 16 x 13
    ##    group cumulative_data_fraction lower_threshold  lift cumulative_lift
    ##    <int>                    <dbl>           <dbl> <dbl>           <dbl>
    ##  1     1                   0.0136          0.900  6.11             6.11
    ##  2     2                   0.0227          0.860  6.11             6.11
    ##  3     3                   0.0318          0.808  3.06             5.24
    ##  4     4                   0.0409          0.778  6.11             5.43
    ##  5     5                   0.05            0.745  6.11             5.56
    ##  6     6                   0.1             0.480  4.44             5   
    ##  7     7                   0.15            0.334  2.22             4.07
    ##  8     8                   0.2             0.265  1.67             3.47
    ##  9     9                   0.3             0.146  0.833            2.59
    ## 10    10                   0.4             0.102  0.556            2.08
    ## 11    11                   0.5             0.0701 0.278            1.72
    ## 12    12                   0.6             0.0559 0.833            1.57
    ## 13    13                   0.7             0.0468 0.556            1.43
    ## 14    14                   0.8             0.0404 0                1.25
    ## 15    15                   0.9             0.0340 0                1.11
    ## 16    16                   1               0.0296 0                1   
    ##    response_rate  score cumulative_response_rate cumulative_score
    ##            <dbl>  <dbl>                    <dbl>            <dbl>
    ##  1        1      0.926                     1                0.926
    ##  2        1      0.884                     1                0.909
    ##  3        0.5    0.826                     0.857            0.885
    ##  4        1      0.791                     0.889            0.864
    ##  5        1      0.767                     0.909            0.847
    ##  6        0.727  0.629                     0.818            0.738
    ##  7        0.364  0.411                     0.667            0.629
    ##  8        0.273  0.306                     0.568            0.548
    ##  9        0.136  0.207                     0.424            0.435
    ## 10        0.0909 0.122                     0.341            0.356
    ## 11        0.0455 0.0862                    0.282            0.302
    ## 12        0.136  0.0619                    0.258            0.262
    ## 13        0.0909 0.0512                    0.234            0.232
    ## 14        0      0.0429                    0.205            0.209
    ## 15        0      0.0371                    0.182            0.189
    ## 16        0      0.0314                    0.164            0.174
    ##    capture_rate cumulative_capture_rate   gain cumulative_gain
    ##           <dbl>                   <dbl>  <dbl>           <dbl>
    ##  1       0.0833                  0.0833  511.            511. 
    ##  2       0.0556                  0.139   511.            511. 
    ##  3       0.0278                  0.167   206.            424. 
    ##  4       0.0556                  0.222   511.            443. 
    ##  5       0.0556                  0.278   511.            456. 
    ##  6       0.222                   0.5     344.            400  
    ##  7       0.111                   0.611   122.            307. 
    ##  8       0.0833                  0.694    66.7           247. 
    ##  9       0.0833                  0.778   -16.7           159. 
    ## 10       0.0556                  0.833   -44.4           108. 
    ## 11       0.0278                  0.861   -72.2            72.2
    ## 12       0.0833                  0.944   -16.7            57.4
    ## 13       0.0556                  1       -44.4            42.9
    ## 14       0                       1      -100              25  
    ## 15       0                       1      -100              11.1
    ## 16       0                       1      -100               0

**Model Performance Comparison Dashboard**

Visualizing important model metrics and Gain & lift chart in this
dashboard

-   **ROC Plot**: Compares AUC of different models  
-   **Precision & Recall Plot**: Highly useful in comparing binary
    classification models  
-   **Gain & Lift Chart:** Charts designed to communicate model
    performance to non-technical stakeholders

``` r
# Performance Visualization 

plot_h2o_performance <- function(h2o_leaderboard, newdata, order_by = c("auc", "logloss"),
                                 max_models = 3, size = 1.5) {
    
    # Inputs
    
    leaderboard_tbl <- h2o_leaderboard %>%
        as.tibble() %>%
        slice(1:max_models)
    
    newdata_tbl <- newdata %>%
        as.tibble()
    
    order_by <- tolower(order_by[[1]])
    order_by_expr <- rlang::sym(order_by)
    
    h2o.no_progress()
    
    # Model metrics
    
    get_model_performance_metrics <- function(model_id, test_tbl) {
        
        model_h2o <- h2o.getModel(model_id)
        perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl))
        
        perf_h2o %>%
            h2o.metric() %>%
            as.tibble() %>%
            select(threshold, tpr, fpr, precision, recall)
        
    }
    
    model_metrics_tbl <- leaderboard_tbl %>%
        mutate(metrics = map(model_id, get_model_performance_metrics, newdata_tbl)) %>%
        unnest() %>%
        mutate(
            model_id = as_factor(model_id) %>% 
                fct_reorder(!! order_by_expr, .desc = ifelse(order_by == "auc", TRUE, FALSE)),
            auc  = auc %>% 
                round(3) %>% 
                as.character() %>% 
                as_factor() %>% 
                fct_reorder(as.numeric(model_id)),
            logloss = logloss %>% 
                round(4) %>% 
                as.character() %>% 
                as_factor() %>% 
                fct_reorder(as.numeric(model_id))
        )
    
    
    # ROC Plot
    
    p1 <- model_metrics_tbl %>%
        ggplot(aes_string("fpr", "tpr", color = "model_id", linetype = order_by)) +
        geom_line(size = size) +
        theme_tq() +
        scale_color_tq() +
        labs(title = "ROC", x = "FPR", y = "TPR") +
        theme(legend.direction = "vertical")
    
    # Precision vs Recall
    
    p2 <- model_metrics_tbl %>%
        ggplot(aes_string("recall", "precision", color = "model_id", linetype = order_by)) +
        geom_line(size = size) +
        theme_tq() +
        scale_color_tq() +
        labs(title = "Precision Vs Recall", x = "Recall", y = "Precision") +
        theme(legend.position = "none")
    
    
    # Gain / Lift
    
    get_gain_lift <- function(model_id, test_tbl) {
        
        model_h2o <- h2o.getModel(model_id)
        perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) 
        
        perf_h2o %>%
            h2o.gainsLift() %>%
            as.tibble() %>%
            select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift)
        
    }
    
    gain_lift_tbl <- leaderboard_tbl %>%
        mutate(metrics = map(model_id, get_gain_lift, newdata_tbl)) %>%
        unnest() %>%
        mutate(
            model_id = as_factor(model_id) %>% 
                fct_reorder(!! order_by_expr, .desc = ifelse(order_by == "auc", TRUE, FALSE)),
            auc  = auc %>% 
                round(3) %>% 
                as.character() %>% 
                as_factor() %>% 
                fct_reorder(as.numeric(model_id)),
            logloss = logloss %>% 
                round(4) %>% 
                as.character() %>% 
                as_factor() %>% 
                fct_reorder(as.numeric(model_id))
        ) %>%
        rename(
            gain = cumulative_capture_rate,
            lift = cumulative_lift
        ) 
    
    #  Gain Plot
    
    p3 <- gain_lift_tbl %>%
        ggplot(aes_string("cumulative_data_fraction", "gain", 
                          color = "model_id", linetype = order_by)) +
        geom_line(size = size) +
        geom_segment(x = 0, y = 0, xend = 1, yend = 1, 
                     color = "black", size = size) +
        theme_tq() +
        scale_color_tq() +
        expand_limits(x = c(0, 1), y = c(0, 1)) +
        labs(title = "Gain",
             x = "Cumulative Data Fraction", y = "Gain") +
        theme(legend.position = "none")
    
    # Lift Plot
    
    p4 <- gain_lift_tbl %>%
        ggplot(aes_string("cumulative_data_fraction", "lift", 
                          color = "model_id", linetype = order_by)) +
        geom_line(size = size) +
        geom_segment(x = 0, y = 1, xend = 1, yend = 1, 
                     color = "black", size = size) +
        theme_tq() +
        scale_color_tq() +
        expand_limits(x = c(0, 1), y = c(0, 1)) +
        labs(title = "Lift",
             x = "Cumulative Data Fraction", y = "Lift") +
        theme(legend.position = "none")
    
    
    # Combine using cowplot
    p_legend <- get_legend(p1)
    p1 <- p1 + theme(legend.position = "none")
    
    p <- cowplot::plot_grid(p1, p2, p3, p4, ncol = 2) 
    
    p_title <- ggdraw() + 
        draw_label("H2O Model Metrics", size = 18, fontface = "bold", 
                   colour = palette_light()[[1]])
    
    p_subtitle <- ggdraw() + 
        draw_label(glue("Ordered by {toupper(order_by)}"), size = 10,  
                   colour = palette_light()[[1]])
    
    ret <- plot_grid(p_title, p_subtitle, p, p_legend, 
                     ncol = 1, rel_heights = c(0.05, 0.05, 1, 0.05 * max_models))
    
    h2o.show_progress()
    
    return(ret)
    
}
```

**Model Performance Comparison Dashboard : Ordered by AUC**

``` r
automl_models_h2o@leaderboard %>%
    plot_h2o_performance(newdata = test_tbl, order_by = "auc", 
                         size = 1, max_models = 4)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-81-1.png)

**Takeaways**

-   We can conclude from ROC Curve that GLM model has higher AUC than
    Stacked Ensemble.  
-   For Stacked Ensemble Best of Family models, precision drops off very
    quickly as recall increases which is not desirable.  
-   We can notice from Precision & Recall plot that black one i.e GLM
    has better recall than others especially early one and precision
    begins to drop for all 3 models.  
-   We can notice from *Gain Plot* that organisation can get 69% of
    gains by just focusing on the top 25% of employees as compared to
    baseline model where gains are only 25%.
-   We notice from lift chart, we get 3x positive responses as compared
    to basline model which has been lifted from 1x.  
-   The two charts work together to show the results of using the
    modeling approach versus just targeting people at random
-   But all these 3 models seems to be performing very similarly.

**Model Performance Comparison Dashboard : Ordered by Logloss**

On a similar note, we can also choose the best model based on minimal
logloss

``` r
automl_models_h2o@leaderboard %>%
    plot_h2o_performance(newdata = test_tbl, order_by = "logloss", 
                         size = 1, max_models = 4)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-82-1.png)

How does lift apply to Attrition ?

-   Example: Providing stock options strategically to high performers
    that are high risk  
-   Could strategically focus on those in the high risk category that
    are working time  
-   Stock options Assignment would be 3x more effective with the model

**Communicating to Executive**

-   If 35 people are expected to quit, we can obtain 69% gain by
    targeting the top 25% of high risk people.  
-   This reduces costs by 1/3rd versus random selection because we only
    need to offer stock options to high risk candidates.

### Explaining Black-Box Classification Models With LIME

-   Models that tend to be highly predictive like stacked ensembles are
    not interpretable by normal means which is a serious issue.  
-   Explanations are critical to business
-   Key question here is why is churn happening ?  
-   Knowledge empowers to make better decisions

**LIME - Local Interpretable Model-Agnostic Explanations package**

-   Used to determine which features contribute to prediction (& by how
    much) for a single observation (i.e local).  
-   Every complex model is linear at local level.  
-   Performs localized regression 1000x and returns results which are
    then averaged. Features with highest importance becomes the
    expalanation.  
-   LIME can be used to explain Black-Box classification models e.g deep
    learning, stacked ensembles and random forest.  
-   AutoML + LIME can be applied to any classification problem in
    business like customer churn, fraud detection.

``` r
automl_leader <- h2o.loadModel("04_Modeling/h20_models/StackedEnsemble_BestOfFamily_AutoML_20190325_101852")

# Making Predictions

predictions_tbl <- automl_leader %>% 
  h2o.predict(newdata = as.h2o(test_tbl)) %>%
  as.tibble() %>%
  bind_cols(
    test_tbl %>%
      select(Attrition, EmployeeNumber)
  )
```



**Steps Involved in LIME**

1.  Build explainer with lime()  
2.  Create an explanation with explain()

**Multiple Explanations**

**Using lime::explain() function to build explainer**

-   Use bin\_continuous to bin the features. It makes it easy to detect
    what causes the continuous feature to have a high feature weight
    explanation
-   Using n\_bins to tell how many bins you want
-   Using quantile\_bins to tell how to distribute observations with the
    bins. If True, cuts will be selected to evenly distribute the total
    observations within each of bins

``` r
explainer <- train_tbl %>%
  select(-Attrition) %>%
  lime(
    model           =  automl_leader,
    bin_continuous  =  TRUE,
    n_bins          =  4, 
    quantile_bins   = TRUE
    )

explainer
```

    ## $model
    ## Model Details:
    ## ==============
    ## 
    ## H2OBinomialModel: stackedensemble
    ## Model ID:  StackedEnsemble_BestOfFamily_AutoML_20190325_101852 
    ## NULL
    ## 
    ## 
    ## H2OBinomialMetrics: stackedensemble
    ## ** Reported on training data. **
    ## 
    ## MSE:  0.0425205
    ## RMSE:  0.206205
    ## LogLoss:  0.1623063
    ## Mean Per-Class Error:  0.1141125
    ## AUC:  0.9829226
    ## pr_auc:  0.9185428
    ## Gini:  0.9658452
    ## 
    ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
    ##         No Yes    Error      Rate
    ## No     873  16 0.017998   =16/889
    ## Yes     37 139 0.210227   =37/176
    ## Totals 910 155 0.049765  =53/1065
    ## 
    ## Maximum Metrics: Maximum metrics at their respective thresholds
    ##                         metric threshold    value idx
    ## 1                       max f1  0.409975 0.839879 127
    ## 2                       max f2  0.140350 0.877551 225
    ## 3                 max f0point5  0.523454 0.885714 105
    ## 4                 max accuracy  0.409975 0.950235 127
    ## 5                max precision  0.986089 1.000000   0
    ## 6                   max recall  0.094748 1.000000 264
    ## 7              max specificity  0.986089 1.000000   0
    ## 8             max absolute_mcc  0.409975 0.812852 127
    ## 9   max min_per_class_accuracy  0.185882 0.914773 194
    ## 10 max mean_per_class_accuracy  0.140350 0.930144 225
    ## 
    ## Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`
    ## H2OBinomialMetrics: stackedensemble
    ## ** Reported on validation data. **
    ## 
    ## MSE:  0.08123465
    ## RMSE:  0.2850169
    ## LogLoss:  0.2863561
    ## Mean Per-Class Error:  0.245
    ## AUC:  0.813
    ## pr_auc:  0.5554092
    ## Gini:  0.626
    ## 
    ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
    ##         No Yes    Error     Rate
    ## No     152   8 0.050000   =8/160
    ## Yes     11  14 0.440000   =11/25
    ## Totals 163  22 0.102703  =19/185
    ## 
    ## Maximum Metrics: Maximum metrics at their respective thresholds
    ##                         metric threshold    value idx
    ## 1                       max f1  0.346963 0.595745  21
    ## 2                       max f2  0.229984 0.629630  34
    ## 3                 max f0point5  0.678260 0.660377   6
    ## 4                 max accuracy  0.678260 0.902703   6
    ## 5                max precision  0.978007 1.000000   0
    ## 6                   max recall  0.035646 1.000000 158
    ## 7              max specificity  0.978007 1.000000   0
    ## 8             max absolute_mcc  0.346963 0.538636  21
    ## 9   max min_per_class_accuracy  0.092737 0.737500  60
    ## 10 max mean_per_class_accuracy  0.229984 0.783750  34
    ## 
    ## Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`
    ## H2OBinomialMetrics: stackedensemble
    ## ** Reported on cross-validation data. **
    ## ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **
    ## 
    ## MSE:  0.09294898
    ## RMSE:  0.3048753
    ## LogLoss:  0.3169679
    ## Mean Per-Class Error:  0.2331143
    ## AUC:  0.8346425
    ## pr_auc:  0.6259658
    ## Gini:  0.6692849
    ## 
    ## Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:
    ##         No Yes    Error       Rate
    ## No     818  71 0.079865    =71/889
    ## Yes     68 108 0.386364    =68/176
    ## Totals 886 179 0.130516  =139/1065
    ## 
    ## Maximum Metrics: Maximum metrics at their respective thresholds
    ##                         metric threshold    value idx
    ## 1                       max f1  0.292277 0.608451 146
    ## 2                       max f2  0.141569 0.642570 219
    ## 3                 max f0point5  0.492226 0.658784  90
    ## 4                 max accuracy  0.492226 0.883568  90
    ## 5                max precision  0.986183 1.000000   0
    ## 6                   max recall  0.026760 1.000000 396
    ## 7              max specificity  0.986183 1.000000   0
    ## 8             max absolute_mcc  0.292277 0.530175 146
    ## 9   max min_per_class_accuracy  0.110456 0.750000 249
    ## 10 max mean_per_class_accuracy  0.141569 0.771398 219
    ## 
    ## Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`
    ## 
    ## $bin_continuous
    ## [1] TRUE
    ## 
    ## $n_bins
    ## [1] 4
    ## 
    ## $quantile_bins
    ## [1] TRUE
    ## 
    ## $use_density
    ## [1] TRUE
    ## 
    ## $feature_type
    ##                      Age           BusinessTravel                DailyRate 
    ##                "numeric"                 "factor"                "numeric" 
    ##               Department         DistanceFromHome                Education 
    ##                 "factor"                "numeric"                 "factor" 
    ##           EducationField           EmployeeNumber  EnvironmentSatisfaction 
    ##                 "factor"                "numeric"                 "factor" 
    ##                   Gender               HourlyRate           JobInvolvement 
    ##                 "factor"                "numeric"                 "factor" 
    ##                 JobLevel                  JobRole          JobSatisfaction 
    ##                 "factor"                 "factor"                 "factor" 
    ##            MaritalStatus            MonthlyIncome              MonthlyRate 
    ##                 "factor"                "numeric"                "numeric" 
    ##       NumCompaniesWorked                 OverTime        PercentSalaryHike 
    ##                "numeric"                 "factor"                "numeric" 
    ##        PerformanceRating RelationshipSatisfaction         StockOptionLevel 
    ##                 "factor"                 "factor"                 "factor" 
    ##        TotalWorkingYears    TrainingTimesLastYear          WorkLifeBalance 
    ##                "numeric"                "numeric"                 "factor" 
    ##           YearsAtCompany       YearsInCurrentRole  YearsSinceLastPromotion 
    ##                "numeric"                "numeric"                "numeric" 
    ##     YearsWithCurrManager 
    ##                "numeric" 
    ## 
    ## $bin_cuts
    ## $bin_cuts$Age
    ##   0%  25%  50%  75% 100% 
    ##   18   30   36   43   60 
    ## 
    ## $bin_cuts$BusinessTravel
    ## NULL
    ## 
    ## $bin_cuts$DailyRate
    ##   0%  25%  50%  75% 100% 
    ##  102  464  798 1156 1498 
    ## 
    ## $bin_cuts$Department
    ## NULL
    ## 
    ## $bin_cuts$DistanceFromHome
    ##    0%   25%   50%   75%  100% 
    ##  1.00  2.00  7.00 13.75 29.00 
    ## 
    ## $bin_cuts$Education
    ## NULL
    ## 
    ## $bin_cuts$EducationField
    ## NULL
    ## 
    ## $bin_cuts$EmployeeNumber
    ##      0%     25%     50%     75%    100% 
    ##    1.00  486.25 1020.50 1553.75 2068.00 
    ## 
    ## $bin_cuts$EnvironmentSatisfaction
    ## NULL
    ## 
    ## $bin_cuts$Gender
    ## NULL
    ## 
    ## $bin_cuts$HourlyRate
    ##   0%  25%  50%  75% 100% 
    ##   30   48   66   83  100 
    ## 
    ## $bin_cuts$JobInvolvement
    ## NULL
    ## 
    ## $bin_cuts$JobLevel
    ## NULL
    ## 
    ## $bin_cuts$JobRole
    ## NULL
    ## 
    ## $bin_cuts$JobSatisfaction
    ## NULL
    ## 
    ## $bin_cuts$MaritalStatus
    ## NULL
    ## 
    ## $bin_cuts$MonthlyIncome
    ##       0%      25%      50%      75%     100% 
    ##  1009.00  2935.25  4903.50  8395.00 19999.00 
    ## 
    ## $bin_cuts$MonthlyRate
    ##       0%      25%      50%      75%     100% 
    ##  2097.00  8191.25 14328.00 20337.25 26999.00 
    ## 
    ## $bin_cuts$NumCompaniesWorked
    ##   0%  25%  50%  75% 100% 
    ##    0    1    2    4    9 
    ## 
    ## $bin_cuts$OverTime
    ## NULL
    ## 
    ## $bin_cuts$PercentSalaryHike
    ##   0%  25%  50%  75% 100% 
    ##   11   12   14   18   25 
    ## 
    ## $bin_cuts$PerformanceRating
    ## NULL
    ## 
    ## $bin_cuts$RelationshipSatisfaction
    ## NULL
    ## 
    ## $bin_cuts$StockOptionLevel
    ## NULL
    ## 
    ## $bin_cuts$TotalWorkingYears
    ##   0%  25%  50%  75% 100% 
    ##    0    6   10   15   40 
    ## 
    ## $bin_cuts$TrainingTimesLastYear
    ##   0%  25%  50% 100% 
    ##    0    2    3    6 
    ## 
    ## $bin_cuts$WorkLifeBalance
    ## NULL
    ## 
    ## $bin_cuts$YearsAtCompany
    ##   0%  25%  50%  75% 100% 
    ##    0    3    5   10   40 
    ## 
    ## $bin_cuts$YearsInCurrentRole
    ##   0%  25%  50%  75% 100% 
    ##    0    2    3    7   18 
    ## 
    ## $bin_cuts$YearsSinceLastPromotion
    ##   0%  50%  75% 100% 
    ##    0    1    3   15 
    ## 
    ## $bin_cuts$YearsWithCurrManager
    ##   0%  25%  50%  75% 100% 
    ##    0    2    3    7   17 
    ## 
    ## 
    ## $feature_distribution
    ## $feature_distribution$Age
    ## 
    ##      1      2      3      4 
    ## 0.2608 0.2824 0.2176 0.2392 
    ## 
    ## $feature_distribution$BusinessTravel
    ## 
    ##        Non-Travel     Travel_Rarely Travel_Frequently 
    ##            0.0992            0.7096            0.1912 
    ## 
    ## $feature_distribution$DailyRate
    ## 
    ##      1      2      3      4 
    ## 0.2512 0.2496 0.2488 0.2504 
    ## 
    ## $feature_distribution$Department
    ## 
    ##        Human Resources Research & Development                  Sales 
    ##                 0.0392                 0.6656                 0.2952 
    ## 
    ## $feature_distribution$DistanceFromHome
    ## 
    ##      1      2      3      4 
    ## 0.2856 0.2488 0.2152 0.2504 
    ## 
    ## $feature_distribution$Education
    ## 
    ## Below College       College      Bachelor        Master        Doctor 
    ##        0.1192        0.1928        0.3840        0.2712        0.0328 
    ## 
    ## $feature_distribution$EducationField
    ## 
    ##  Human Resources    Life Sciences        Marketing          Medical 
    ##           0.0184           0.4160           0.1080           0.3136 
    ##            Other Technical Degree 
    ##           0.0528           0.0912 
    ## 
    ## $feature_distribution$EmployeeNumber
    ## 
    ##      1      2      3      4 
    ## 0.2504 0.2496 0.2496 0.2504 
    ## 
    ## $feature_distribution$EnvironmentSatisfaction
    ## 
    ##       Low    Medium      High Very High 
    ##    0.1960    0.1936    0.3104    0.3000 
    ## 
    ## $feature_distribution$Gender
    ## 
    ## Female   Male 
    ## 0.3928 0.6072 
    ## 
    ## $feature_distribution$HourlyRate
    ## 
    ##      1      2      3      4 
    ## 0.2512 0.2632 0.2440 0.2416 
    ## 
    ## $feature_distribution$JobInvolvement
    ## 
    ##       Low    Medium      High Very High 
    ##    0.0536    0.2488    0.5944    0.1032 
    ## 
    ## $feature_distribution$JobLevel
    ## 
    ##      1      2      3      4      5 
    ## 0.3680 0.3608 0.1456 0.0752 0.0504 
    ## 
    ## $feature_distribution$JobRole
    ## 
    ## Healthcare Representative           Human Resources 
    ##                    0.0840                    0.0312 
    ##     Laboratory Technician                   Manager 
    ##                    0.1792                    0.0712 
    ##    Manufacturing Director         Research Director 
    ##                    0.0984                    0.0584 
    ##        Research Scientist           Sales Executive 
    ##                    0.2072                    0.2184 
    ##      Sales Representative 
    ##                    0.0520 
    ## 
    ## $feature_distribution$JobSatisfaction
    ## 
    ##       Low    Medium      High Very High 
    ##    0.2016    0.1848    0.3000    0.3136 
    ## 
    ## $feature_distribution$MaritalStatus
    ## 
    ##   Single  Married Divorced 
    ##   0.3192   0.4608   0.2200 
    ## 
    ## $feature_distribution$MonthlyIncome
    ## 
    ##      1      2      3      4 
    ## 0.2504 0.2496 0.2496 0.2504 
    ## 
    ## $feature_distribution$MonthlyRate
    ## 
    ##      1      2      3      4 
    ## 0.2504 0.2496 0.2496 0.2504 
    ## 
    ## $feature_distribution$NumCompaniesWorked
    ## 
    ##      1      2      3      4 
    ## 0.4848 0.1000 0.2048 0.2104 
    
![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/Snip20190325_8.png)
![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/Snip20190325_12.png)


Creating an explanation with explain

``` r
explanation <- test_tbl %>%
    slice(1:20) %>%
    select(-Attrition) %>%
    lime::explain(
        explainer = explainer,
        n_labels   = 1,
        n_features = 8,
        n_permutations = 5000,
        kernel_width   = 1
    )
```


``` r
explanation %>%
    as.tibble()
```

    ## # A tibble: 160 x 13
    ##    model_type     case  label label_prob model_r2 model_intercept
    ##    <chr>          <chr> <chr>      <dbl>    <dbl>           <dbl>
    ##  1 classification 1     No         0.964    0.212           0.670
    ##  2 classification 1     No         0.964    0.212           0.670
    ##  3 classification 1     No         0.964    0.212           0.670
    ##  4 classification 1     No         0.964    0.212           0.670
    ##  5 classification 1     No         0.964    0.212           0.670
    ##  6 classification 1     No         0.964    0.212           0.670
    ##  7 classification 1     No         0.964    0.212           0.670
    ##  8 classification 1     No         0.964    0.212           0.670
    ##  9 classification 2     No         0.856    0.338           0.764
    ## 10 classification 2     No         0.856    0.338           0.764
    ##    model_prediction feature                 feature_value feature_weight
    ##               <dbl> <chr>                           <dbl>          <dbl>
    ##  1            0.896 OverTime                            1         0.168 
    ##  2            0.896 Department                          3        -0.0797
    ##  3            0.896 YearsSinceLastPromotion             7        -0.0703
    ##  4            0.896 MonthlyIncome                    9419         0.0705
    ##  5            0.896 StockOptionLevel                    2         0.0638
    ##  6            0.896 YearsInCurrentRole                  9         0.0627
    ##  7            0.896 JobSatisfaction                     4         0.0624
    ##  8            0.896 Age                                30        -0.0515
    ##  9            0.664 YearsAtCompany                     33        -0.227 
    ## 10            0.664 OverTime                            2        -0.172 
    ##    feature_desc                data        prediction
    ##    <chr>                       <list>      <list>    
    ##  1 OverTime = No               <list [31]> <list [2]>
    ##  2 Department = Sales          <list [31]> <list [2]>
    ##  3 3 < YearsSinceLastPromotion <list [31]> <list [2]>
    ##  4 8395 < MonthlyIncome        <list [31]> <list [2]>
    ##  5 StockOptionLevel = 1        <list [31]> <list [2]>
    ##  6 7 < YearsInCurrentRole      <list [31]> <list [2]>
    ##  7 JobSatisfaction = Very High <list [31]> <list [2]>
    ##  8 Age <= 30                   <list [31]> <list [2]>
    ##  9 10 < YearsAtCompany         <list [31]> <list [2]>
    ## 10 OverTime = Yes              <list [31]> <list [2]>
    ## # … with 150 more rows

#### Recreating Plot Features with LIME

Function **plot\_features\_tq** create a visualization containing
individual plot for each observations

-   It plots the 8 most influential variables that best explain the
    linear model in that observations local region.  
-   Label variable if it causes an increase in the probability as
    **supports** or if it causes decrease in the probability
    **contradicts**.  
-   It provides us with the model fit labelled as Explanation fit for
    each model, which allows us to see how well that model explains the
    local region.

``` r
plot_features_tq <- function(explanation, ncol) {
  
  data_transformed <- explanation %>%
    as.tibble() %>%
    mutate(
      feature_desc = as_factor(feature_desc) %>%
        fct_reorder(abs(feature_weight), .desc = FALSE),
      key = ifelse( feature_weight > 0, "Supports", "Contradicts") %>%
        fct_relevel("Supports"),
      case_text = glue("Case : {case}"),
      label_text = glue("Label : {label}"),
      prob_text  = glue("Probability: {round(label_prob, 2)}"),
      r2_text    = glue("Explanation Fit : {model_r2 %>% round(2)}")
    ) %>%
    select(feature_desc, feature_weight, key, case_text:r2_text)
  
  
  data_transformed %>%
    ggplot(aes(feature_desc, feature_weight, fill = key)) +
    geom_bar(stat = "identity") +
    coord_flip() +
    theme_tq() +
    scale_fill_tq() +
    labs(y = "Weight" , x = "Feature") +
    facet_wrap(~ case_text + label_text + prob_text + r2_text, 
               ncol = ncol , scales = "free")
  
} 
```

We can infer that **Case 5** has the highest probability of leaving the
company and variables that appear to be influencing this high
probability include OverTime (Yes), NumCompaniesWorked (&gt;4), Joblevel
(1), Business Travel (Frequently) & DistanceFromHome (13.8 miles).

``` r
explanation %>%
  filter(case %in% 1:6) %>%
  plot_features_tq(ncol = 2)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-87-1.png)

#### Recreating Plot explanation with LIME

``` r
plot_explanations_tq <- function(explanation) {
  
  
  data_transformed <- explanation %>%
    as.tibble() %>%
    mutate(
      case = as_factor(case),
      order_1 = rank(feature)
    ) %>%
    group_by(feature) %>%
    mutate(
      order_2 = rank(feature_value)
    ) %>%
    ungroup() %>%
    mutate(
      order = order_1 * 1000 + order_2
    ) %>%
    mutate(
      feature_desc  = as.factor(feature_desc) %>%
        fct_reorder(order, .desc = T)
    ) %>%
    select(case, feature_desc, feature_weight, label)
  
  
  data_transformed %>%
    ggplot(aes(case, feature_desc)) +
    geom_tile(aes(fill = feature_weight)) + 
    facet_wrap(~ label) +
    theme_tq() + 
    scale_fill_gradient2(low = palette_light()[[2]],
                         mid = "white", 
                         high = palette_light()[[1]]) +
    theme(
      panel.grid = element_blank(),
      legend.position = "right",
      axis.text.x = element_text(angle = 45, hjust = 1, vjust = 1)
    ) + 
    labs(y = "Feature", x = "Case", 
         fill  = glue("Feature 
                      Weight"))
  
}
```

-   Heatmap showing how the different features selected across all the
    observations influence each case
-   This plot is useful if we are trying to find common features that
    influence all the observations

``` r
plot_explanations_tq(explanation)
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-89-1.png)

### Phase 5 : Evaluation

-   In this phase, we try to evaluate degree to which data science
    insights meets the business objectives  
-   Success of data science project depends upon ROI.  
-   We must always focus to present a business case for making any
    changes, then your organization.  
    decides whether or not to proceed with some or all recommendations
    based on ROI.

**Expected Value Framework**

-   A way of assigning value to a decision using both the probability of
    its occurence and potential benefit of outcome  
-   Decisions becomes based on probability rather than intuition

**Step 2: Tie financial value of individual decisions to optimize for
profit**

-   *MonthlyIncome*: Used later as a way to calculate cost of policy
    change
-   *OverTime* : Needed to determine whether or not the employee is
    working overtime
-   *Attrition cost* is only occurred if employee leaves. The ‘Yes’
    column is the likelihood of leaving. When combined, we can get an
    expected attrition cost .

**Calculating the Expected ROI Of a Policy Change**

**Policy I: No OverTime for Everyone**

-   “No OT Policy” i.e Threshold = 0, anyone with over time is targeted
    , analysis of test set (15% of Total Population)
-   **Baseline**: Do nothing results in no additional costs (e.g no cost
    incurred from reduction in OT)

> Expected Cost = Yes \* (Total Potential Cost) + No \* (Policy change
> cost)

-   For Baseline : Expected cost equation simplifies further since there
    is no policy change

> Expected Cost = Yes \* (Attrition Cost)

**Calculating Expected Value With OT**

``` r
predictions_with_OT_tbl <- automl_leader %>%
  h2o.predict(newdata = as.h2o(test_tbl)) %>%
  as.tibble() %>%
  bind_cols(
    test_tbl %>%
      select(EmployeeNumber, MonthlyIncome, OverTime)
  )
```



``` r
ev_with_OT_tbl <- predictions_with_OT_tbl %>%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %>%
  mutate(
    cost_of_policy_change = 0
  ) %>%
  mutate(
    expected_attrition_cost = 
      Yes * (attrition_cost + cost_of_policy_change) +
      No  * (cost_of_policy_change)
  )

total_ev_with_OT_tbl <- ev_with_OT_tbl %>%
  summarise(
    total_expected_attrition_cost_0 = sum(expected_attrition_cost)
  )
```

We observed that total expected cost with Overtime policy is $3,191,435.

``` r
total_ev_with_OT_tbl
```

    ## # A tibble: 1 x 1
    ##   total_expected_attrition_cost_0
    ##                             <dbl>
    ## 1                        3159744.

**Calculating Expected Value Without OT**

-   **Policy Change**: The goal is to update the calculation to reflect
    a new policy based on your realistic parameters you and your
    organisation are potentially experiencing.  
-   We are interested in showing savings with no overtime policy, so we
    will be switching Overtime NO to Yes on test set and then predicting
    their prob again using h2o.predict().  
-   Adjusting Overtime had a pretty big impact on likelihood of leaving.
    Let’s say organization has an average overtime of 10%.

``` r
test_without_OT_tbl <- test_tbl %>%
  mutate(OverTime = fct_recode(OverTime, "No" = "Yes"))

predictions_without_OT_tbl <- automl_leader %>%
  h2o.predict(newdata = as.h2o(test_without_OT_tbl)) %>%
  as.tibble() %>%
  bind_cols(
    test_tbl %>%
      select(EmployeeNumber, MonthlyIncome, OverTime),
    test_without_OT_tbl %>%
      select(OverTime)
  ) %>%
  rename(
    OverTime_0 = OverTime,
    OverTime_1 = OverTime1
  )
```


``` r
avg_overtime_pct <- 0.10


ev_without_OT_tbl<- predictions_without_OT_tbl %>%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %>%
  mutate(
    cost_of_policy_change = case_when(
      OverTime_0 == "Yes" & OverTime_1 == "No" ~ avg_overtime_pct * attrition_cost,
      TRUE ~ 0
    )
  ) %>%
  mutate(
    expected_attrition_cost = 
      Yes * (attrition_cost + cost_of_policy_change) +
      No  * (cost_of_policy_change)
  )

total_ev_without_OT_tbl <- ev_without_OT_tbl %>%
  summarise(
    total_expected_attrition_cost_1 = sum(expected_attrition_cost)
  )
```

We observed that total expected cost with Overtime policy is $2,795,000.
This implies cost is going down ( $3,191,435 to $2,795,000) after
implementing the policy.

``` r
total_ev_without_OT_tbl
```

    ## # A tibble: 1 x 1
    ##   total_expected_attrition_cost_1
    ##                             <dbl>
    ## 1                        2762888.

*Expected Savings Calculation*

-   We can potentially save **$396k** for the organization i.e 12.4%
    savings.
-   To annualize the savings, multiply the factor of total observations
    to test observations.  
    (train + test) / test = (1250 + 220) / 220 = 6.7.  
-   Annualizing Savings: $396k X 6.7 = $2.6M.

``` r
bind_cols(
  total_ev_with_OT_tbl,
  total_ev_without_OT_tbl
) %>%
  mutate(
    savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
    pct_savings = savings / total_expected_attrition_cost_0
  )
```

    ## # A tibble: 1 x 4
    ##   total_expected_attrition_cost_0 total_expected_attrition_cost_1 savings
    ##                             <dbl>                           <dbl>   <dbl>
    ## 1                        3159744.                        2762888. 396856.
    ##   pct_savings
    ##         <dbl>
    ## 1       0.126

So, we can potentially save the organization $2.6M per year if we
implement No overtime policy for everyone.  
**Targeting by Threshold Primer**

-   In this case, we **target by Threshold**, if threshold = 30%, only
    employees with prob\_leave &gt;= 0.30 are targeted.  
-   Most Critical : Don’t want to miss FNs. Typically cost is higher for
    FNs.

**Comparing FNs vs FPs**

-   FN’s are more costly than FPs  
-   **False Negatives** - We predict employee stays but actually leaves,
    we fail to target and do not reduce OT for 100% of attrition cost.  
-   **False Positives** - We predict leaves but stays, we target and
    reduce OT for 10-30% of attrition cost.  
-   For example: Lets say FN = $100 k , FP = $30K ,FN/FP = 3X more
    costly. In this case, when FN’s = 3X FP’s, lower threshold (lower
    the threshold from 28% to 13% ,catch the people that have 13% or
    more probability of quitting.  
-   Expected Savings Increases 13.9% ((549 - 482) / 482 \* 100), $67k
    Savings / 0.15 = $446k per year additional savings for full
    population.

``` r
performance_h2o <- automl_leader %>%
  h2o.performance(newdata = as.h2o(test_tbl))
```



``` r
performance_h2o %>%
  h2o.confusionMatrix()
```

    ## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.323813276742962:
    ##         No Yes    Error     Rate
    ## No     173  11 0.059783  =11/184
    ## Yes     11  25 0.305556   =11/36
    ## Totals 184  36 0.100000  =22/220

Most Critical : Don’t want to miss False Negatives. Typically cost is
higher for FNs.

``` r
rates_by_threshold_tbl <- performance_h2o %>%
  h2o.metric() %>%
  as.tibble()

rates_by_threshold_tbl %>%
  glimpse()
```

    ## Observations: 220
    ## Variables: 20
    ## $ threshold               <dbl> 0.9567169, 0.9191046, 0.9025171, 0.89106…
    ## $ f1                      <dbl> 0.05405405, 0.10526316, 0.15384615, 0.20…
    ## $ f2                      <dbl> 0.03448276, 0.06849315, 0.10204082, 0.13…
    ## $ f0point5                <dbl> 0.1250000, 0.2272727, 0.3125000, 0.38461…
    ## $ accuracy                <dbl> 0.8409091, 0.8454545, 0.8500000, 0.85454…
    ## $ precision               <dbl> 1.0000000, 1.0000000, 1.0000000, 1.00000…
    ## $ recall                  <dbl> 0.02777778, 0.05555556, 0.08333333, 0.11…
    ## $ specificity             <dbl> 1.0000000, 1.0000000, 1.0000000, 1.00000…
    ## $ absolute_mcc            <dbl> 0.1527691, 0.2165431, 0.2658205, 0.30765…
    ## $ min_per_class_accuracy  <dbl> 0.02777778, 0.05555556, 0.08333333, 0.11…
    ## $ mean_per_class_accuracy <dbl> 0.5138889, 0.5277778, 0.5416667, 0.55555…
    ## $ tns                     <dbl> 184, 184, 184, 184, 184, 184, 183, 183, …
    ## $ fns                     <dbl> 35, 34, 33, 32, 31, 30, 30, 29, 28, 27, …
    ## $ fps                     <dbl> 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 2…
    ## $ tps                     <dbl> 1, 2, 3, 4, 5, 6, 6, 7, 8, 9, 10, 11, 12…
    ## $ tnr                     <dbl> 1.0000000, 1.0000000, 1.0000000, 1.00000…
    ## $ fnr                     <dbl> 0.9722222, 0.9444444, 0.9166667, 0.88888…
    ## $ fpr                     <dbl> 0.000000000, 0.000000000, 0.000000000, 0…
    ## $ tpr                     <dbl> 0.02777778, 0.05555556, 0.08333333, 0.11…
    ## $ idx                     <int> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12…

F1 is the optimal balance between Precision & Recall.However, the
threshold @Max F1 is not typically the optimal value for the business
case because FNs are typically more costly than FPs.

**tnr & fpr properties**

1.  Probability of correctly classifying a negative  
2.  When actual is negative, we just sometimes classify it as positive  
3.  tnr + fpr = 1

**tpr & fnr properties**

1.  Probability of correctly classifying a positive  
2.  When actual is positive, we just sometimes classify it as negative  
3.  tpr + fnr = 1

``` r
rates_by_threshold_tbl %>%
  select(threshold, f1, tnr:tpr) %>%
  filter( f1 == max(f1)) %>%
  slice(1)  
```

    ## # A tibble: 1 x 6
    ##   threshold    f1   tnr   fnr    fpr   tpr
    ##       <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>
    ## 1     0.324 0.694 0.940 0.306 0.0598 0.694

**Visualizing Expected Rates**

-   Fundamentally, expected rates are the probability of getting the
    model prediction correct or incorrect at a given threshold.  
-   Idea is to understand how expected rates interact as threshold
    increases.  
-   **Cost Tradeoff** : In a perfect world, our model would never make
    mistakes & we could target TPs perfectly. We do not live in a
    perfect world, FPs and FNs results in costs.

``` r
rates_by_threshold_tbl %>%
  select(threshold, f1, tnr:tpr) %>%
  gather(key = key, value = value, tnr:tpr, factor_key = TRUE) %>%
  mutate(key = fct_reorder2(key, threshold, value)) %>%
  ggplot(aes(threshold, value, color = key)) +
  geom_point() + 
  geom_smooth() +
  theme_tq() +
  scale_color_tq() +
  theme(legend.position = "right") +
  labs(
    title = "Expected Rates",
    y = "Value",
    x = "Threshold"
  )
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-98-1.png)

-   We can notice from the plot that at any point on given threshold tnr
    and fpr are related, these rates together add up to 1  
-   We can notice from the plot that at any point on given threshold tpr
    and fnr are related , these rates together add up to 1

``` r
predictions_with_OT_tbl <- automl_leader %>%
  h2o.predict(newdata = as.h2o(test_tbl)) %>%
  as.tibble() %>%
  bind_cols(
    test_tbl %>%
      select(EmployeeNumber, MonthlyIncome, OverTime)
  )
```


``` r
predictions_with_OT_tbl 
```

    ## # A tibble: 220 x 6
    ##    predict    No    Yes EmployeeNumber MonthlyIncome OverTime
    ##    <fct>   <dbl>  <dbl>          <dbl>         <dbl> <fct>   
    ##  1 No      0.964 0.0359            228          9419 No      
    ##  2 No      0.856 0.144            1278         13577 Yes     
    ##  3 No      0.970 0.0298           1250         17779 No      
    ##  4 No      0.954 0.0456           2065          5390 No      
    ##  5 Yes     0.200 0.800            1767          2437 Yes     
    ##  6 No      0.927 0.0728           1308          2372 No      
    ##  7 No      0.951 0.0490             18          2661 No      
    ##  8 No      0.967 0.0331            460          6347 No      
    ##  9 No      0.970 0.0303           1369          5363 No      
    ## 10 No      0.966 0.0344           1040          6347 No      
    ## # … with 210 more rows

``` r
ev_with_OT_tbl <- predictions_with_OT_tbl %>%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %>%
  mutate(
    cost_of_policy_change = 0
  ) %>%
  mutate(
    expected_attrition_cost = 
      Yes * (attrition_cost + cost_of_policy_change) +
      No  * (cost_of_policy_change)
  )

ev_with_OT_tbl
```

    ## # A tibble: 220 x 9
    ##    predict    No    Yes EmployeeNumber MonthlyIncome OverTime
    ##    <fct>   <dbl>  <dbl>          <dbl>         <dbl> <fct>   
    ##  1 No      0.964 0.0359            228          9419 No      
    ##  2 No      0.856 0.144            1278         13577 Yes     
    ##  3 No      0.970 0.0298           1250         17779 No      
    ##  4 No      0.954 0.0456           2065          5390 No      
    ##  5 Yes     0.200 0.800            1767          2437 Yes     
    ##  6 No      0.927 0.0728           1308          2372 No      
    ##  7 No      0.951 0.0490             18          2661 No      
    ##  8 No      0.967 0.0331            460          6347 No      
    ##  9 No      0.970 0.0303           1369          5363 No      
    ## 10 No      0.966 0.0344           1040          6347 No      
    ##    attrition_cost cost_of_policy_change expected_attrition_cost
    ##             <dbl>                 <dbl>                   <dbl>
    ##  1         72979.                     0                   2618.
    ##  2         64663.                     0                   9298.
    ##  3         56259.                     0                   1677.
    ##  4         81037.                     0                   3698.
    ##  5         86943.                     0                  69570.
    ##  6         87073.                     0                   6341.
    ##  7         86495.                     0                   4242.
    ##  8         79123.                     0                   2616.
    ##  9         81091.                     0                   2458.
    ## 10         79123.                     0                   2723.
    ## # … with 210 more rows

``` r
total_ev_with_OT_tbl <- ev_with_OT_tbl %>%
  summarise(
    total_expected_attrition_cost_0 = sum(expected_attrition_cost)
  )

total_ev_with_OT_tbl
```

    ## # A tibble: 1 x 1
    ##   total_expected_attrition_cost_0
    ##                             <dbl>
    ## 1                        3159744.

**Calculating Expected Value With Targeted OT**

``` r
max_f1_tbl <- rates_by_threshold_tbl %>%
  select(threshold, f1, tnr:tpr) %>%
  filter(f1 == max(f1)) %>%
  slice(1)

max_f1_tbl
```

    ## # A tibble: 1 x 6
    ##   threshold    f1   tnr   fnr    fpr   tpr
    ##       <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>
    ## 1     0.324 0.694 0.940 0.306 0.0598 0.694

``` r
tnr <- max_f1_tbl$tnr
fnr <- max_f1_tbl$fnr
fpr <- max_f1_tbl$fpr
tpr <- max_f1_tbl$tpr

threshold <- max_f1_tbl$threshold
threshold 
```

    ## [1] 0.3238133

Anyone with Yes &gt; = 0.3125333 and OverTime = Yes had their OT toggled
to No.

``` r
test_targeted_OT_tbl <- test_tbl %>%
  add_column(Yes = predictions_with_OT_tbl$Yes) %>%
  mutate(
    OverTime = case_when(
      Yes >= threshold ~ factor("No", levels = levels(test_tbl$OverTime)) ,
      TRUE ~ OverTime
     )
  ) %>%
  select(-Yes)

test_targeted_OT_tbl
```

    ## # A tibble: 220 x 32
    ##      Age Attrition BusinessTravel    DailyRate Department            
    ##    <dbl> <fct>     <fct>                 <dbl> <fct>                 
    ##  1    30 No        Travel_Rarely          1339 Sales                 
    ##  2    55 No        Non-Travel              177 Research & Development
    ##  3    54 No        Travel_Rarely           685 Research & Development
    ##  4    49 No        Travel_Frequently      1023 Sales                 
    ##  5    43 Yes       Travel_Frequently       807 Research & Development
    ##  6    58 No        Travel_Rarely           848 Research & Development
    ##  7    34 No        Travel_Rarely          1346 Research & Development
    ##  8    37 No        Travel_Rarely          1192 Research & Development
    ##  9    35 No        Travel_Rarely           817 Research & Development
    ## 10    50 No        Non-Travel              145 Sales                 
    ##    DistanceFromHome Education     EducationField   EmployeeNumber
    ##               <dbl> <fct>         <fct>                     <dbl>
    ##  1                5 Bachelor      Life Sciences               228
    ##  2                8 Below College Medical                    1278
    ##  3                3 Bachelor      Life Sciences              1250
    ##  4                2 Bachelor      Medical                    2065
    ##  5               17 Bachelor      Technical Degree           1767
    ##  6               23 Master        Life Sciences              1308
    ##  7               19 College       Medical                      18
    ##  8                5 College       Medical                     460
    ##  9                1 Bachelor      Medical                    1369
    ## 10                1 Bachelor      Life Sciences              1040
    ##    EnvironmentSatisfaction Gender HourlyRate JobInvolvement JobLevel
    ##    <fct>                   <fct>       <dbl> <fct>          <fct>   
    ##  1 Medium                  Female         41 High           3       
    ##  2 Very High               Male           37 Medium         4       
    ##  3 Very High               Male           85 High           4       
    ##  4 Very High               Male           63 Medium         2       
    ##  5 High                    Male           38 Medium         1       
    ##  6 Low                     Male           88 High           1       
    ##  7 Medium                  Male           93 High           1       
    ##  8 Very High               Male           61 High           2       
    ##  9 Very High               Female         60 Medium         2       
    ## 10 Very High               Female         95 High           2       
    ##    JobRole                   JobSatisfaction MaritalStatus MonthlyIncome
    ##    <fct>                     <fct>           <fct>                 <dbl>
    ##  1 Sales Executive           Very High       Married                9419
    ##  2 Healthcare Representative Medium          Divorced              13577
    ##  3 Research Director         Very High       Married               17779
    ##  4 Sales Executive           Medium          Married                5390
    ##  5 Research Scientist        High            Married                2437
    ##  6 Research Scientist        High            Divorced               2372
    ##  7 Laboratory Technician     Very High       Divorced               2661
    ##  8 Manufacturing Director    Very High       Divorced               6347
    ##  9 Laboratory Technician     Very High       Married                5363
    ## 10 Sales Executive           High            Married                6347
    ##    MonthlyRate NumCompaniesWorked OverTime PercentSalaryHike
    ##          <dbl>              <dbl> <fct>                <dbl>
    ##  1        8053                  2 No                      12
    ##  2       25592                  1 Yes                     15
    ##  3       23474                  3 No                      14
    ##  4       13243                  2 No                      14
    ##  5       15587                  9 No                      16
    ##  6       26076                  1 No                      12
    ##  7        8758                  0 No                      11
    ##  8       23177                  7 No                      16
    ##  9       10846                  0 No                      12
    ## 10       24920                  0 No                      12
    ##    PerformanceRating RelationshipSatisfaction StockOptionLevel
    ##    <fct>             <fct>                    <fct>           
    ##  1 Excellent         High                     1               
    ##  2 Excellent         Very High                1               
    ##  3 Excellent         Low                      0               
    ##  4 Excellent         Very High                0               
    ##  5 Excellent         Very High                1               
    ##  6 Excellent         Very High                2               
    ##  7 Excellent         High                     1               
    ##  8 Excellent         High                     2               
    ##  9 Excellent         Medium                   1               
    ## 10 Excellent         Low                      1               
    ##    TotalWorkingYears TrainingTimesLastYear WorkLifeBalance YearsAtCompany
    ##                <dbl>                 <dbl> <fct>                    <dbl>
    ##  1                12                     2 Better                      10
    ##  2                34                     3 Better                      33
    ##  3                36                     2 Better                      10
    ##  4                17                     3 Good                         9
    ##  5                 6                     4 Better                       1
    ##  6                 2                     3 Better                       2
    ##  7                 3                     2 Better                       2
    ##  8                 8                     2 Good                         6
    ##  9                10                     0 Better                       9
    ## 10                19                     3 Better                      18
    ##    YearsInCurrentRole YearsSinceLastPromotion YearsWithCurrManager
    ##                 <dbl>                   <dbl>                <dbl>
    ##  1                  9                       7                    4
    ##  2                  9                      15                    0
    ##  3                  9                       0                    9
    ##  4                  6                       0                    8
    ##  5                  0                       0                    0
    ##  6                  2                       2                    2
    ##  7                  2                       1                    2
    ##  8                  2                       0                    4
    ##  9                  7                       0                    0
    ## 10                  7                       0                   13
    ## # … with 210 more rows

Running h2o.predict() on the modified data will give us the
probabilities of attrition = Yes i.e implementing the policy change.

``` r
predictions_targeted_OT_tbl <- automl_leader %>%
  h2o.predict(newdata = as.h2o(test_targeted_OT_tbl)) %>%
  as.tibble() %>%
  bind_cols(
    test_tbl %>%
      select(EmployeeNumber, MonthlyIncome, OverTime),
    test_targeted_OT_tbl %>%
      select(OverTime)
  ) %>%
  rename(
    OverTime_0 = OverTime,
    OverTime_1 = OverTime1
  )
```


``` r
predictions_targeted_OT_tbl
```

    ## # A tibble: 220 x 7
    ##    predict    No    Yes EmployeeNumber MonthlyIncome OverTime_0 OverTime_1
    ##    <fct>   <dbl>  <dbl>          <dbl>         <dbl> <fct>      <fct>     
    ##  1 No      0.964 0.0359            228          9419 No         No        
    ##  2 No      0.856 0.144            1278         13577 Yes        Yes       
    ##  3 No      0.970 0.0298           1250         17779 No         No        
    ##  4 No      0.954 0.0456           2065          5390 No         No        
    ##  5 No      0.795 0.205            1767          2437 Yes        No        
    ##  6 No      0.927 0.0728           1308          2372 No         No        
    ##  7 No      0.951 0.0490             18          2661 No         No        
    ##  8 No      0.967 0.0331            460          6347 No         No        
    ##  9 No      0.970 0.0303           1369          5363 No         No        
    ## 10 No      0.966 0.0344           1040          6347 No         No        
    ## # … with 210 more rows

``` r
avg_overtime_pct <- 0.10


ev_targeted_OT_tbl <- predictions_targeted_OT_tbl %>%
  mutate(
    attrition_cost = calculate_attrition_cost(
      n = 1,
      salary = MonthlyIncome * 12,
      net_revenue_per_employee = 250000
    )
  ) %>%
  mutate(
    cost_of_policy_change = case_when(
      OverTime_0 == "Yes" & OverTime_1 == "No" ~ attrition_cost * avg_overtime_pct,
      TRUE ~ 0
    )
  ) %>%
  mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
  )


ev_targeted_OT_tbl
```

    ## # A tibble: 220 x 14
    ##    predict    No    Yes EmployeeNumber MonthlyIncome OverTime_0 OverTime_1
    ##    <fct>   <dbl>  <dbl>          <dbl>         <dbl> <fct>      <fct>     
    ##  1 No      0.964 0.0359            228          9419 No         No        
    ##  2 No      0.856 0.144            1278         13577 Yes        Yes       
    ##  3 No      0.970 0.0298           1250         17779 No         No        
    ##  4 No      0.954 0.0456           2065          5390 No         No        
    ##  5 No      0.795 0.205            1767          2437 Yes        No        
    ##  6 No      0.927 0.0728           1308          2372 No         No        
    ##  7 No      0.951 0.0490             18          2661 No         No        
    ##  8 No      0.967 0.0331            460          6347 No         No        
    ##  9 No      0.970 0.0303           1369          5363 No         No        
    ## 10 No      0.966 0.0344           1040          6347 No         No        
    ##    attrition_cost cost_of_policy_change cb_tn cb_fp  cb_tp  cb_fn
    ##             <dbl>                 <dbl> <dbl> <dbl>  <dbl>  <dbl>
    ##  1         72979.                    0     0     0  72979. 72979.
    ##  2         64663.                    0     0     0  64663. 64663.
    ##  3         56259.                    0     0     0  56259. 56259.
    ##  4         81037.                    0     0     0  81037. 81037.
    ##  5         86943.                 8694. 8694. 8694. 95637. 95637.
    ##  6         87073.                    0     0     0  87073. 87073.
    ##  7         86495.                    0     0     0  86495. 86495.
    ##  8         79123.                    0     0     0  79123. 79123.
    ##  9         81091.                    0     0     0  81091. 81091.
    ## 10         79123.                    0     0     0  79123. 79123.
    ##    expected_attrition_cost
    ##                      <dbl>
    ##  1                   2618.
    ##  2                   9298.
    ##  3                   1677.
    ##  4                   3698.
    ##  5                  26547.
    ##  6                   6341.
    ##  7                   4242.
    ##  8                   2616.
    ##  9                   2458.
    ## 10                   2723.
    ## # … with 210 more rows

``` r
total_ev_targeted_OT_tbl <- ev_targeted_OT_tbl %>%
  summarise(total_expected_attrition_cost_1 = sum(expected_attrition_cost))

total_ev_targeted_OT_tbl
```

    ## # A tibble: 1 x 1
    ##   total_expected_attrition_cost_1
    ##                             <dbl>
    ## 1                        2746695.

**Savings Calculation**

Percentage savings goes up to 14.2% from 12.4% , So targted OT makes
more sense

``` r
savings_tbl <- bind_cols(
  total_ev_with_OT_tbl,
  total_ev_targeted_OT_tbl
) %>%
  mutate(
    savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
    pct_savings = savings / total_expected_attrition_cost_0
  )

savings_tbl
```

    ## # A tibble: 1 x 4
    ##   total_expected_attrition_cost_0 total_expected_attrition_cost_1 savings
    ##                             <dbl>                           <dbl>   <dbl>
    ## 1                        3159744.                        2746695. 413049.
    ##   pct_savings
    ##         <dbl>
    ## 1       0.131

**Threshold Optimization**

-   An Iterative approach to expected value.  
-   Critical for maximizing profitability of a policy change.

**calculate\_savings\_by\_threshold()**

-   A function that calculates & returns the savings when the user
    provides data, h2o model, threshold and expected rates (tnr, fpr,
    fnr, tpr)  
-   Default: The function is set up for a *“No OT Policy”* because the
    threshold is so low anyone with OT gets converted to No OT

**Savings By Threshold**

Function that can be used on a single threshold to calculate savings

``` r
calculate_savings_by_threshold <- function(data, h2o_model, threshold = 0, 
                                           tnr = 0, fpr = 1, fnr = 0, tpr = 1) {
  
  data_0_tbl <- as.tibble(data)
  
  # Expected Value
  
  # Calculating Expected Value with OT
  
  pred_0_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_0_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime)
    )
  
  ev_0_tbl <- pred_0_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = 250000
      )
    ) %>%
    mutate(
      cost_of_policy_change = 0
    ) %>%
    mutate(
      expected_attrition_cost = 
        Yes * (attrition_cost + cost_of_policy_change) +
        No  * (cost_of_policy_change)
    )
  
  total_ev_0_tbl <- ev_0_tbl %>%
    summarise(
      total_expected_attrition_cost_0 = sum(expected_attrition_cost)
    )
  
  # Calculating Expected Value with Targeted OT
  data_1_tbl <- data_0_tbl %>%
    add_column(Yes = pred_0_tbl$Yes) %>%
    mutate(
      OverTime = case_when(
        Yes >= threshold ~ factor("No", levels = levels(data_0_tbl$OverTime)) ,
        TRUE ~ OverTime
      )
    ) %>%
    select(-Yes)
  
  pred_1_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_1_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime),
      data_1_tbl %>%
        select(OverTime)
    ) %>%
    rename(
      OverTime_0 = OverTime,
      OverTime_1 = OverTime1
    )
  
  avg_overtime_pct <- 0.10
  
  ev_1_tbl <- pred_1_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = 250000
      )
    ) %>%
    mutate(
      cost_of_policy_change = case_when(
        OverTime_0 == "Yes" & OverTime_1 == "No" ~ attrition_cost * avg_overtime_pct,
        TRUE ~ 0
      )
    ) %>%
    mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
    )
  
  total_ev_1_tbl <- ev_1_tbl %>%
    summarise(
      total_expected_attrition_cost_1 = sum(expected_attrition_cost)
    )
  
  # Savings Calculation
  savings_tbl <- bind_cols(
    total_ev_0_tbl,
    total_ev_1_tbl
  ) %>%
    mutate(
      savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
      pct_savings = savings / total_expected_attrition_cost_0
    )
  
  return(savings_tbl$savings)
  
}
```

**Max F1 Score : Threshold @ Max F1 i.e Target employees by balancing
FN’s and FP’s**

Adjusting threshold to max F1 i.e targeting high risk employees which
gives us savings which is bit more than what we had with No OT policy

``` r
# Threshold @ Max F1

rates_by_threshold_tbl %>%
  select(threshold, f1, tnr:tpr) %>%
  filter(f1 == max(f1))
```

    ## # A tibble: 1 x 6
    ##   threshold    f1   tnr   fnr    fpr   tpr
    ##       <dbl> <dbl> <dbl> <dbl>  <dbl> <dbl>
    ## 1     0.324 0.694 0.940 0.306 0.0598 0.694

Threshold @ Max F1 = 0.313

``` r
max_f1_savings <- calculate_savings_by_threshold(test_tbl, automl_leader, 
                                                 threshold = max_f1_tbl$threshold,
                                                 tnr = max_f1_tbl$tnr,
                                                 fnr = max_f1_tbl$fnr,
                                                 fpr = max_f1_tbl$fpr,
                                                 tpr = max_f1_tbl$tpr)
```



``` r
max_f1_savings
```

    ## [1] 413049.3

Savings in this case is $453K with threshold @ Max F1, more than No OT
policy.

**No OT Policy, Threshold = 0**

-   Anyone With OT Targeted, analysis of test set (15% of Total
    Population).
-   $396k / 0.15 = $2.6M per year savings.

``` r
# No OT Policy

test_tbl %>%
  calculate_savings_by_threshold(automl_leader, threshold = 0,
                                 tnr = 0, fnr = 0, tpr = 1, fpr = 1)
```



    ## [1] 396856.1

**Do Nothing Policy i.e threshold = 1**

``` r
 # Do Nothing Policy

test_tbl %>%
  calculate_savings_by_threshold(automl_leader, threshold = 1,
                                 tnr = 1, fnr = 1, tpr = 0, fpr = 0)
```

    ## [1] 0

-   No Savings in this case because threshold is so high that no one is
    targeted (No one has 100% probability of leaving)

**Optimize by Threshold** : **Maximizing Expected ROI**

**Threshold @ Max Savings** : Target employees by weighted analysis of
cost of FN and cost of FP

-   Generally, threshold @ F1 Max is default threshold for most
    classification algorithms
-   But it is not optimized for business case. F1 treats False positives
    & False negatives equally. They are not FN’s are in many cases 3X+
    more costly to organization  
-   **False Negatives** - We predict employee stays but actually leaves,
    we fail to target and do not reduce OT for 100% of attrition cost  
-   **False Positives** - We predict leaves but stays, we target and
    reduce OT for 10-30% of attrition cost  
-   FN’s are more costly than FPs Comparing FNs vs FPs
-   For example: Lets say FN = $100 k , FP = $30K ,FN/FP = 3X more
    costly. In this case, when FN’s = 3X FP’s, lower threshold (lower
    the threshold from 28% to 13% ,catch the people that have 13% or
    more probability of quitting

**Sampling Indices in test data**

> When we perform an iterative process, this often takes a lot of time.
> We can sample the indices to reduce the number of iterations from 220
> (no .of obs in test data) to 20 (sampled data) which reduces our
> iteration time by a factor of 11x.

**Iteratively Applying the function rates\_by\_threshold\_optimized\_tbl
to optimize**

``` r
smpl <-  seq(1, 220, length.out = 20) %>% round(digits = 0)

rates_by_threshold_optimized_tbl <- rates_by_threshold_tbl %>%
  select(threshold, tnr:tpr) %>%
  slice(smpl) %>%
  mutate(
    savings = pmap_dbl(.l = list(
      threshold = threshold,
      tnr = tnr,
      fnr = fnr,
      fpr = fpr,
      tpr = tpr
    ),
    .f = partial(calculate_savings_by_threshold, data = test_tbl, h2o_model = automl_leader)
    )
  )
```

 
``` r
rates_by_threshold_optimized_tbl
```

    ## # A tibble: 20 x 6
    ##    threshold    tnr    fnr     fpr    tpr savings
    ##        <dbl>  <dbl>  <dbl>   <dbl>  <dbl>   <dbl>
    ##  1    0.957  1      0.972  0       0.0278   5420.
    ##  2    0.740  0.995  0.667  0.00543 0.333  256614.
    ##  3    0.456  0.967  0.5    0.0326  0.5    331490.
    ##  4    0.324  0.940  0.306  0.0598  0.694  413049.
    ##  5    0.261  0.880  0.306  0.120   0.694  482191.
    ##  6    0.180  0.821  0.278  0.179   0.722  518469.
    ##  7    0.140  0.772  0.222  0.228   0.778  516766.
    ##  8    0.113  0.712  0.194  0.288   0.806  511841.
    ##  9    0.0956 0.658  0.167  0.342   0.833  499782.
    ## 10    0.0763 0.598  0.139  0.402   0.861  490380.
    ## 11    0.0659 0.549  0.0833 0.451   0.917  474196.
    ## 12    0.0574 0.489  0.0556 0.511   0.944  461748.
    ## 13    0.0524 0.435  0.0278 0.565   0.972  441870.
    ## 14    0.0481 0.375  0      0.625   1      429102.
    ## 15    0.0444 0.315  0      0.685   1      415447.
    ## 16    0.0408 0.25   0      0.75    1      408216.
    ## 17    0.0382 0.190  0      0.810   1      408216.
    ## 18    0.0344 0.125  0      0.875   1      402298.
    ## 19    0.0315 0.0652 0      0.935   1      402298.
    ## 20    0.0296 0      0      1       1      396856.

**Visualizing Optimized Savings**

``` r
rates_by_threshold_optimized_tbl %>%
  ggplot(aes(threshold, savings)) +
  #Vlines
  geom_vline(xintercept = max_f1_tbl$threshold, 
             color = palette_light()[[5]] , size = 2) +
  geom_vline(aes(xintercept = threshold), 
             color = palette_light()[[3]] , size = 2,
            data =  rates_by_threshold_optimized_tbl %>%
              filter(savings ==  max(savings))
  ) +
  # Points
  geom_line(color = palette_light()[[1]]) +
  geom_point(color = palette_light()[[1]]) +
  
  # Optimal Point 
  geom_point(shape = 21, size = 5, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(savings == max(savings))) +
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -1, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(savings == max(savings))) +
  # F1 Max
  geom_vline(xintercept = max_f1_tbl$threshold,
             color = palette_light()[[5]], size = 2) +
  annotate(geom = "label", label = scales::dollar(max_f1_savings),
           x = max_f1_tbl$threshold, y = max_f1_savings, vjust = -1,
           color = palette_light()[[1]]) +
  
  # No OT Policy
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(threshold == min(threshold))) +
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(threshold == min(threshold))) +
  
  # Do Nothing
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(threshold == max(threshold))) +
  geom_label(aes(label = scales::dollar(round(savings,0))), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl %>%
               filter(threshold == max(threshold))) +
  
  # Aesthestics 
  theme_tq() +
  expand_limits(x = c(-.1, 1.1), y = 8e5) + 
  scale_x_continuous(labels  = scales::percent,
                     breaks = seq(0, 1, by = 0.2)) +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Optimization Results : Expected Savings Maximized At 13.3% ",
       x = "Threshold (%)", 
       y = "Savings")
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-111-1.png)

-   Lower the threshold from 31% to 14.2%: Catch the people that have
    14.2% or more probability of quitting.
-   Expected Savings Increases 17.4% ((532 - 453) / 453 \* 100).
-   $79k Savings / 0.15 = $526k per year additional savings for full
    population.
-   Targeting employees with Yes &gt; = 14.2% will maximize expected
    savings i.e **$532,600**.
-   Big Savings : Targeting employees with Yes &gt; = 14.2% , Increased
    savings by 17.4% versus F1.

**Sensitivity Analysis**

**Modelling Assumptions**:

1.  You will never have perfect information and therefore assumptions
    must be made.  
2.  Always test you assumptions: Even though you need to test some
    assumptions, you can still try different values to test your
    assumptions. This is why we do **Sensitivity Analysis**.

``` r
calculate_savings_by_threshold_2 <- function(data, h2o_model, threshold = 0, 
                                             tnr = 0, fpr = 1, fnr = 0, tpr = 1,
                                             avg_overtime_pct = 0.10,
                                             net_revenue_per_employee = 250000) {
  
  data_0_tbl <- as.tibble(data)
  
 
  # Expected Value
  
  # Calculating Expected Value with OT
  
  pred_0_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_0_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime)
    )
  
  ev_0_tbl <- pred_0_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        # changed in _2 -----
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %>%
    mutate(
      cost_of_policy_change = 0
    ) %>%
    mutate(
      expected_attrition_cost = 
        Yes * (attrition_cost + cost_of_policy_change) +
        No  * (cost_of_policy_change)
    )
  
  total_ev_0_tbl <- ev_0_tbl %>%
    summarise(
      total_expected_attrition_cost_0 = sum(expected_attrition_cost)
    )
  
  # Calculating Expected Value with Targeted OT
  
  data_1_tbl <- data_0_tbl %>%
    add_column(Yes = pred_0_tbl$Yes) %>%
    mutate(
      OverTime = case_when(
        Yes >= threshold ~ factor("No", levels = levels(data_0_tbl$OverTime)) ,
        TRUE ~ OverTime
      )
    ) %>%
    select(-Yes) 
  
  pred_1_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_1_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime),
      data_1_tbl %>%
        select(OverTime)
    ) %>%
    rename(
      OverTime_0 = OverTime,
      OverTime_1 = OverTime1
    )
  

  avg_overtime_pct <- avg_overtime_pct 
  
  
  ev_1_tbl <- pred_1_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %>%
    mutate(
      cost_of_policy_change = case_when(
        OverTime_0 == "Yes" & OverTime_1 == "No" ~ attrition_cost * avg_overtime_pct,
        TRUE ~ 0
      )
    ) %>%
    mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
    )
  
  total_ev_1_tbl <- ev_1_tbl %>%
    summarise(
      total_expected_attrition_cost_1 = sum(expected_attrition_cost)
    )
  
  #Savings Calculation
  savings_tbl <- bind_cols(
    total_ev_0_tbl,
    total_ev_1_tbl
  ) %>%
    mutate(
      savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
      pct_savings = savings / total_expected_attrition_cost_0
    )
  
  return(savings_tbl$savings)
  
}
```

``` r
test_tbl %>%
  calculate_savings_by_threshold_2(automl_leader, avg_overtime_pct = 0.10,
                                   net_revenue_per_employee = 250000)
```


    ## [1] 396856.1

Savings are reduced

``` r
test_tbl %>%
  calculate_savings_by_threshold_2(automl_leader, avg_overtime_pct = 0.15,
                                   net_revenue_per_employee = 300000)
```

  
    ## [1] 179246.4

**Sensitivity Analysis for OverTime**

**Classifier Calibration**: This combination of threshold and expected
rates settings has our classifier calibrated to optimum FN / FP Ratio
for max savings. If cost / benefit change, settings need to be
recalibrated (re-optimized)

``` r
max_savings_rates_tbl <- rates_by_threshold_optimized_tbl %>%
  filter(savings == max(savings))

max_savings_rates_tbl
```

    ## # A tibble: 1 x 6
    ##   threshold   tnr   fnr   fpr   tpr savings
    ##       <dbl> <dbl> <dbl> <dbl> <dbl>   <dbl>
    ## 1     0.180 0.821 0.278 0.179 0.722 518469.

``` r
calculate_savings_by_threshold_2(
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_savings_rates_tbl$threshold,
  tnr = max_savings_rates_tbl$tnr,
  fnr =  max_savings_rates_tbl$fnr,
  fpr = max_savings_rates_tbl$fpr,
  tpr =  max_savings_rates_tbl$tpr
)
```

 
    ## [1] 518469.5

``` r
# Preloaded Function : Preloads the calibrated settings that optimize threshold &  maximize expected savings 

calculate_savings_by_threshold_2_preloaded <- partial(
  calculate_savings_by_threshold_2,
  # Function Arguments
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_savings_rates_tbl$threshold,
  tnr = max_savings_rates_tbl$tnr,
  fnr =  max_savings_rates_tbl$fnr,
  fpr = max_savings_rates_tbl$fpr,
  tpr =  max_savings_rates_tbl$tpr
  
)


calculate_savings_by_threshold_2_preloaded(
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000)
```

  
    ## [1] 518469.5

Multiple Combinations: We have two inputs that we are simultaneously
going to change:

1.  **Average OverTime Percent** : If an employee works 100% they
    essentially double their hours. We don’t expect this to be average.
    Rather, it’s likely that the worst case is 30% or roughly 30% of 40
    = 12 hours per week

2.  **Net Revenue per Employee(NRPE)** : On an income statement, take
    the gross revenue minus Costs of Goods sold to get Net Revenue. They
    spread this out across every employee to get an estimate of their
    financial value to organization.

**Worst Case**: We believe $200,000 for lowest NRPE  
**Best Case**: We believe $400,000 for highest NRPE

``` r
sensitivity_tbl <- list(
  avg_overtime_pct = seq(0.05, 0.30, by = 0.05), 
  net_revenue_per_employee = seq(200000, 400000, by = 50000)
) %>%
  cross_df() %>%
  mutate(
    savings = pmap_dbl(
      .l = list(
        avg_overtime_pct = avg_overtime_pct,
        net_revenue_per_employee = net_revenue_per_employee
      ),
      .f = calculate_savings_by_threshold_2_preloaded
    )
  )
```

  
``` r
sensitivity_tbl
```

    ## # A tibble: 30 x 3
    ##    avg_overtime_pct net_revenue_per_employee savings
    ##               <dbl>                    <dbl>   <dbl>
    ##  1             0.05                   200000 534281.
    ##  2             0.1                    200000 428535.
    ##  3             0.15                   200000 322789.
    ##  4             0.2                    200000 217042.
    ##  5             0.25                   200000 111296.
    ##  6             0.3                    200000   5549.
    ##  7             0.05                   250000 646820.
    ##  8             0.1                    250000 518469.
    ##  9             0.15                   250000 390119.
    ## 10             0.2                    250000 261768.
    ## # … with 20 more rows

**Heat Maps** are a great way to show how two variables interact with a
third variable (the target variable)

``` r
sensitivity_tbl %>%
  ggplot(aes(avg_overtime_pct, net_revenue_per_employee)) +
  geom_tile(aes(fill = savings)) +
  geom_label(aes(label = savings %>% round(0) %>% scales::dollar())) +
  theme_tq() +
  theme(legend.position = "none") +
  scale_fill_gradient2(
    low = palette_light()[[2]],
    mid = "white",
    high = palette_light()[[1]],
    midpoint = 0
  ) + 
  scale_x_continuous(
    labels = scales::percent,
    breaks = seq(0.05, 0.30, by = 0.05)
  ) +
  scale_y_continuous(
    labels = scales::dollar
  ) +
  labs(
    title = "Profitability Heatmap : Expected Savings Sensitivity Analysis ",
    subtitle = "How sensitive is savings to net revenue per employee and average overtime percentage ?",
    x = "Average Overtime Percentage",
    y = "Net Revenue Per Employee"
  )
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-117-1.png)

As long as people are not working OverTime more than 25%, we are break
even that is profit.

**Threshold Optimization For Stock Options**

**Challenge : People with no stock options are leaving**

**Find optimal threshold**

``` r
avg_overtime_pct <- 0.10
net_revenue_per_employee <- 250000
stock_option_cost <- 5000

data <- test_tbl 
h2o_model <- automl_leader

calculate_savings_by_threshold_3 <- function(data, h2o_model, threshold = 0, 
                                             tnr = 0, fpr = 1, fnr = 0, tpr = 1,
                                             avg_overtime_pct = 0.10,
                                             net_revenue_per_employee = 250000,
                                             stock_option_cost = 5000) {
  
  data_0_tbl <- as.tibble(data)
  
  # Expected Value
  
  # Calculating Expected Value with OT
  
  pred_0_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_0_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime, StockOptionLevel)
    )
  
  ev_0_tbl <- pred_0_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %>%
    mutate(
      cost_of_policy_change = 0
    ) %>%
    mutate(
      expected_attrition_cost = 
        Yes * (attrition_cost + cost_of_policy_change) +
        No  * (cost_of_policy_change)
    )
  
  total_ev_0_tbl <- ev_0_tbl %>%
    summarise(
      total_expected_attrition_cost_0 = sum(expected_attrition_cost)
    )
  
  # Calculating Expected Value with Targeted OT & Stock Option Policy
  
  data_1_tbl <- data_0_tbl %>%
    add_column(Yes = pred_0_tbl$Yes) %>%
    mutate(
      OverTime = case_when(
        Yes >= threshold ~ factor("No", levels = levels(data_0_tbl$OverTime)) ,
        TRUE ~ OverTime
      )
    ) %>%
    mutate(
      StockOptionLevel = case_when(
        Yes >= threshold & StockOptionLevel == 0 
        ~ factor("1", levels = levels(data_0_tbl$StockOptionLevel)) ,
        TRUE ~ StockOptionLevel
      )
    ) %>%
    select(-Yes) 
  
  pred_1_tbl <- h2o_model %>%
    h2o.predict(newdata = as.h2o(data_1_tbl)) %>%
    as.tibble() %>%
    bind_cols(
      data_0_tbl %>%
        select(EmployeeNumber, MonthlyIncome, OverTime, StockOptionLevel),
      data_1_tbl %>%
        select(OverTime, StockOptionLevel)
    ) %>%
    rename(
      OverTime_0 = OverTime,
      OverTime_1 = OverTime1,
      StockOptionLevel_0 = StockOptionLevel,
      StockOptionLevel_1 = StockOptionLevel1   
    )
  

  avg_overtime_pct <- avg_overtime_pct 
  stock_option_cost <- stock_option_cost 
  
  
  ev_1_tbl <- pred_1_tbl %>%
    mutate(
      attrition_cost = calculate_attrition_cost(
        n = 1,
        salary = MonthlyIncome * 12,
        net_revenue_per_employee = net_revenue_per_employee
      )
    ) %>%
    # cost OT
    mutate(
      cost_OT = case_when(
        OverTime_0 == "Yes" & OverTime_1 == "No"
        ~  avg_overtime_pct * MonthlyIncome * 12,
        TRUE ~ 0
      )
    ) %>%
    # cost Stock Option
    mutate(
      cost_SO = case_when(
        StockOptionLevel_1 == "1" & StockOptionLevel_0 == "0"
        ~ stock_option_cost,
        TRUE ~ 0
      )
    ) %>%
    mutate(cost_of_policy_change = cost_OT + cost_SO) %>%
    mutate(
      cb_tn = cost_of_policy_change,
      cb_fp = cost_of_policy_change,
      cb_tp = cost_of_policy_change + attrition_cost,
      cb_fn = cost_of_policy_change + attrition_cost,
      expected_attrition_cost = 
        Yes * (tpr * cb_tp + fnr * cb_fn) +
        No *  (tnr * cb_tn + fpr * cb_fp)
    )
  
  total_ev_1_tbl <- ev_1_tbl %>%
    summarise(
      total_expected_attrition_cost_1 = sum(expected_attrition_cost)
    )
  
  # Savings Calculation
  savings_tbl <- bind_cols(
    total_ev_0_tbl,
    total_ev_1_tbl
  ) %>%
    mutate(
      savings = total_expected_attrition_cost_0 -  total_expected_attrition_cost_1,
      pct_savings = savings / total_expected_attrition_cost_0
    )
  
  return(savings_tbl$savings)
  
}
```

``` r
max_f1_tbl <- rates_by_threshold_tbl %>%
  select(threshold, f1, tnr:tpr) %>%
  filter(f1 == max(f1))

max_f1_savings <- calculate_savings_by_threshold_3(
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_f1_tbl$threshold,
  tnr = max_f1_tbl$tnr,
  fnr =  max_f1_tbl$fnr,
  fpr = max_f1_tbl$fpr,
  tpr =  max_f1_tbl$tpr,
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000,
  stock_option_cost = 5000
)
```

   
**Optimisation**

``` r
smpl <- seq(1, 220, length.out = 20) %>% round(digits = 0)
  
calculate_savings_by_threshold_3_preloaded <- partial(
  calculate_savings_by_threshold_3,
  # Function Arguments
  data = test_tbl,
  h2o_model = automl_leader,
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000,
  stock_option_cost = 5000)

rates_by_threshold_optimized_tbl_3 <- rates_by_threshold_tbl %>%
  select(threshold, tnr:tpr) %>%
  slice(smpl) %>%
  mutate(
    savings = pmap_dbl(.l = list(
      threshold = threshold,
      tnr = tnr,
      fnr = fnr,
      fpr = fpr,
      tpr = tpr
    ),
    .f = calculate_savings_by_threshold_3_preloaded
    )
  )
```

 
``` r
rates_by_threshold_optimized_tbl_3
```

    ## # A tibble: 20 x 6
    ##    threshold    tnr    fnr     fpr    tpr  savings
    ##        <dbl>  <dbl>  <dbl>   <dbl>  <dbl>    <dbl>
    ##  1    0.957  1      0.972  0       0.0278   31319.
    ##  2    0.740  0.995  0.667  0.00543 0.333   469050.
    ##  3    0.456  0.967  0.5    0.0326  0.5     709481.
    ##  4    0.324  0.940  0.306  0.0598  0.694   909679.
    ##  5    0.261  0.880  0.306  0.120   0.694  1025848.
    ##  6    0.180  0.821  0.278  0.179   0.722  1052881.
    ##  7    0.140  0.772  0.222  0.228   0.778  1061113.
    ##  8    0.113  0.712  0.194  0.288   0.806  1044772.
    ##  9    0.0956 0.658  0.167  0.342   0.833  1027305.
    ## 10    0.0763 0.598  0.139  0.402   0.861   997224.
    ## 11    0.0659 0.549  0.0833 0.451   0.917   952199.
    ## 12    0.0574 0.489  0.0556 0.511   0.944   920897.
    ## 13    0.0524 0.435  0.0278 0.565   0.972   894789.
    ## 14    0.0481 0.375  0      0.625   1       867870.
    ## 15    0.0444 0.315  0      0.685   1       832960.
    ## 16    0.0408 0.25   0      0.75    1       804168.
    ## 17    0.0382 0.190  0      0.810   1       804168.
    ## 18    0.0344 0.125  0      0.875   1       786774.
    ## 19    0.0315 0.0652 0      0.935   1       786774.
    ## 20    0.0296 0      0      1       1       753165.

``` r
rates_by_threshold_optimized_tbl_3 %>%
  filter(savings == max(savings))
```

    ## # A tibble: 1 x 6
    ##   threshold   tnr   fnr   fpr   tpr  savings
    ##       <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>
    ## 1     0.140 0.772 0.222 0.228 0.778 1061113.

``` r
rates_by_threshold_optimized_tbl_3 %>%
  ggplot(aes(threshold, savings)) +
  
  #Vlines
  geom_vline(xintercept = max_f1_tbl$threshold, 
             color = palette_light()[[5]] , size = 2) +
  geom_vline(aes(xintercept = threshold), 
             color = palette_light()[[3]] , size = 2,
            data =  rates_by_threshold_optimized_tbl_3 %>%
              filter(savings ==  max(savings))
  ) +
  # Points
  geom_line(color = palette_light()[[1]]) +
  geom_point(color = palette_light()[[1]]) +
  
  # F1 Max
  annotate(geom = "label", label = scales::dollar(max_f1_savings),
           x = max_f1_tbl$threshold, y = max_f1_savings, vjust = -1,
           color = palette_light()[[1]]) +
  
  # Optimal Point 
  geom_point(shape = 21, size = 5, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(savings == max(savings))) +
  
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -2, color = palette_light()[[3]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(savings == max(savings))) +

  # No OT Policy
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(threshold == min(threshold))) +
  geom_label(aes(label = scales::dollar(savings)), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(threshold == min(threshold))) +
  
  # Do Nothing
  geom_point(shape = 21, size = 5, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(threshold == max(threshold))) +
  geom_label(aes(label = scales::dollar(round(savings,0))), 
             vjust = -1, color = palette_light()[[2]],
             data = rates_by_threshold_optimized_tbl_3 %>%
               filter(threshold == max(threshold))) +
  
  # Aesthestics 
  theme_tq() +
  expand_limits(x = c(-.1, 1.1), y = 12e5) + 
  scale_x_continuous(labels  = scales::percent,
                     breaks = seq(0, 1, by = 0.2)) +
  scale_y_continuous(labels = scales::dollar) +
  labs(title = "Optimization Results : Expected Savings Maximized At 26.7% ",
       x = "Threshold (%)", 
       y = "Savings")
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-122-1.png)

**Sensitivity Analysis at Optimal Threshold**

``` r
net_revenue_per_employee <- 250000
avg_overtime_pct <- seq(0.05, 0.30, by = 0.05)
stock_option_cost <- seq(5000, 25000, by = 5000)

max_savings_rates_tbl_3 <- rates_by_threshold_optimized_tbl_3 %>%
filter(savings == max(savings))

max_savings_rates_tbl_3
```

    ## # A tibble: 1 x 6
    ##   threshold   tnr   fnr   fpr   tpr  savings
    ##       <dbl> <dbl> <dbl> <dbl> <dbl>    <dbl>
    ## 1     0.140 0.772 0.222 0.228 0.778 1061113.

``` r
calculate_savings_by_threshold_3_preloaded <- partial(
  calculate_savings_by_threshold_3,
  # Function Arguments
  data = test_tbl,
  h2o_model = automl_leader,
  threshold = max_savings_rates_tbl$threshold,
  tnr = max_savings_rates_tbl_3$tnr,
  fnr =  max_savings_rates_tbl_3$fnr,
  fpr = max_savings_rates_tbl_3$fpr,
  tpr =  max_savings_rates_tbl_3$tpr
  
)
```

``` r
calculate_savings_by_threshold_3_preloaded(
  avg_overtime_pct = 0.10,
  net_revenue_per_employee = 250000,
  stock_option_cost = 5000)  
```

  
    ## [1] 1052881

``` r
sensitivity_tbl_3 <- list(
  avg_overtime_pct  = seq(0.05, 0.30, by = 0.05),
  net_revenue_per_employee = 250000,
  stock_option_cost = seq(5000, 25000, by = 5000)
) %>%
  cross_df() %>%
  mutate(
    savings = pmap_dbl(
      .l = list(
        avg_overtime_pct = avg_overtime_pct,
        net_revenue_per_employee = net_revenue_per_employee,
        stock_option_cost = stock_option_cost
      ),
      .f = calculate_savings_by_threshold_3_preloaded
    )
  )
```

   
``` r
sensitivity_tbl_3
```

    ## # A tibble: 30 x 4
    ##    avg_overtime_pct net_revenue_per_employee stock_option_cost  savings
    ##               <dbl>                    <dbl>             <dbl>    <dbl>
    ##  1             0.05                   250000              5000 1136672.
    ##  2             0.1                    250000              5000 1052881.
    ##  3             0.15                   250000              5000  969090.
    ##  4             0.2                    250000              5000  885299.
    ##  5             0.25                   250000              5000  801508.
    ##  6             0.3                    250000              5000  717716.
    ##  7             0.05                   250000             10000  911672.
    ##  8             0.1                    250000             10000  827881.
    ##  9             0.15                   250000             10000  744090.
    ## 10             0.2                    250000             10000  660299.
    ## # … with 20 more rows

``` r
sensitivity_tbl_3 %>%
  ggplot(aes(avg_overtime_pct, stock_option_cost)) +
  geom_tile(aes(fill = savings)) +
  geom_label(aes(label = savings %>% round(0) %>% scales::dollar())) +
  theme_tq() +
  theme(legend.position = "none") +
  scale_fill_gradient2(
    low = palette_light()[[2]],
    mid = "white",
    high = palette_light()[[1]],
    midpoint = 0
  ) + 
  scale_x_continuous(
    labels = scales::percent,
    breaks = seq(0.05, 0.30, by = 0.05)
  ) +
  scale_y_continuous(
    labels = scales::dollar,
    breaks = seq(5000, 25000, by = 5000)
  ) +
  labs(
    title = "Profitability Heatmap : Expected Savings Sensitivity Analysis ",
    subtitle = "How sensitive is savings to stock options cost and average overtime percentage ?",
    x = "Average Overtime Percentage",
    y = "Average Stock Options Cost"
  )
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-127-1.png)

As longs as average overtime percentage is less than 18% and avg stocks
options cost less than $20,000, we are break even that is profit.

**Recommendation Algorithm Strategy**

**Purpose** To enable immediate manager to reduce attrition using an
automated recommendation system

**Strategy Logic:**

-   Uses Correlation Analysis to determine logical strategies that can
    be adjusted via stakeholder feedback  
-   Investigate your strategies and try to put them into 2 or 3 groups.
    This helps managers focus on specific areas.

! \[Alt text\] (/Untitled/Users/raj/Pictures/Snip20190109\_2.png⁩)

**Strategy Groups**

-   **Work Life** : OverTime, BusinessTravel
-   **Personal Development** : Traning & Mentoring Employees, Sponsoring
    Higher Education
-   **Professional Development**: Promotion based on experience & Job
    level

**Recipes for Correlation Analysis**

After applying recipe, we get a discretized Output i.e all are binary
features 0’s and 1’s only. This discretization of features will help us
in identifying strategies.

``` r
recipe_obj <- recipe(Attrition ~ . , data = train_readable_tbl) %>%
  step_zv(all_predictors()) %>%
  step_num2factor(factor_names) %>%
  step_discretize(all_numeric(), options = list(min_unique = 1)) %>%
  step_dummy(all_nominal(), one_hot = TRUE) %>%
  prep()
```

``` r
train_corr_tbl <- bake(recipe_obj, new_data = train_readable_tbl)
train_corr_tbl %>% glimpse()
```

    ## Observations: 1,250
    ## Variables: 141
    ## $ Age_bin_missing                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ Age_bin1                            <dbl> 0, 0, 0, 0, 1, 0, 0, 1, 0, 0…
    ## $ Age_bin2                            <dbl> 0, 0, 0, 1, 0, 1, 0, 0, 0, 1…
    ## $ Age_bin3                            <dbl> 1, 0, 1, 0, 0, 0, 0, 0, 1, 0…
    ## $ Age_bin4                            <dbl> 0, 1, 0, 0, 0, 0, 1, 0, 0, 0…
    ## $ BusinessTravel_Non.Travel           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ BusinessTravel_Travel_Rarely        <dbl> 1, 0, 1, 0, 1, 0, 1, 1, 0, 1…
    ## $ BusinessTravel_Travel_Frequently    <dbl> 0, 1, 0, 1, 0, 1, 0, 0, 1, 0…
    ## $ DailyRate_bin_missing               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ DailyRate_bin1                      <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 1, 0…
    ## $ DailyRate_bin2                      <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…
    ## $ DailyRate_bin3                      <dbl> 1, 0, 0, 0, 0, 1, 0, 0, 0, 0…
    ## $ DailyRate_bin4                      <dbl> 0, 0, 1, 1, 0, 0, 1, 1, 0, 1…
    ## $ Department_Human.Resources          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ Department_Research...Development   <dbl> 0, 1, 1, 1, 1, 1, 1, 1, 1, 1…
    ## $ Department_Sales                    <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ DistanceFromHome_bin_missing        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ DistanceFromHome_bin1               <dbl> 1, 0, 1, 0, 1, 1, 0, 0, 0, 0…
    ## $ DistanceFromHome_bin2               <dbl> 0, 0, 0, 1, 0, 0, 1, 0, 0, 0…
    ## $ DistanceFromHome_bin3               <dbl> 0, 1, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ DistanceFromHome_bin4               <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 1, 1…
    ## $ Education_Below.College             <dbl> 0, 1, 0, 0, 1, 0, 0, 1, 0, 0…
    ## $ Education_College                   <dbl> 1, 0, 1, 0, 0, 1, 0, 0, 0, 0…
    ## $ Education_Bachelor                  <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 1, 1…
    ## $ Education_Master                    <dbl> 0, 0, 0, 1, 0, 0, 0, 0, 0, 0…
    ## $ Education_Doctor                    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ EducationField_Human.Resources      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ EducationField_Life.Sciences        <dbl> 1, 1, 0, 1, 0, 1, 0, 1, 1, 0…
    ## $ EducationField_Marketing            <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ EducationField_Medical              <dbl> 0, 0, 0, 0, 1, 0, 1, 0, 0, 1…
    ## $ EducationField_Other                <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…
    ## $ EducationField_Technical.Degree     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ EmployeeNumber_bin_missing          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ EmployeeNumber_bin1                 <dbl> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1…
    ## $ EmployeeNumber_bin2                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ EmployeeNumber_bin3                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ EmployeeNumber_bin4                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ EnvironmentSatisfaction_Low         <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 0, 0…
    ## $ EnvironmentSatisfaction_Medium      <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ EnvironmentSatisfaction_High        <dbl> 0, 1, 0, 0, 0, 0, 1, 0, 0, 1…
    ## $ EnvironmentSatisfaction_Very.High   <dbl> 0, 0, 1, 1, 0, 1, 0, 1, 1, 0…
    ## $ Gender_Female                       <dbl> 1, 0, 0, 1, 0, 0, 1, 0, 0, 0…
    ## $ Gender_Male                         <dbl> 0, 1, 1, 0, 1, 1, 0, 1, 1, 1…
    ## $ HourlyRate_bin_missing              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ HourlyRate_bin1                     <dbl> 0, 0, 0, 0, 1, 0, 0, 0, 1, 0…
    ## $ HourlyRate_bin2                     <dbl> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0…
    ## $ HourlyRate_bin3                     <dbl> 0, 0, 0, 0, 0, 1, 1, 1, 0, 0…
    ## $ HourlyRate_bin4                     <dbl> 1, 0, 1, 0, 0, 0, 0, 0, 0, 1…
    ## $ JobInvolvement_Low                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ JobInvolvement_Medium               <dbl> 0, 1, 1, 0, 0, 0, 0, 0, 1, 0…
    ## $ JobInvolvement_High                 <dbl> 1, 0, 0, 1, 1, 1, 0, 1, 0, 1…
    ## $ JobInvolvement_Very.High            <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…
    ## $ JobLevel_X1                         <dbl> 0, 0, 1, 1, 1, 1, 1, 1, 0, 0…
    ## $ JobLevel_X2                         <dbl> 1, 1, 0, 0, 0, 0, 0, 0, 0, 1…
    ## $ JobLevel_X3                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…
    ## $ JobLevel_X4                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ JobLevel_X5                         <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ JobRole_Healthcare.Representative   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…
    ## $ JobRole_Human.Resources             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ JobRole_Laboratory.Technician       <dbl> 0, 0, 1, 0, 1, 1, 1, 1, 0, 0…
    ## $ JobRole_Manager                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ JobRole_Manufacturing.Director      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…
    ## $ JobRole_Research.Director           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ JobRole_Research.Scientist          <dbl> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0…
    ## $ JobRole_Sales.Executive             <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ JobRole_Sales.Representative        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ JobSatisfaction_Low                 <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…
    ## $ JobSatisfaction_Medium              <dbl> 0, 1, 0, 0, 1, 0, 0, 0, 0, 0…
    ## $ JobSatisfaction_High                <dbl> 0, 0, 1, 1, 0, 0, 0, 1, 1, 1…
    ## $ JobSatisfaction_Very.High           <dbl> 1, 0, 0, 0, 0, 1, 0, 0, 0, 0…
    ## $ MaritalStatus_Single                <dbl> 1, 0, 1, 0, 0, 1, 0, 0, 1, 0…
    ## $ MaritalStatus_Married               <dbl> 0, 1, 0, 1, 1, 0, 1, 0, 0, 1…
    ## $ MaritalStatus_Divorced              <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0…
    ## $ MonthlyIncome_bin_missing           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ MonthlyIncome_bin1                  <dbl> 0, 0, 1, 1, 0, 0, 1, 1, 0, 0…
    ## $ MonthlyIncome_bin2                  <dbl> 0, 0, 0, 0, 1, 1, 0, 0, 0, 0…
    ## $ MonthlyIncome_bin3                  <dbl> 1, 1, 0, 0, 0, 0, 0, 0, 0, 1…
    ## $ MonthlyIncome_bin4                  <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…
    ## $ MonthlyRate_bin_missing             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ MonthlyRate_bin1                    <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…
    ## $ MonthlyRate_bin2                    <dbl> 0, 0, 0, 0, 0, 1, 1, 1, 1, 0…
    ## $ MonthlyRate_bin3                    <dbl> 1, 0, 0, 0, 1, 0, 0, 0, 0, 1…
    ## $ MonthlyRate_bin4                    <dbl> 0, 1, 0, 1, 0, 0, 0, 0, 0, 0…
    ## $ NumCompaniesWorked_bin_missing      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ NumCompaniesWorked_bin1             <dbl> 0, 1, 0, 1, 0, 1, 0, 1, 1, 0…
    ## $ NumCompaniesWorked_bin2             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ NumCompaniesWorked_bin3             <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…
    ## $ NumCompaniesWorked_bin4             <dbl> 1, 0, 1, 0, 1, 0, 0, 0, 0, 1…
    ## $ OverTime_No                         <dbl> 0, 1, 0, 0, 1, 1, 0, 1, 1, 1…
    ## $ OverTime_Yes                        <dbl> 1, 0, 1, 1, 0, 0, 1, 0, 0, 0…
    ## $ PercentSalaryHike_bin_missing       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ PercentSalaryHike_bin1              <dbl> 1, 0, 0, 1, 1, 0, 0, 0, 0, 0…
    ## $ PercentSalaryHike_bin2              <dbl> 0, 0, 0, 0, 0, 1, 0, 0, 0, 1…
    ## $ PercentSalaryHike_bin3              <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0…
    ## $ PercentSalaryHike_bin4              <dbl> 0, 1, 0, 0, 0, 0, 1, 1, 1, 0…
    ## $ PerformanceRating_Low               <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ PerformanceRating_Good              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ PerformanceRating_Excellent         <dbl> 1, 0, 1, 1, 1, 1, 0, 0, 0, 1…
    ## $ PerformanceRating_Outstanding       <dbl> 0, 1, 0, 0, 0, 0, 1, 1, 1, 0…
    ## $ RelationshipSatisfaction_Low        <dbl> 1, 0, 0, 0, 0, 0, 1, 0, 0, 0…
    ## $ RelationshipSatisfaction_Medium     <dbl> 0, 0, 1, 0, 0, 0, 0, 1, 1, 1…
    ## $ RelationshipSatisfaction_High       <dbl> 0, 0, 0, 1, 0, 1, 0, 0, 0, 0…
    ## $ RelationshipSatisfaction_Very.High  <dbl> 0, 1, 0, 0, 1, 0, 0, 0, 0, 0…
    ## $ StockOptionLevel_X0                 <dbl> 1, 0, 1, 1, 0, 1, 0, 0, 1, 0…
    ## $ StockOptionLevel_X1                 <dbl> 0, 1, 0, 0, 1, 0, 0, 1, 0, 0…
    ## $ StockOptionLevel_X2                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…
    ## $ StockOptionLevel_X3                 <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…
    ## $ TotalWorkingYears_bin_missing       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ TotalWorkingYears_bin1              <dbl> 0, 0, 0, 0, 1, 0, 0, 1, 0, 0…
    ## $ TotalWorkingYears_bin2              <dbl> 1, 1, 1, 1, 0, 1, 0, 0, 1, 0…
    ## $ TotalWorkingYears_bin3              <dbl> 0, 0, 0, 0, 0, 0, 1, 0, 0, 0…
    ## $ TotalWorkingYears_bin4              <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…
    ## $ TrainingTimesLastYear_bin_missing   <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ TrainingTimesLastYear_bin1          <dbl> 1, 0, 0, 0, 0, 1, 0, 1, 1, 0…
    ## $ TrainingTimesLastYear_bin2          <dbl> 0, 1, 1, 1, 1, 0, 1, 0, 0, 1…
    ## $ TrainingTimesLastYear_bin3          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ WorkLifeBalance_Bad                 <dbl> 1, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ WorkLifeBalance_Good                <dbl> 0, 0, 0, 0, 0, 1, 1, 0, 0, 1…
    ## $ WorkLifeBalance_Better              <dbl> 0, 1, 1, 1, 1, 0, 0, 1, 1, 0…
    ## $ WorkLifeBalance_Best                <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ YearsAtCompany_bin_missing          <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ YearsAtCompany_bin1                 <dbl> 0, 0, 1, 0, 1, 0, 1, 1, 0, 0…
    ## $ YearsAtCompany_bin2                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ YearsAtCompany_bin3                 <dbl> 1, 1, 0, 1, 0, 1, 0, 0, 1, 1…
    ## $ YearsAtCompany_bin4                 <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ YearsInCurrentRole_bin_missing      <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ YearsInCurrentRole_bin1             <dbl> 0, 0, 1, 0, 1, 0, 1, 1, 0, 0…
    ## $ YearsInCurrentRole_bin2             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ YearsInCurrentRole_bin3             <dbl> 1, 1, 0, 1, 0, 1, 0, 0, 1, 1…
    ## $ YearsInCurrentRole_bin4             <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ YearsSinceLastPromotion_bin_missing <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ YearsSinceLastPromotion_bin1        <dbl> 1, 1, 1, 0, 0, 0, 1, 1, 1, 0…
    ## $ YearsSinceLastPromotion_bin2        <dbl> 0, 0, 0, 1, 1, 1, 0, 0, 0, 0…
    ## $ YearsSinceLastPromotion_bin3        <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 1…
    ## $ YearsWithCurrManager_bin_missing    <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ YearsWithCurrManager_bin1           <dbl> 0, 0, 1, 1, 1, 0, 1, 1, 0, 0…
    ## $ YearsWithCurrManager_bin2           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0…
    ## $ YearsWithCurrManager_bin3           <dbl> 1, 1, 0, 0, 0, 1, 0, 0, 0, 1…
    ## $ YearsWithCurrManager_bin4           <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 1, 0…
    ## $ Attrition_No                        <dbl> 0, 1, 0, 1, 1, 1, 1, 1, 1, 1…
    ## $ Attrition_Yes                       <dbl> 1, 0, 1, 0, 0, 0, 0, 0, 0, 0…

**Data Manipulation for Correlation Visualization**

``` r
cor_level <- 0.06

correlation_results_tbl <- train_corr_tbl %>%
  select(-Attrition_No) %>%
  get_cor(Attrition_Yes, fct_reorder = TRUE, fct_rev = TRUE) %>%
  filter(abs(Attrition_Yes) >= cor_level) %>%
  mutate(
    relationship = case_when(
      Attrition_Yes > 0 ~ "Supports",
      TRUE ~ "Contradicts"
    )
  ) %>%
  mutate(feature_text = as.character(feature)) %>%
  separate(feature_text, into = "feature_base", sep = "_", extra = "drop") %>%
  mutate(feature_base = as_factor(feature_base) %>% fct_rev())


length_unique_groups <- correlation_results_tbl %>%
  pull(feature_base) %>% 
  unique() %>%
  length()
```

``` r
correlation_results_tbl %>%
  ggplot(aes(Attrition_Yes, feature_base, color = relationship)) +
  geom_point() + 
  geom_label(aes(label = feature), vjust = -0.5) +
  expand_limits(x = c(-0.3, 0.3), y = c(1,length_unique_groups + 1)) +
  theme_tq() +
  scale_color_tq() +
  labs(
    title = "Correlation Analysis : Recommendation Strategy Development",
    subtitle =  "Discretizing feature to help identify a strategy"
    )
```

![](https://raw.githubusercontent.com/rajkstats/rajkstats.rbind.io/master/static/post/2019-01-04-automl_files/figure-html/unnamed-chunk-131-1.png)

**Recommendation Strategy Development**

-   Implementing Strategies into code  
-   We will be using **Good Better Best Approach** for building a
    recommendation strategy development

Function **recommend\_strategies** evaluates recommendation startegies
given an employee number

``` r
recommend_strategies <- function(data, employee_number){
  
  data %>%
    filter(EmployeeNumber == employee_number) %>%
    mutate_if(is.factor, as.numeric) %>%
    
    # Personal Development Strategy 
    mutate(
      personal_development_strategy =  case_when(
        
        # (Worst Case) Create Personal Development Plan: Job Involvement, JobSatisfaction, PerformanceRating 
        PerformanceRating == 1 | 
          JobSatisfaction == 1 |
          JobInvolvement  <= 2      ~  "Create Personal Development Plan",
        # (Better Case) Promote Training & Formation: YearsAtCompany, TotalWorkingYears 
        YearsAtCompany < 3 |
          TotalWorkingYears < 6    ~  "Promote Training & Formation" ,
        
        # (Best Case 1) Seek Mentorship Role: YearsInCurrentRole, YearsAtCompany, PerformanceRating, JobSatisfaction
        (YearsInCurrentRole > 3 |  YearsAtCompany >= 5) & 
          PerformanceRating >= 3 & 
          JobSatisfaction == 4  ~  "Seek Mentorship Role",
        
        
        # (Best Case 2) Seek Leadership Role : JobInvolvement, JobSatisfaction, PerformanceRating
        JobInvolvement >= 3 & 
          JobSatisfaction >= 3 &
          PerformanceRating>= 3  ~ "Seek Leadership Role",
        
        # Catch All
        TRUE ~ "Retain and Maintain"
      )
    ) %>%
    # select(EmployeeNumber, personal_development_strategy)
  
  
    # Professional Development Strategy
  mutate(
    professional_development_strategy =  case_when(
      
      #  Ready for Rotation: YearsInCurrentRole,  JobSatisfaction (LOW)
      YearsInCurrentRole >= 2 &
        JobSatisfaction <= 2                 ~"Ready for Rotation",
      
      #  Ready for Promotion Level 2: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 1 &
        YearsInCurrentRole >=2 &
        JobInvolvement >=3 &
        PerformanceRating >=3                 ~ "Ready for Promotion",   
      
      #  Ready for Promotion Level 3: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 2 &
        YearsInCurrentRole >=2 &
        JobInvolvement >=4 &
        PerformanceRating >=3                 ~ "Ready for Promotion",   
      
      #  Ready for Promotion Level 4: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 3 &
        YearsInCurrentRole >=3 &
        JobInvolvement >=4 &
        PerformanceRating >=3                 ~ "Ready for Promotion",   
      
      #  Ready for Promotion Level 5: JobLevel, YearsInCurrentRole, JobInvolvement, PerformanceRating
      JobLevel == 4 &
        YearsInCurrentRole >=4 &
        JobInvolvement >=4 &
        PerformanceRating >=3                 ~ "Ready for Promotion",   
      
      #  Incentivize Specialization : YearsInCurrentRole, JobSatisfaction, PerformanceRating
      YearsInCurrentRole >= 4 & 
        JobSatisfaction >= 4 &
        PerformanceRating >= 3         ~ "Incentivize Specialization",
      
      # Catch All
      TRUE ~ "Retain and Maintain"
    )
  ) %>%
  #select(EmployeeNumber, personal_development_strategy,professional_development_strategy)  
    
   # Work-Environment Strategy
    mutate(
      work_environment_strategy =  case_when(
        
        # Improve Work - Life Balance : OverTime, WorkLifeBalance
        OverTime == 2 | 
          WorkLifeBalance == 1                 ~ "Improve Work-Life Balance",
        
        # Monitor Business Travel: BusinessTravel, DistanceFromHome, WorkLifeBalance    
        ( BusinessTravel == 3|
            DistanceFromHome >= 10) &
          WorkLifeBalance == 2                 ~ "Monitor Business Travel",
        
        
        # Review Job Assignment: EnvironmentSatisfaction, YearsInCurrentRole 
        EnvironmentSatisfaction == 1 &
          YearsInCurrentRole >= 2             ~ "Review Job Assignment",
        
        # Promote Job Engagement: JobInvolvement
        JobInvolvement <= 2   ~ "Promote Job Engagement",
        
        # Catch All
        TRUE ~ "Retain and Maintain"
      )
    ) %>%
    select(EmployeeNumber, personal_development_strategy,professional_development_strategy, work_environment_strategy)
}
```

Now, we will evaluate strategies for employees both in training and test
data.

Recommendation strategies for *Employee number 4* from training data:

``` r
train_readable_tbl %>%
  recommend_strategies(4)
```

    ## # A tibble: 1 x 4
    ##   EmployeeNumber personal_development_strategy   
    ##            <dbl> <chr>                           
    ## 1              4 Create Personal Development Plan
    ##   professional_development_strategy work_environment_strategy
    ##   <chr>                             <chr>                    
    ## 1 Retain and Maintain               Improve Work-Life Balance

Recommendation strategies for *Employee number 228* from test data :

``` r
test_readable_tbl %>%
  recommend_strategies(228)
```

    ## # A tibble: 1 x 4
    ##   EmployeeNumber personal_development_strategy
    ##            <dbl> <chr>                        
    ## 1            228 Seek Mentorship Role         
    ##   professional_development_strategy work_environment_strategy
    ##   <chr>                             <chr>                    
    ## 1 Incentivize Specialization        Retain and Maintain

### Phase 6 : Deployment

In this phase, we can deploy a nice formatted dashboard or shiny apps
for stakeholders to take decisions given an Employee number (To Be
Continued…)

### Inspired From

-   Matt Dancho - [Business Science
    University](https://www.business-science.io/)

### Disclaimer

I am not authorized, endorsed by, or in any way officially connected
with H2O.ai. The views are mine not my employers.
